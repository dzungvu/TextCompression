b'FACE DETECTION IN COLOR IMAGES \nRein-Lien Hsu, Mohamed Abdel-Mottaleb*, and Ani1 K. Jain  Dept. of Computer Science \n& Engineering, Michigan \nState University, MI 48824 * Philips Research, \n345 Scarborough \nRd., Briarcliff Manor, \nNY 10510 * Electrical and Computer \nEngineering Dept., University \nof Miami, FL 33124 Email: { hsureinl,, jain} @cse.msu.edu, mohamed.abde1-mottaleb@philips.com, mottaleb@miami.edu ABSTRACT Human face \ndetection is often the first step \nin applications such as video surveill:ance, human computer interface, \nface recognition, and image \ndatabase management. We propose a face detection algorithm for color images in \nthe presence of varying lighting \nconditions as well as complex backgrounds. Our method detects skin regions \nover the entire image, and then generates \nface candidates based on \nthe spatial arrangement of these skin patches. \nThe algorithm constructs eye, \nmouth, and boundary maps \nfor verifying each face candidate. Experimental results \ndemonstrate successful detection over a wide variety \nof facial variations \nin color, position, scale, \nrotation, pose, and expression \nfrom several photo collections. 1. INTRODUCTION Various approaches to face detection are discussed \nin [lo]. These approaches \nutilize techniques such \nas neural networks, machine learning, (deformable) \ntemplate matching, Hough transform, motion \nextraction, and color analysis. The neural network-based \n[ 113 and view-based \n[14] approaches require \na large number of face and non- \nface training \nexamples, and are \ndesigned to find \nfrontal faces in grayscale images. \nA recent statistical approach [12] extends the detection \nof frontal faces \nto profile views \nby training two \nseparate classifiers. Model-based \napproaches are \nwidely used in tracking \nfaces and often assume that the \ninitial locations of faces are \nknown. Skin color provides an important cue for face detection. \nHowever, the color-based approaches face \ndifficulties in \nrobust detection \nof skin colors in the presence \nof complex background and variations in lighting \nconditions. We propose a face detection algorithm \nwhich is able to handle a wide variety of variations \nin color images. 2. FACE DETECTION \nALGORITHM 0-7 803-6725- \n110 1/$10.00 02001 IEEE 1046 The use of color information can simplify face localization \nin complex environments [3,10]. \nAn overview of our face detection algorithm \nis depicted in Fig. 1, which contains two major modules: (i) face localization for finding face candidates; and (ii) facial feature detection \nfor verifying detected face \ncandidates. Major modules of \nthe algorithm are briefly described below. Figure 1: Face detection algorithm. \n2.1. Lighting compensation and skin tone detection \nThe appearance of the skin-tone \ncolor can change \ndue to different lighting conditions. \nWe introduce a lighting \ncompensation technique that \nestimates reference \nwhite to normalize the \ncolor appearance. \nWe regard pixels with top 5 percent of the luma \n(nonlinear gamma-corrected \nluminance) values \nas the reference \nwhite if the number \nof these reference-white pixels \nis larger than 100. The red, green, and blue components of a \ncolor image are adjusted so that these reference-white pixels are scaled to the gray level of 255. Modeling skin color requires choosing an appropriate \ncolor space \nand identifying \na cluster associated \nwith skin color in this \nspace. Based on Terrillon et al.s [15] comparison of the nine \ncolor spaces \nfor face \ndetection, we \nuse the \nYCbCr space since \nit is \nwidely used in \nvideo compression standards. \nSince the skin-tone color depends on luminance, we nonlinearly \ntransform the YCbCr color space to make the skin cluster luma-independent. \nThis also enables robust detection \nof dark and light skin \ntone colors. A parametric ellipse in the \nnonlinearly 'b'transformed Cb-Cr \ncolor subspace is used \nas a model of \nskin color. Figure 2 shows an example of skin \ndetection. I Figure 2: Skin detection: \n(a) a yellow-biased face image; \n(b) a lighting compensated image; \n(c) skin regions \nof (a) shown as pseudo-color; (d) skin regions \nof (b). 2.2. Localization of facial features Among the various facial features, eyes and mouth are the \nmost suitable features \nfor recognition and estimation \nof 3D head pose \n[5]. Most approaches to eye and face \nlocalization [7, 131 are template based. However, \nour approach is able to directly locate \neyes, mouth, and face \nboundary based \non nieasurenients derived from the color- space components of an image. \nEyes usually contain both dark and bright pixels in the \nluma component. Grayscale morphological \noperators (e.g., dilation and erosion) \n[8] can be designed to emphasize brighter and \ndarker pixels in the luma \ncomponent around eye regions. These \noperations have been \nused to construct \nfeature vectors for a complete face \nat multiple scales for frontal face authentication [9]. In our eye detection algorithm, the grayscale dilation and erosion using \na hemispheric structuring element at \nan estimated scale are \napplied independently \nto construct EyeMap in the luma. \nIn addition, an analysis of the chrominance components \nindicated that high \nCb and low Cr values are found around \nthe eyes. \nThe EyeMap in the chroma is constructed \nfrom Cb, the inverse \nof Cr, and the ratio \nCb/Cr. The two resulting eye maps are combined \nby a multiplication operation. The resultant eye map brightens both the eyes \nand suppresses other facial areas, \nas can be seen in Fig. 3. Eye candidates are selected \nby using (i) \na pyramid decomposition of the enhanced eye maps \nfor coarse localizations and (ii) binary morphological closing and an \niterative thresholding \nfor fine localizations. \nThe mouth region contains more red component compared \nto the blue component than other facial regions. \nHence, the chrominance component C, is greater than \nC, near the mouth areas. \nWe further notice that the mouth has \na relatively lower \nresponse in the c&, feature but \na high response in \nC:. Therefore, the difference between \nC: and c,&, can emphasize the \nmouth regions. Figure \n4 shows the mouth \nmaps of the subjects \nin Fig. 3. Figure 3: Construction of the eye maps for two subjects. Figure 4: Construction of the mouth maps. FCrW kWdUN W f mw bknh Figure 5: Face boundary \nand the eyes-mouth triangle. The eyes and mouth candidates are verified \nby checking (i) luma variations of \neye and mouth blobs; \n(ii) geometry and orientation constraints of eyes-mouth triangles; and \n(iii) the presence of \na face boundary around eyes-mouth \ntriangles. Based \non the locations of \neyedmouth candidates, our algorithm first constructs a face boundary map from the luma, and then utilizes \na Hough transform to \nextract the best-fitting ellipse. \nThe fitted ellipse is \nassociated with a quality measurement \nfor computing eyes- \nand-mouth triangle weights. Figure \n5 shows the boundary \nmap which \nis constructed from both the magnitude and the \norientation components \nof the luma gradient within the \nregions having positive orientations of \nthe gradient orientations. The Hough transform is useful for the detection of parametric shapes; its efficiency depends on \nthe dimensionality of the accumulator. \nAn ellipse in a plane has \nfive paramcters: an orientation angle, two \ncoordinates of the \ncenter, and lengths of major and minor \naxes. Since \nwe know the locations of eyes and mouth, the \norientation of the \nellipse can be estimated from the direction of \na vector that starts from midpoint between \neyes to the mouth. \nThe location of the \nellipse center is estimated from the face boundary. Hence, we only require \n1047 'b'a two-dimensional accumulator \nfor an ellipse in a plane. \nOur method detects skin regions \nover the entire image, and \nThe ellipse with the highest vote \nis selected. then generates \nface candidates \nbased on the spatial \narrangement of these skin patches. \nThe algorithm Each eye-mouth triangle candidate \nis associated with a constructs eye/mouth/boundary maps \nfor verifying each weight that is computed from its eyedmouth maps, ellipse face candidate. Detection results \nfor several photo \nvote and \nface orientation that favors upright \nfaces and collections have been \npresented. Our goal is to design \na symmetric facial geometry (see [6] for details). system that \ndetects faces and facial features, \nallows users \nto edit detected faces, and \nuses the facial \nfeatures as 3. EXPERIMENTAL RESULTS \nindices for retrieval from image and video databases. \nPersonal photo \ncollections usually contain \ncolor images \nthat are taken under varying lighting \nconditions as well as \nwith complex backgrounds. Further, these images \nmy have quality variations and contain multiple \nfaces with 5. REFERENCES [I] MPEG7 content set from \nHeinrich Hertz Institute, \n<http://ww w.darmstadt.gmd.de/mobile/MPEG7/Documents/N2466.html>. -- variations in \ncolor, position, scale, rotation, pose, and facial expression. We present detection results \nin Tables 1 and 2 on the \nI [l] and the Champion \n[2] databases, \n[2] Champion database, <http://www.libfind.unl.edu/alumni/ champions>, respectively. Figure 6 shows that our \nalgorithm can detect \nmultiple faces of \ndifferent sizes with a wide variety of \nfacial variations. \nFurther, the algorithm \ncan detect \nboth dark skin-tone and bright skin-tone \nbecause of the \nnonlinear transform of the Cb-Cr color space. \nTable 1: Detection results on the HHI image database. \nFP: False \nPositives, DR: \nDetection Rate. I F~;F~~ I Half- Profile \n1 Profile 1 All Table 2: Detection results on the Champion \nimage database. Head Pose No. ofimages I 227 I Frontal, Near-frontal, Half profile I Size (oixel) 1 150 x 220 I Sta e 1: Groii led Skin-re-iou 0.15 (avera e) t 0.08 (s. d.) on a 860MHz CPU Time (sec) No. of FP DR (W) Time (sec) Sta e 2: Facial Feature \nlacation 9.29 (avera e) i 7.77 (s. d.) 4. CONCLUSIONS AND FUTURE \nWORK We have presented a \nface detection algorithm \nfor color \nimages using a skin-tone \ncolor model and facial features. \nOur method first corrects the color bias by a novel lighting \ncompensation technique that automatically \nestimates the reference white pixels. \nWe overcome the difficulty of \ndetecting the low-luma and high-luma skin \ntones by applying a nonlinear transform \nto the YCbCr \ncolor space. \n[3] M. Abdel-Mottaleb and \nA. Elgammal, Face detection \nin complex environments \nfrom color images, \nIEEE KIP, pp. 622- \n626, Oct. 1999. \n[4] J. Canny, A computational approach \nto edge detection, \nIEEE Trans. PAMI, vol. 8, pp. 679-698, Nov. 1986. [5] T. Horprasert, Y. Yacoob, and \nL.S. Davis, Computing \n3-D head orientation from a monocular image, Proc. Intl ConJ: Automatic Face and \nGesture Recognition, pp. 242-247, Oct. \n1996. [6] R.-L. Hsu, M. Abdel-Mottaleb, and A.K. Jain, Face detection in color images, Tech. Report MSU-CSE-01-7, Michigan State Univ., March 2001. [7] W. Huang, Q. Sun, C.-P. \nLam, and J.-K. Wu, A robust approach to face and eyes detection \nfrom images with \ncluttered background, ICPR, vol. I , pp. 110-1 14, Aug. 1998. [8] P.T. Jackway and \nM. Deriche, Scale-space properties of the multiscale morphological \ndilation-erosion, ZEEE Trans. PAMI, vol. 18, pp. 38-51, Jan. 1996. [9] C. Kotropoulos, A. Tefas, and I. Pitas, Frontal face \nauthentication using morphological \nelastic graph matching, IEEE Trans. Image Processing, vol. 9, pp. 555-560, \nApril 2000. [IO] D. Maio and D. Maltoni, Real-time face location on gray- scale static images, Pattern Recognition, \nvo1.33, no. 9, pp. 1525-1539, Sept. 2000. \n[ 1 I] H.A. Rowley, \nS. Baluja, and T. Kanade, Neural network- \nbased face detection, \nIEEE Trans. PAMI, vol. 20, pp. 23-38, \nJan. 1998. [I21 H. Schneiderman and \nT. Kanade, A statistical method for 3D object detection applied \nto faces and cars, \nIEEE CVPR, June 2000. \n[I31 F. Smeraldi, 0. Carmona, and \nJ. Bigun, Saccadic search with gabor \nfeatures applied to eye detection and real-time head 1048 'b'tracking, Image and Vision Computing, vol. 18, no. 4, pp. 323- [IS] J.C. Tenillon, M.N. Shirazi, H. Fukamachi, and S. 329,2000. Akamatsu, Comparative performance of different skin \nchrominance models \nand chrominance spaces for \nthe automatic [I41 K.K. Sung and T. Poggio, Example-based learning for detection \nof human faces in color images, \nProc. IEEE Zntl view-based human face detection, \nlEEE Trans. PAMI, vol. 20, Con$ Automatic Face \nand Gesrure Recognition, pp. 54-61, pp. 39-51, Jan. 1998. 2000. Figure 6: Face detection results \non the I and the \nChampion databases, \nand a collection \nof family photos. The detected faces, \nrepresented as ellipses, \nare overlaid on the color-compensated \nimages. There are a \nfew false positives \nand negatives in \nthe family group photographs. 1049 '