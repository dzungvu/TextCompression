b"Richfeaturehierarchiesforaccurateobjectdetectionandsemanticsegmentation\nRossGirshick\n1\nJeffDonahue\n1\n;\n2\nTrevorDarrell\n1\n;\n2\nJitendraMalik\n1\n1\nUCBerkeleyand\n2\nICSI\nf\nrbg,jdonahue,trevor,malik\ng\n@eecs.berkeley.edu\nAbstract\nObjectdetectionperformance,asmeasuredonthe\ncanonicalPASCALVOCdataset,hasplateauedinthelast\nfewyears.Thebest-performingmethodsarecomplexen-\nsemblesystemsthattypicallycombinemultiplelow-level\nimagefeatureswithhigh-levelcontext.Inthispaper,we\nproposeasimpleandscalabledetectionalgorithmthatim-\nprovesmeanaverageprecision(mAP)bymorethan30%\nrelativetothepreviousbestresultonVOC2012achieving\namAPof53.3%.Ourapproachcombinestwokeyinsights:\n(1)onecanapplyhigh-capacityconvolutionalneuralnet-\nworks(CNNs)tobottom-upregionproposalsinorderto\nlocalizeandsegmentobjectsand(2)whenlabeledtraining\ndataisscarce,supervisedpre-trainingforanauxiliarytask,\nfollowedby,yieldsa\ncantperformanceboost.Sincewecombineregionpropos-\nalswithCNNs,wecallourmethod\nR-CNN:\nRegionswith\nCNNfeatures.Wealsopresentexperimentsthatprovide\ninsightintowhatthenetworklearns,revealingarichhier-\narchyofimagefeatures.Sourcecodeforthecompletesys-\ntemisavailableat\nhttp://www.cs.berkeley.edu/\n\nrbg/rcnn\n.\n1.Introduction\nFeaturesmatter.Thelastdecadeofprogressonvarious\nvisualrecognitiontaskshasbeenbasedconsiderablyonthe\nuseofSIFT[\n26\n]andHOG[\n7\n].Butifwelookatperfor-\nmanceonthecanonicalvisualrecognitiontask,PASCAL\nVOCobjectdetection[\n12\n],itisgenerallyacknowledged\nthatprogresshasbeenslowduring2010-2012,withsmall\ngainsobtainedbybuildingensemblesystemsandemploy-\ningminorvariantsofsuccessfulmethods.\nSIFTandHOGareblockwiseorientationhistograms,\narepresentationwecouldassociateroughlywithcomplex\ncellsinV1,thecorticalareaintheprimatevisualpath-\nway.Butwealsoknowthatrecognitionoccursseveral\nstagesdownstream,whichsuggeststhattheremightbehier-\narchical,multi-stageprocessesforcomputingfeaturesthat\nareevenmoreinformativeforvisualrecognition.\nFukushima'sneocognitron[\n16\n],abiologically-\nFigure1:Objectdetectionsystemoverview.\nOursystem(1)\ntakesaninputimage,(2)extractsaround2000bottom-upregion\nproposals,(3)computesfeaturesforeachproposalusingalarge\nconvolutionalneuralnetwork(CNN),andthen(4)each\nregionusingcllinearSVMs.R-CNNachievesamean\naverageprecision(mAP)of\n53.7%onPASCALVOC2010\n.For\ncomparison,[\n32\n]reports35.1%mAPusingthesameregionpro-\nposals,butwithaspatialpyramidandbag-of-visual-wordsap-\nproach.Thepopulardeformablepartmodelsperformat33.4%.\ninspiredhierarchicalandshift-invariantmodelforpattern\nrecognition,wasanearlyattemptatjustsuchaprocess.\nTheneocognitron,however,lackedasupervisedtrainingal-\ngorithm.LeCun\netal\n.[\n23\n]providedthemissingalgorithm\nbyshowingthatstochasticgradientdescent,viabackprop-\nagation,cantrainconvolutionalneuralnetworks(CNNs),a\nclassofmodelsthatextendtheneocognitron.\nCNNssawheavyuseinthe1990s(\ne.g\n.,[\n24\n]),butthen\nfelloutoffashion,particularlyincomputervision,withthe\nriseofsupportvectormachines.In2012,Krizhevsky\netal\n.\n[\n22\n]rekindledinterestinCNNsbyshowingsubstantially\nhigherimageaccuracyontheImageNetLarge\nScaleVisualRecognitionChallenge(ILSVRC)[\n9\n,\n10\n].\nTheirsuccessresultedfromtrainingalargeCNNon1.2\nmillionlabeledimages,togetherwithafewtwistsonLe-\nCun'sCNN(\ne.g\n.,\nmax(\nx;\n0)\nrectifyingnon-linearitiesand\ndropoutregularization).\nTheoftheImageNetresultwasvigorously\ndebatedduringtheILSVRC2012workshop.Thecentral\nissuecanbedistilledtothefollowing:Towhatextentdo\ntheCNNresultsonImageNetgeneralizeto\nobjectdetectionresultsonthePASCALVOCChallenge?\nWeanswerthisquestiondecisivelybybridgingthe\nchasmbetweenimageandobjectdetection.\nThispaperisthetoshowthataCNNcanleadtodra-\n1\n"b"maticallyhigherobjectdetectionperformanceonPASCAL\nVOCascomparedtosystemsbasedonsimplerHOG-like\nfeatures.\n1\nAchievingthisresultrequiredsolvingtwoprob-\nlems:localizingobjectswithadeepnetworkandtraininga\nhigh-capacitymodelwithonlyasmallquantityofannotated\ndetectiondata.\nUnlikeimagedetectionrequireslocaliz-\ning(likelymany)objectswithinanimage.Oneapproach\nframeslocalizationasaregressionproblem.However,work\nfromSzegedy\netal\n.[\n31\n],concurrentwithourown,indi-\ncatesthatthisstrategymaynotfarewellinpractice(they\nreportamAPof30.5%onVOC2007comparedtothe\n58.5%achievedbyourmethod).Analternativeistobuilda\nsliding-windowdetector.CNNshavebeenusedinthisway\nforatleasttwodecades,typicallyonconstrainedobjectcat-\negories,suchasfaces[\n28\n,\n33\n]andpedestrians[\n29\n].Inorder\ntomaintainhighspatialresolution,theseCNNstypically\nonlyhavetwoconvolutionalandpoolinglayers.Wealso\nconsideredadoptingasliding-windowapproach.However,\nunitshighupinournetwork,whichhasveconvolutional\nlayers,haveverylargereceptive(\n195\n\n195\npixels)\nandstrides(\n32\n\n32\npixels)intheinputimage,whichmakes\npreciselocalizationwithinthesliding-windowparadigman\nopentechnicalchallenge.\nInstead,wesolvetheCNNlocalizationproblembyop-\neratingwithintherecognitionusingregionsparadigm,as\narguedforbyGu\netal\n.in[\n18\n].Attest-time,ourmethod\ngeneratesaround2000category-independentregionpro-\nposalsfortheinputimage,extractsaed-lengthfeature\nvectorfromeachproposalusingaCNN,andthenclassi-\neachregionwithcatelinearSVMs.We\nuseasimpletechnique(afimagewarping)tocompute\naed-sizeCNNinputfromeachregionproposal,regard-\nlessoftheregion'sshape.Figure\n1\npresentsanoverviewof\nourmethodandhighlightssomeofourresults.Sinceour\nsystemcombinesregionproposalswithCNNs,wedubthe\nmethodR-CNN:RegionswithCNNfeatures.\nAsecondchallengefacedindetectionisthatlabeled\ndataisscarceandtheamountcurrentlyavailableisinsuf\ncientfortrainingalargeCNN.Theconventionalsolutionto\nthisproblemistouse\nunsupervised\npre-training,followed\nbysupervised(\ne.g\n.,[\n29\n]).Thesecondmajor\ncontributionofthispaperistoshowthat\nsupervised\npre-\ntrainingonalargeauxiliarydataset(ILSVRC),followedby\nonasmalldataset(PASCAL),\nisaneffectiveparadigmforlearninghigh-capacityCNNs\nwhendataisscarce.Inourexperiments,forde-\ntectionimprovesmAPperformanceby8percentagepoints.\nAfteroursystemachievesamAPof54%on\nVOC2010comparedto33%forthehighly-tuned,HOG-\nbaseddeformablepartmodel(DPM)[\n14\n,\n17\n].\n1\nAtechreportdescribingR-CNNappearedat\nhttp://arxiv.\norg/abs/1311.2524v1\ninNov.2013.\nOursystemisalsoquiteefTheonly\ncomputationsareareasonablysmallmatrix-vectorproduct\nandgreedynon-maximumsuppression.Thiscomputational\npropertyfollowsfromfeaturesthataresharedacrossallcat-\negoriesandthatarealsotwoordersofmagnitudelower-\ndimensionalthanpreviouslyusedregionfeatures(\ncf.\n[\n32\n]).\nOneadvantageofHOG-likefeaturesistheirsimplic-\nity:it'seasiertounderstandtheinformationtheycarry(al-\nthough[\n34\n]showsthatourintuitioncanfailus).Canwe\ngaininsightintotherepresentationlearnedbytheCNN?\nPerhapsthedenselyconnectedlayers,withmorethan54\nmillionparameters,arethekey?Theyarenot.We\nlobotomizedtheCNNandfoundthatasurprisinglylarge\nproportion,94%,ofitsparameterscanberemovedwith\nonlyamoderatedropindetectionaccuracy.Instead,by\nprobingunitsinthenetworkweseethattheconvolutional\nlayerslearnadiversesetofrichfeatures(Figure\n3\n).\nUnderstandingthefailuremodesofourapproachisalso\ncriticalforimprovingit,andsowereportresultsfromthe\ndetectionanalysistoolofHoiem\netal\n.[\n20\n].Asanimmedi-\nateconsequenceofthisanalysis,wedemonstratethatasim-\npleboundingboxregressionmethodreduces\nmislocalizations,whicharethedominanterrormode.\nBeforedevelopingtechnicaldetails,wenotethatbe-\ncauseR-CNNoperatesonregionsitisnaturaltoextendit\ntothetaskofsemanticsegmentation.Withminor\ncations,wealsoachievestate-of-the-artresultsonthePAS-\nCALVOCsegmentationtask,withanaveragesegmentation\naccuracyof47.9%ontheVOC2011testset.\n2.ObjectdetectionwithR-CNN\nOurobjectdetectionsystemconsistsofthreemodules.\nThegeneratescategory-independentregionproposals.\nTheseproposalsthesetofcandidatedetectionsavail-\nabletoourdetector.Thesecondmoduleisalargeconvo-\nlutionalneuralnetworkthatextractsaed-lengthfeature\nvectorfromeachregion.Thethirdmoduleisasetofclass-\nlinearSVMs.Inthissection,wepresentourdesign\ndecisionsforeachmodule,describetheirtest-timeusage,\ndetailhowtheirparametersarelearned,andshowresultson\nPASCALVOC2010-12.\n2.1.Moduledesign\nRegionproposals.\nAvarietyofrecentpapersoffermeth-\nodsforgeneratingcategory-independentregionproposals.\nExamplesinclude:objectness[\n1\n],selectivesearch[\n32\n],\ncategory-independentobjectproposals[\n11\n],constrained\nparametricmin-cuts(CPMC)[\n5\n],multi-scalecombinatorial\ngrouping[\n3\n],andCiresan\netal\n.[\n6\n],whodetectmitoticcells\nbyapplyingaCNNtoregularly-spacedsquarecrops,which\nareaspecialcaseofregionproposals.WhileR-CNNisag-\nnostictotheparticularregionproposalmethod,weusese-\nlectivesearchtoenableacontrolledcomparisonwithprior\n"b"Figure2:Warpedtrainingsamples\nfromVOC2007train.\ndetectionwork(\ne.g\n.,[\n32\n,\n35\n]).\nFeatureextraction.\nWeextracta4096-dimensionalfea-\nturevectorfromeachregionproposalusingtheCaffe[\n21\n]\nimplementationoftheCNNdescribedbyKrizhevsky\net\nal\n.[\n22\n].Featuresarecomputedbyforwardpropagatinga\nmean-subtracted\n227\n\n227\nRGBimagethroughvecon-\nvolutionallayersandtwofullyconnectedlayers.Werefer\nreadersto[\n21\n,\n22\n]formorenetworkarchitecturedetails.\nInordertocomputefeaturesforaregionproposal,we\nmustconverttheimagedatainthatregionintoaform\nthatiscompatiblewiththeCNN(itsarchitecturerequires\ninputsofaed\n227\n\n227\npixelsize).Ofthemanypossi-\nbletransformationsofourarbitrary-shapedregions,weopt\nforthesimplest.Regardlessofthesizeoraspectratioofthe\ncandidateregion,wewarpallpixelsinatightboundingbox\naroundittotherequiredsize.Priortowarping,wedilatethe\ntightboundingboxsothatatthewarpedsizethereareex-\nactly\np\npixelsofwarpedimagecontextaroundtheoriginal\nbox(weuse\np\n=16\n).Figure\n2\nshowsarandomsamplingof\nwarpedtrainingregions.Thesupplementarymaterialdis-\ncussesalternativestowarping.\n2.2.Tdetection\nAttesttime,werunselectivesearchonthetestimage\ntoextractaround2000regionproposals(weuseselective\nsearch'sfastmodeinallexperiments).Wewarpeach\nproposalandforwardpropagateitthroughtheCNNinor-\ndertoreadofffeaturesfromthedesiredlayer.Then,for\neachclass,wescoreeachextractedfeaturevectorusingthe\nSVMtrainedforthatclass.Givenallscoredregionsinan\nimage,weapplyagreedynon-maximumsuppression(for\neachclassindependently)thatrejectsaregionifithasan\nintersection-over-union(IoU)overlapwithahigherscoring\nselectedregionlargerthanalearnedthreshold.\nRun-timeanalysis.\nTwopropertiesmakedetectionef\ncient.First,allCNNparametersaresharedacrossallcate-\ngories.Second,thefeaturevectorscomputedbytheCNN\narelow-dimensionalwhencomparedtoothercommonap-\nproaches,suchasspatialpyramidswithbag-of-visual-word\nencodings.ThefeaturesusedintheUVAdetectionsystem\n[\n32\n],forexample,aretwoordersofmagnitudelargerthan\nours(360k\nvs\n.4k-dimensional).\nTheresultofsuchsharingisthatthetimespentcom-\nputingregionproposalsandfeatures(13s/imageonaGPU\nor53s/imageonaCPU)isamortizedoverallclasses.The\nonlycomputationsaredotproductsbetween\nfeaturesandSVMweightsandnon-maximumsuppression.\nInpractice,alldotproductsforanimagearebatchedinto\nasinglematrix-matrixproduct.Thefeaturematrixistypi-\ncally\n2000\n\n4096\nandtheSVMweightmatrixis\n4096\n\nN\n,\nwhere\nN\nisthenumberofclasses.\nThisanalysisshowsthatR-CNNcanscaletothousands\nofobjectclasseswithoutresortingtoapproximatetech-\nniques,suchashashing.Eveniftherewere100kclasses,\ntheresultingmatrixmultiplicationtakesonly10secondson\namodernmulti-coreCPU.Thisefyisnotmerelythe\nresultofusingregionproposalsandsharedfeatures.The\nUVAsystem,duetoitshigh-dimensionalfeatures,would\nbetwoordersofmagnitudeslowerwhilerequiring134GB\nofmemoryjusttostore100klinearpredictors,comparedto\njust1.5GBforourlower-dimensionalfeatures.\nItisalsointerestingtocontrastR-CNNwiththerecent\nworkfromDean\netal\n.onscalabledetectionusingDPMs\nandhashing[\n8\n].TheyreportamAPofaround16%onVOC\n2007atarun-timeof5minutesperimagewhenintroducing\n10kdistractorclasses.Withourapproach,10kdetectorscan\nruninaboutaminuteonaCPU,andbecausenoapproxi-\nmationsaremademAPwouldremainat59%(Section\n3.2\n).\n2.3.Training\nSupervisedpre-training.\nWediscriminativelypre-trained\ntheCNNonalargeauxiliarydataset(ILSVRC2012)with\nimage-levelannotations\n(\ni.e\n.,noboundingboxlabels).Pre-\ntrainingwasperformedusingtheopensourceCaffeCNN\nlibrary[\n21\n].Inbrief,ourCNNnearlymatchestheperfor-\nmanceofKrizhevsky\netal\n.[\n22\n],obtainingatop-1errorrate\n2.2percentagepointshigherontheILSVRC2012valida-\ntionset.Thisdiscrepancyisduetointhe\ntrainingprocess.\n.\nToadaptourCNNtothe\nnewtask(detection)andthenewdomain(warpedVOC\nwindows),wecontinuestochasticgradientdescent(SGD)\ntrainingoftheCNNparametersusingonlywarpedre-\ngionproposalsfromVOC.AsidefromreplacingtheCNN's\n1000-waylayerwitharan-\ndomlyinitialized21-waylayer(forthe20\nVOCclassesplusbackground),theCNNarchitectureisun-\nchanged.Wetreatallregionproposalswith\n\n0\n:\n5\nIoUover-\nlapwithaground-truthboxaspositivesforthatbox'sclass\nandtherestasnegatives.WestartSGDatalearningrate\nof0.001(1/10thoftheinitialpre-trainingrate),whichal-\nlowstomakeprogresswhilenotclobberingthe\ninitialization.IneachSGDiteration,weuniformlysample\n32positivewindows(overallclasses)and96background\nwindowstoconstructamini-batchofsize128.Webias\nthesamplingtowardspositivewindowsbecausetheyareex-\ntremelyrarecomparedtobackground.\n"b"VOC2010test\naerobikebirdboatbottlebuscarcatchaircowtabledoghorsembikepersonplantsheepsofatraintv\nmAP\nDPMv5[\n17\n]\ny\n49.253.813.115.335.553.449.727.017.228.814.717.846.451.247.710.834.220.743.838.3\n33.4\nUVA[\n32\n]\n56.242.415.312.621.849.336.846.112.932.130.036.543.552.932.915.341.131.847.044.8\n35.1\nRegionlets[\n35\n]\n65.048.925.924.624.556.154.551.217.028.930.235.840.255.743.514.343.932.654.045.9\n39.7\nSegDPM[\n15\n]\ny\n61.453.425.625.235.551.750.650.819.333.826.840.448.354.447.114.838.735.052.843.1\n40.4\nR-CNN\n67.164.146.732.030.556.457.265.927.047.340.966.657.865.953.626.756.538.152.850.2\n50.2\nR-CNNBB\n71.865.853.036.835.959.760.069.927.950.641.470.062.069.058.129.559.439.361.252.4\n53.7\nTable1:Detectionaverageprecision(%)onVOC2010test.\nR-CNNismostdirectlycomparabletoUVAandRegionletssinceall\nmethodsuseselectivesearchregionproposals.Boundingboxregression(BB)isdescribedinSection\n3.4\n.Atpublicationtime,SegDPM\nwasthetop-performeronthePASCALVOCleaderboard.\ny\nDPMandSegDPMusecontextrescoringnotusedbytheothermethods.\nObjectcategorycl\nConsidertrainingabinary\ntodetectcars.It'sclearthatanimageregion\ntightlyenclosingacarshouldbeapositiveexample.Simi-\nlarly,it'sclearthatabackgroundregion,whichhasnothing\ntodowithcars,shouldbeanegativeexample.Lessclear\nishowtolabelaregionthatpartiallyoverlapsacar.Were-\nsolvethisissuewithanIoUoverlapthreshold,belowwhich\nregionsareasnegatives.Theoverlapthreshold,\n0\n:\n3\n,\nwasselectedbyagridsearchover\nf\n0\n;\n0\n:\n1\n;:::;\n0\n:\n5\ng\nona\nvalidationset.Wefoundthatselectingthisthresholdcare-\nfullyisimportant.Settingitto\n0\n:\n5\n,asin[\n32\n],decreased\nmAPby\n5\npoints.Similarly,settingitto\n0\ndecreasedmAP\nby\n4\npoints.Positiveexamplesaresimplytobethe\nground-truthboundingboxesforeachclass.\nOncefeaturesareextractedandtraininglabelsareap-\nplied,weoptimizeonelinearSVMperclass.Sincethe\ntrainingdataistoolargetoinmemory,weadoptthe\nstandardhardnegativeminingmethod[\n14\n,\n30\n].Hardneg-\nativeminingconvergesquicklyandinpracticemAPstops\nincreasingafteronlyasinglepassoverallimages.\nInsupplementarymaterialwediscusswhythepositive\nandnegativeexamplesaredifferentlyin\nversusSVMtraining.Wealsodiscusswhyit'snecessary\ntotraindetectionratherthansimplyuseoutputs\nfromthelayer(fc\n8\n)oftheCNN.\n2.4.ResultsonPASCALVOC\nFollowingthePASCALVOCbestpractices[\n12\n],we\nvalidatedalldesigndecisionsandhyperparametersonthe\nVOC2007dataset(Section\n3.2\n).Forresultsonthe\nVOC2010-12datasets,wetheCNNonVOC\n2012trainandoptimizedourdetectionSVMsonVOC2012\ntrainval.Wesubmittedtestresultstotheevaluationserver\nonlyonceforeachofthetwomajoralgorithmvariants(with\nandwithoutboundingboxregression).\nTable\n1\nshowscompleteresultsonVOC2010.Wecom-\npareourmethodagainstfourstrongbaselines,including\nSegDPM[\n15\n],whichcombinesDPMdetectorswiththe\noutputofasemanticsegmentationsystem[\n4\n]andusesad-\nditionalinter-detectorcontextandrescor-\ning.ThemostgermanecomparisonistotheUVAsystem\nfromUijlings\netal\n.[\n32\n],sinceoursystemsusethesamere-\ngionproposalalgorithm.Toclassifyregions,theirmethod\nbuildsafour-levelspatialpyramidandpopulatesitwith\ndenselysampledSIFT,ExtendedOpponentSIFT,andRGB-\nSIFTdescriptors,eachvectorquantizedwith4000-word\ncodebooks.isperformedwithahistogram\nintersectionkernelSVM.Comparedtotheirmulti-feature,\nnon-linearkernelSVMapproach,weachievealargeim-\nprovementinmAP,from35.1%to53.7%mAP,whilealso\nbeingmuchfaster(Section\n2.2\n).Ourmethodachievessim-\nilarperformance(53.3%mAP)onVOC2011/12test.\n3.Visualization,ablation,andmodesoferror\n3.1.Visualizinglearnedfeatures\nFirst-layercanbevisualizeddirectlyandareeasy\ntounderstand[\n22\n].Theycaptureorientededgesandoppo-\nnentcolors.Understandingthesubsequentlayersismore\nchallenging.ZeilerandFerguspresentavisuallyattrac-\ntivedeconvolutionalapproachin[\n36\n].Weproposeasimple\n(andcomplementary)non-parametricmethodthatdirectly\nshowswhatthenetworklearned.\nTheideaistosingleoutaparticularunit(feature)inthe\nnetworkanduseitasifitwereanobjectdetectorinitsown\nright.Thatis,wecomputetheunit'sactivationsonalarge\nsetofheld-outregionproposals(about10million),sortthe\nproposalsfromhighesttolowestactivation,performnon-\nmaximumsuppression,andthendisplaythetop-scoringre-\ngions.Ourmethodletstheselectedunitspeakforitself\nbyshowingexactlywhichinputsiton.Weavoidaver-\naginginordertoseedifferentvisualmodesandgaininsight\nintotheinvariancescomputedbytheunit.\nWevisualizeunitsfromlayerpool\n5\n,whichisthemax-\npooledoutputofthenetwork'sandconvolutional\nlayer.Thepool\n5\nfeaturemapis\n6\n\n6\n\n256=9216\n-\ndimensional.Ignoringboundaryeffects,eachpool\n5\nunithas\nareceptiveof\n195\n\n195\npixelsintheoriginal\n227\n\n227\npixelinput.Acentralpool\n5\nunithasanearlyglobalview,\nwhileoneneartheedgehasasmaller,clippedsupport.\nEachrowinFigure\n3\ndisplaysthetop16activationsfor\napool\n5\nunitfromaCNNthatweonVOC2007\ntrainval.Sixofthe256functionallyuniqueunitsarevisu-\nalized(thesupplementarymaterialincludesmore).These\n"b"Figure3:Topregionsforsixpool\n5\nunits.\nReceptiveandactivationvaluesaredrawninwhite.Someunitsarealignedtoconcepts,\nsuchaspeople(row1)ortext(4).Otherunitscapturetextureandmaterialproperties,suchasdotarrays(2)andspeculartions(6).\nVOC2007test\naerobikebirdboatbottlebuscarcatchaircowtabledoghorsembikepersonplantsheepsofatraintv\nmAP\nR-CNNpool\n5\n51.860.236.427.823.252.860.649.218.347.844.340.856.658.742.423.446.136.751.355.7\n44.2\nR-CNNfc\n6\n59.361.843.134.025.153.160.652.821.747.842.747.852.558.544.625.648.334.053.158.0\n46.2\nR-CNNfc\n7\n57.657.938.531.823.751.258.951.420.050.540.946.051.655.943.323.348.135.351.057.4\n44.7\nR-CNNFTpool\n5\n58.263.337.927.626.154.166.951.426.755.543.443.157.759.045.828.150.840.653.156.4\n47.3\nR-CNNFTfc\n6\n63.566.047.937.729.962.570.260.232.057.947.053.560.164.252.231.355.050.057.763.0\n53.1\nR-CNNFTfc\n7\n64.269.750.041.932.062.671.060.732.758.546.556.160.666.854.231.552.848.957.964.7\n54.2\nR-CNNFTfc\n7\nBB\n68.172.856.843.036.866.374.267.634.463.554.561.269.168.658.733.462.951.162.564.8\n58.5\nDPMv5[\n17\n]\n33.260.310.216.127.354.358.223.020.024.126.712.758.148.243.212.021.136.146.043.5\n33.7\nDPMST[\n25\n]\n23.858.210.58.527.150.452.07.319.222.818.18.055.944.832.413.315.922.846.244.9\n29.1\nDPMHSC[\n27\n]\n32.258.311.516.330.649.954.823.521.527.734.013.758.151.639.912.423.534.447.445.2\n34.3\nTable2:Detectionaverageprecision(%)onVOC2007test.\nRows1-3showR-CNNperformancewithoutRows4-6show\nresultsfortheCNNpre-trainedonILSVRC2012andthen(FT)onVOC2007trainval.Row7includesasimpleboundingbox\nregression(BB)stagethatreduceslocalizationerrors(Section\n3.4\n).Rows8-10presentDPMmethodsasastrongbaseline.Theuses\nonlyHOG,whilethenexttwousedifferentfeaturelearningapproachestoaugmentorreplaceHOG.\nunitswereselectedtoshowarepresentativesampleofwhat\nthenetworklearns.Inthesecondrow,weseeaunitthat\nondogfacesanddotarrays.Theunitcorrespondingto\nthethirdrowisaredblobdetector.Therearealsodetectors\nforhumanfacesandmoreabstractpatternssuchastextand\ntriangularstructureswithwindows.Thenetworkappears\ntolearnarepresentationthatcombinesasmallnumberof\nclass-tunedfeaturestogetherwithadistributedrepresenta-\ntionofshape,texture,color,andmaterialproperties.The\nsubsequentfullyconnectedlayerfc\n6\nhastheabilitytomodel\nalargesetofcompositionsoftheserichfeatures.\n3.2.Ablationstudies\nPerformancelayer-by-layer,without.\nToun-\nderstandwhichlayersarecriticalfordetectionperformance,\nweanalyzedresultsontheVOC2007datasetforeachofthe\nCNN'slastthreelayers.Layerpool\n5\nwasdescribed\ninSection\n3.1\n.Thetwolayersaresummarizedbelow.\nLayerfc\n6\nisfullyconnectedtopool\n5\n.Tocomputefea-\ntures,itmultipliesa\n4096\n\n9216\nweightmatrixbythepool\n5\nfeaturemap(reshapedasa9216-dimensionalvector)and\nthenaddsavectorofbiases.Thisintermediatevectoris\ncomponent-wisehalf-wave(\nx\n \nmax(0\n;x\n)\n).\nLayerfc\n7\nisthelayerofthenetwork.Itisimple-\nmentedbymultiplyingthefeaturescomputedbyfc\n6\nbya\n4096\n\n4096\nweightmatrix,andsimilarlyaddingavector\nofbiasesandapplyinghalf-wave\nWestartbylookingatresultsfromtheCNN\nwithout\n\nonPASCAL,\ni.e\n.allCNNparameterswerepre-\ntrainedonILSVRC2012only.Analyzingperformance\nlayer-by-layer(Table\n2\nrows1-3)revealsthatfeaturesfrom\nfc\n7\ngeneralizeworsethanfeaturesfromfc\n6\n.Thismeans\nthat29%,orabout16.8million,oftheCNN'sparameters\ncanberemovedwithoutdegradingmAP.Moresurprisingis\nthatremoving\nboth\nfc\n7\nandfc\n6\nproducesquitegoodresults\neventhoughpool\n5\nfeaturesarecomputedusing\nonly6%\nof\ntheCNN'sparameters.MuchoftheCNN'srepresentational\npowercomesfromitsconvolutionallayers,ratherthanfrom\n"b"themuchlargerdenselyconnectedlayers.Thissug-\ngestspotentialutilityincomputingadensefeaturemap,in\nthesenseofHOG,ofanarbitrary-sizedimagebyusingonly\ntheconvolutionallayersoftheCNN.Thisrepresentation\nwouldenableexperimentationwithsliding-windowdetec-\ntors,includingDPM,ontopofpool\n5\nfeatures.\nPerformancelayer-by-layer,with.\nWenow\nlookatresultsfromourCNNafterhavingitspa-\nrametersonVOC2007trainval.Theimprovementisstrik-\ning(Table\n2\nrows4-6):increasesmAPby8.0\npercentagepointsto54.2%.Theboostfromis\nmuchlargerforfc\n6\nandfc\n7\nthanforpool\n5\n,whichsuggests\nthatthepool\n5\nfeatureslearnedfromImageNetaregeneral\nandthatmostoftheimprovementisgainedfromlearning\nnon-linearontopofthem.\nComparisontorecentfeaturelearningmethods.\nRela-\ntivelyfewfeaturelearningmethodshavebeentriedonPAS-\nCALVOCdetection.Welookattworecentapproachesthat\nbuildondeformablepartmodels.Forreference,wealsoin-\ncluderesultsforthestandardHOG-basedDPM[\n17\n].\nTheDPMfeaturelearningmethod,DPMST[\n25\n],\naugmentsHOGfeatureswithhistogramsofsketchtoken\nprobabilities.Intuitively,asketchtokenisatightdistri-\nbutionofcontourspassingthroughthecenterofanimage\npatch.Sketchtokenprobabilitiesarecomputedateachpixel\nbyarandomforestthatwastrainedtoclassify\n35\n\n35\npixel\npatchesintooneof150sketchtokensorbackground.\nThesecondmethod,DPMHSC[\n27\n],replacesHOGwith\nhistogramsofsparsecodes(HSC).TocomputeanHSC,\nsparsecodeactivationsaresolvedforateachpixelusing\nalearneddictionaryof100\n7\n\n7\npixel(grayscale)atoms.\nTheresultingactivationsareinthreeways(fulland\nbothhalf-waves),spatiallypooled,unit\n`\n2\nnormalized,and\nthenpowertransformed(\nx\n \nsign\n(\nx\n)\nj\nx\nj\n\n).\nAllR-CNNvariantsstronglyoutperformthethreeDPM\nbaselines(Table\n2\nrows8-10),includingthetwothatuse\nfeaturelearning.ComparedtothelatestversionofDPM,\nwhichusesonlyHOGfeatures,ourmAPismorethan20\npercentagepointshigher:54.2%\nvs\n.33.7%\na61%rela-\ntiveimprovement\n.ThecombinationofHOGandsketchto-\nkensyields2.5mAPpointsoverHOGalone,whileHSC\nimprovesoverHOGby4mAPpoints(whencompared\ninternallytotheirprivateDPMbaselinesbothusenon-\npublicimplementationsofDPMthatunderperformtheopen\nsourceversion[\n17\n]).ThesemethodsachievemAPsof\n29.1%and34.3%,respectively.\n3.3.Detectionerroranalysis\nWeappliedtheexcellentdetectionanalysistoolfrom\nHoiem\netal\n.[\n20\n]inordertorevealourmethod'serror\nmodes,understandhowchangesthem,andto\nseehowourerrortypescomparewithDPM.Afullsum-\nmaryoftheanalysistoolisbeyondthescopeofthispa-\nperandweencouragereaderstoconsult[\n20\n]tounderstand\nsomedetails(suchasnormalizedAP).Sincethe\nanalysisisbestabsorbedinthecontextoftheassociated\nplots,wepresentthediscussionwithinthecaptionsofFig-\nure\n4\nandFigure\n5\n.\nFigure4:Distributionoftop-rankedfalsepositive(FP)types.\nEachplotshowstheevolvingdistributionofFPtypesasmoreFPs\nareconsideredinorderofdecreasingscore.EachFPiscatego-\nrizedinto1of4types:Locpoorlocalization(adetectionwith\nanIoUoverlapwiththecorrectclassbetween0.1and0.5,oradu-\nplicate);Simconfusionwithasimilarcategory;Othconfusion\nwithadissimilarobjectcategory;BGaFPthatonback-\nground.ComparedwithDPM(see[\n20\n]),moreof\nourerrorsresultfrompoorlocalization,ratherthanconfusionwith\nbackgroundorotherobjectclasses,indicatingthattheCNNfea-\nturesaremuchmorediscriminativethanHOG.Looselocaliza-\ntionlikelyresultsfromouruseofbottom-upregionproposalsand\nthepositionalinvariancelearnedfrompre-trainingtheCNNfor\nwhole-imageColumnthreeshowshowoursimple\nboundingboxregressionmethodesmanylocalizationerrors.\n3.4.Boundingboxregression\nBasedontheerroranalysis,weimplementedasimple\nmethodtoreducelocalizationerrors.Inspiredbythebound-\ningboxregressionemployedinDPM[\n14\n],wetrainalinear\nregressionmodeltopredictanewdetectionwindowgiven\nthepool\n5\nfeaturesforaselectivesearchregionproposal.\nFulldetailsaregiveninthesupplementarymaterial.Re-\nsultsinTable\n1\n,Table\n2\n,andFigure\n4\nshowthatthissimple\napproachesalargenumberofmislocalizeddetections,\nboostingmAPby3to4points.\n4.Semanticsegmentation\nRegionisastandardtechniqueforseman-\nticsegmentation,allowingustoeasilyapplyR-CNNtothe\n"b"Figure5:Sensitivitytoobjectcharacteristics.\nEachplotshowsthemean(overclasses)normalizedAP(see[\n20\n])forthehighestand\nlowestperformingsubsetswithinsixdifferentobjectcharacteristics(occlusion,truncation,boundingboxarea,aspectratio,viewpoint,part\nvisibility).Weshowplotsforourmethod(R-CNN)withandwithout(FT)andboundingboxregression(BB)aswellasfor\nDPMvoc-release5.Overall,doesnotreducesensitivity(thedifferencebetweenmaxandmin),butdoessubstantiallyimprove\nboththehighestandlowestperformingsubsetsfornearlyallcharacteristics.Thisindicatesthatdoesmorethansimplyimprove\nthelowestperformingsubsetsforaspectratioandboundingboxarea,asonemightconjecturebasedonhowwewarpnetworkinputs.\nInstead,improvesrobustnessforallcharacteristicsincludingocclusion,truncation,viewpoint,andpartvisibility.\nPASCALVOCsegmentationchallenge.Tofacilitateadi-\nrectcomparisonwiththecurrentleadingsemanticsegmen-\ntationsystem(calledO\n2\nPforsecond-orderpooling)[\n4\n],\nweworkwithintheiropensourceframework.O\n2\nPuses\nCPMCtogenerate150regionproposalsperimageandthen\npredictsthequalityofeachregion,foreachclass,using\nsupportvectorregression(SVR).Thehighperformanceof\ntheirapproachisduetothequalityoftheCPMCregions\nandthepowerfulsecond-orderpoolingofmultiplefeature\ntypes(enrichedvariantsofSIFTandLBP).Wealsonote\nthatFarabet\netal\n.[\n13\n]recentlydemonstratedgoodresults\nonseveraldensescenelabelingdatasets(notincludingPAS-\nCAL)usingaCNNasamulti-scaleper-pixel.\nWefollow[\n2\n,\n4\n]andextendthePASCALsegmentation\ntrainingsettoincludetheextraannotationsmadeavailable\nbyHariharan\netal\n.[\n19\n].Designdecisionsandhyperparam-\neterswerecross-validatedontheVOC2011validationset.\nFinaltestresultswereevaluatedonlyonce.\nCNNfeaturesforsegmentation.\nWeevaluatethreestrate-\ngiesforcomputingfeaturesonCPMCregions,allofwhich\nbeginbywarpingtherectangularwindowaroundthere-\ngionto\n227\n\n227\n.Thestrategy(\nfull\n)ignoresthere-\ngion'sshapeandcomputesCNNfeaturesdirectlyonthe\nwarpedwindow,exactlyaswedidfordetection.However,\nthesefeaturesignorethenon-rectangularshapeofthere-\ngion.Tworegionsmighthaveverysimilarboundingboxes\nwhilehavingverylittleoverlap.Therefore,thesecondstrat-\negy(\nfg\n)computesCNNfeaturesonlyonaregion'sfore-\ngroundmask.Wereplacethebackgroundwiththemean\ninputsothatbackgroundregionsarezeroaftermeansub-\ntraction.Thethirdstrategy(\nfull+fg\n)simplyconcatenates\nthe\nfull\nand\nfg\nfeatures;ourexperimentsvalidatetheircom-\nplementarity.\nResultsonVOC2011.\nTable\n3\nshowsasummaryofour\nresultsontheVOC2011validationsetcomparedwithO\n2\nP.\n(Seesupplementarymaterialforcompleteper-categoryre-\nsults.)Withineachfeaturecomputationstrategy,layerfc\n6\nfull\nR-CNN\nfg\nR-CNN\nfull+fg\nR-CNN\nO\n2\nP[\n4\n]\nfc\n6\nfc\n7\nfc\n6\nfc\n7\nfc\n6\nfc\n7\n46.4\n43.0\n42.5\n43.7\n42.1\n47.9\n45.8\nTable3:Segmentationmeanaccuracy(%)onVOC2011vali-\ndation.\nColumn1presentsO\n2\nP;2-7useourCNNpre-trainedon\nILSVRC2012.\nalwaysoutperformsfc\n7\nandthefollowingdiscussionrefers\ntothefc\n6\nfeatures.The\nfg\nstrategyslightlyoutperforms\nfull\n,\nindicatingthatthemaskedregionshapeprovidesastronger\nsignal,matchingourintuition.However,\nfull+fg\nachieves\nanaverageaccuracyof47.9%,ourbestresultbyamar-\nginof4.2%(alsomodestlyoutperformingO\n2\nP),indicating\nthatthecontextprovidedbythe\nfull\nfeaturesishighlyinfor-\nmativeevengiventhe\nfg\nfeatures.Notably,trainingthe20\nSVRsonour\nfull+fg\nfeaturestakesanhouronasinglecore,\ncomparedto10+hoursfortrainingonO\n2\nPfeatures.\nInTable\n4\nwepresentresultsontheVOC2011test\nset,comparingourbest-performingmethod,fc\n6\n(\nfull+fg\n),\nagainsttwostrongbaselines.Ourmethodachievesthehigh-\nestsegmentationaccuracyfor11outof21categories,and\nthehighestoverallsegmentationaccuracyof47.9%,aver-\nagedacrosscategories(butlikelytieswiththeO\n2\nPresult\nunderanyreasonablemarginoferror).Stillbetterperfor-\nmancecouldlikelybeachievedby\n5.Conclusion\nInrecentyears,objectdetectionperformancehadstag-\nnated.Thebestperformingsystemswerecomplexen-\nsemblescombiningmultiplelow-levelimagefeatureswith\nhigh-levelcontextfromobjectdetectorsandsceneclassi-\nThispaperpresentsasimpleandscalableobjectde-\ntectionalgorithmthatgivesa30%relativeimprovement\noverthebestpreviousresultsonPASCALVOC2012.\nWeachievedthisperformancethroughtwoinsights.The\nistoapplyhigh-capacityconvolutionalneuralnet-\nworkstobottom-upregionproposalsinordertolocalize\nandsegmentobjects.Thesecondisaparadigmfortrain-\n"b"VOC2011test\nbg\naerobikebirdboatbottlebuscarcatchaircowtabledoghorsembikepersonplantsheepsofatraintv\nmean\nR&P[\n2\n]\n83.4\n46.818.936.631.242.757.347.444.18.139.4\n36.1\n36.349.548.350.726.347.222.142.043.2\n40.8\nO\n2\nP[\n4\n]\n85.4\n69.7\n22.345.2\n44.4\n46.966.757.856.2\n13.546.1\n32.341.2\n59.1\n55.351.0\n36.2\n50.4\n27.8\n46.9\n44.6\n47.6\nours\n(\nfull+fg\nR-CNNfc\n6\n)\n84.2\n66.9\n23.758.3\n37.4\n55.473.358.756.5\n9.745.529.5\n49.3\n40.1\n57.853.9\n33.8\n60.7\n22.7\n47.1\n41.3\n47.9\nTable4:Segmentationaccuracy(%)onVOC2011test.\nWecompareagainsttwostrongbaselines:theRegionsandParts(R&P)\nmethodof[\n2\n]andthesecond-orderpooling(O\n2\nP)methodof[\n4\n].WithoutanyourCNNachievestopsegmentationperfor-\nmance,outperformingR&PandroughlymatchingO\n2\nP.\ninglargeCNNswhenlabeledtrainingdataisscarce.We\nshowthatitishighlyeffectivetopre-trainthenetwork\nwithsupervision\nforaauxiliarytaskwithabundantdata\n(imageandthentothenetworkfor\nthetargettaskwheredataisscarce(detection).Weconjec-\nturethatthesupervised\ntuningparadigmwillbehighlyeffectiveforavarietyof\ndata-scarcevisionproblems.\nWeconcludebynotingthatitisthatwe\nachievedtheseresultsbyusingacombinationofclassi-\ncaltoolsfromcomputervision\nand\ndeeplearning(bottom-\nupregionproposalsandconvolutionalneuralnetworks).\nRatherthanopposinglinesofinquiry,thetwoare\nnaturalandinevitablepartners.\nAcknowledgments.\nThisresearchwassupportedinpart\nbyDARPAMind'sEyeandMSEEprograms,byNSF\nawardsIIS-0905647,IIS-1134072,andIIS-1212798,\nMURIN000014-10-1-0933,andbysupportfromToyota.\nTheGPUsusedinthisresearchweregenerouslydonated\nbytheNVIDIACorporation.\nReferences\n[1]\nB.Alexe,T.Deselaers,andV.Ferrari.Measuringtheobjectnessof\nimagewindows.\nTPAMI\n,2012.\n[2]\nP.Arbel\n\naez,B.Hariharan,C.Gu,S.Gupta,L.Bourdev,andJ.Malik.\nSemanticsegmentationusingregionsandparts.In\nCVPR\n,2012.\n[3]\nP.Arbel\n\naez,J.Pont-Tuset,J.Barron,F.Marques,andJ.Malik.Mul-\ntiscalecombinatorialgrouping.In\nCVPR\n,2014.\n[4]\nJ.Carreira,R.Caseiro,J.Batista,andC.Sminchisescu.Semantic\nsegmentationwithsecond-orderpooling.In\nECCV\n,2012.\n[5]\nJ.CarreiraandC.Sminchisescu.CPMC:Automaticobjectsegmen-\ntationusingconstrainedparametricmin-cuts.\nTPAMI\n,2012.\n[6]\nD.Ciresan,A.Giusti,L.Gambardella,andJ.Schmidhuber.Mi-\ntosisdetectioninbreastcancerhistologyimageswithdeepneural\nnetworks.In\nMICCAI\n,2013.\n[7]\nN.DalalandB.Triggs.Histogramsoforientedgradientsforhuman\ndetection.In\nCVPR\n,2005.\n[8]\nT.Dean,M.A.Ruzon,M.Segal,J.Shlens,S.Vijayanarasimhan,\nandJ.Yagnik.Fast,accuratedetectionof100,000objectclasseson\nasinglemachine.In\nCVPR\n,2013.\n[9]\nJ.Deng,A.Berg,S.Satheesh,H.Su,A.Khosla,and\nL.Fei-Fei.ImageNetLargeScaleVisualRecognitionCompe-\ntition2012(ILSVRC2012).\nhttp://www.image-net.org/\nchallenges/LSVRC/2012/\n.\n[10]\nJ.Deng,W.Dong,R.Socher,L.-J.Li,K.Li,andL.Fei-Fei.Ima-\ngeNet:Alarge-scalehierarchicalimagedatabase.In\nCVPR\n,2009.\n[11]\nI.EndresandD.Hoiem.Categoryindependentobjectproposals.In\nECCV\n,2010.\n[12]\nM.Everingham,L.VanGool,C.K.I.Williams,J.Winn,andA.Zis-\nserman.ThePASCALVisualObjectClasses(VOC)Challenge.\nIJCV\n,2010.\n[13]\nC.Farabet,C.Couprie,L.Najman,andY.LeCun.Learninghierar-\nchicalfeaturesforscenelabeling.\nTPAMI\n,2013.\n[14]\nP.Felzenszwalb,R.Girshick,D.McAllester,andD.Ramanan.\nObjectdetectionwithdiscriminativelytrainedpartbasedmodels.\nTPAMI\n,2010.\n[15]\nS.Fidler,R.Mottaghi,A.Yuille,andR.Urtasun.Bottom-upseg-\nmentationfortop-downdetection.In\nCVPR\n,2013.\n[16]\nK.Fukushima.Neocognitron:Aself-organizingneuralnetwork\nmodelforamechanismofpatternrecognitionunaffectedbyshift\ninposition.\nBiologicalcybernetics\n,36(4):193202,1980.\n[17]\nR.Girshick,P.Felzenszwalb,andD.McAllester.Discriminatively\ntraineddeformablepartmodels,release5.\nhttp://www.cs.\nberkeley.edu/\n\nrbg/latent-v5/\n.\n[18]\nC.Gu,J.J.Lim,P.Arbel\n\naez,andJ.Malik.Recognitionusingre-\ngions.In\nCVPR\n,2009.\n[19]\nB.Hariharan,P.Arbel\n\naez,L.Bourdev,S.Maji,andJ.Malik.Seman-\nticcontoursfrominversedetectors.In\nICCV\n,2011.\n[20]\nD.Hoiem,Y.Chodpathumwan,andQ.Dai.Diagnosingerrorin\nobjectdetectors.In\nECCV\n.2012.\n[21]\nY.Jia.Caffe:Anopensourceconvolutionalarchitectureforfast\nfeatureembedding.\nhttp://caffe.berkeleyvision.org/\n,\n2013.\n[22]\nA.Krizhevsky,I.Sutskever,andG.Hinton.ImageNet\nwithdeepconvolutionalneuralnetworks.In\nNIPS\n,2012.\n[23]\nY.LeCun,B.Boser,J.Denker,D.Henderson,R.Howard,W.Hub-\nbard,andL.Jackel.Backpropagationappliedtohandwrittenzipcode\nrecognition.\nNeuralComp.\n,1989.\n[24]\nY.LeCun,L.Bottou,Y.Bengio,andP.Haffner.Gradient-based\nlearningappliedtodocumentrecognition.\nProc.oftheIEEE\n,1998.\n[25]\nJ.J.Lim,C.L.Zitnick,andP.Doll\n\nar.Sketchtokens:Alearned\nmid-levelrepresentationforcontourandobjectdetection.In\nCVPR\n,\n2013.\n[26]\nD.Lowe.Distinctiveimagefeaturesfromscale-invariantkeypoints.\nIJCV\n,2004.\n[27]\nX.RenandD.Ramanan.Histogramsofsparsecodesforobjectde-\ntection.In\nCVPR\n,2013.\n[28]\nH.A.Rowley,S.Baluja,andT.Kanade.Neuralnetwork-basedface\ndetection.\nTPAMI\n,1998.\n[29]\nP.Sermanet,K.Kavukcuoglu,S.Chintala,andY.LeCun.Pedestrian\ndetectionwithunsupervisedmulti-stagefeaturelearning.In\nCVPR\n,\n2013.\n[30]\nK.SungandT.Poggio.Example-basedlearningforview-basedhu-\nmanfacedetection.TechnicalReportA.I.MemoNo.1521,Mas-\nsachussetsInstituteofTechnology,1994.\n[31]\nC.Szegedy,A.Toshev,andD.Erhan.Deepneuralnetworksfor\nobjectdetection.In\nNIPS\n,2013.\n[32]\nJ.Uijlings,K.vandeSande,T.Gevers,andA.Smeulders.Selective\nsearchforobjectrecognition.\nIJCV\n,2013.\n[33]\nR.Vaillant,C.Monrocq,andY.LeCun.Originalapproachforthe\nlocalisationofobjectsinimages.\nIEEProconVision,Image,and\nSignalProcessing\n,1994.\n[34]\nC.Vondrick,A.Khosla,T.Malisiewicz,andA.Torralba.HOG-\ngles:visualizingobjectdetectionfeatures.\nICCV\n,2013.\n[35]\nX.Wang,M.Yang,S.Zhu,andY.Lin.Regionletsforgenericobject\ndetection.In\nICCV\n,2013.\n[36]\nM.Zeiler,G.Taylor,andR.Fergus.Adaptivedeconvolutionalnet-\nworksformidandhighlevelfeaturelearning.In\nCVPR\n,2011.\n"