b'LinearSpatialPyramidMatchingUsingSparseCoding\nforImageClassication\nJianchaoYang\n,KaiYu\n,YihongGong\n,ThomasHuang\nBeckmanInstitute,UniversityofIllinoisatUrbana-Champaign\nNECLaboratoriesAmeri\nca,Cupertino,CA95014,USA\n{jyang29,huang\n}@ifp.uiuc.edu,\n{kyu,ygong\n}@sv.nec-lab.com\nAbstractRecentlySVMsusingspatialp\nyramidmatching(SPM)\nkernelhavebeenhighlysuccessfulinimageclassication.\nDespiteitspopularity,thesenonlinearSVMshaveacom-\nplexity\nO(n2n3)intrainingand\nO(n)intesting,where\nnisthetrainingsize,implyingthatitisnontrivialtoscale-\nupthealgorithmstohandlemorethanthousandsoftraining\nimages.InthispaperwedevelopanextensionoftheSPM\nmethod,bygeneralizingvectorquantizationtosparsecod-\n\ningfollowedbymulti-scalespatialmaxpooling,andpro-\nposealinearSPMkernelbasedonSIFTsparsecodes.This\nnewapproachremarkablyreducesthecomplexityofSVMs\ntoO(n)intrainingandaconstantintesting.Inanum-\nberofimagecategorizationexperiments,wendthat,in\ntermsofclassicationaccuracy,thesuggestedlinearSPM\nbasedonsparsecodingofSIFTdescriptorsalwayssignif-\nicantlyoutperformsthelinearSPMkernelonhistograms,\n\nandisevenbetterthanthenonlinearSPMkernels,leading\ntostate-of-the-artperformanceonseveralbenchmarksby\nusingasingletypeofdescriptors.\n1.Introduction\nInrecentyearsthe\nbag-of-features\n(BoF)modelhasbeen\nextremelypopularinimagecategorization.Themethod\n\ntreatsanimageasacollectionofunorderedappearance\ndescriptorsextractedfromlocalpatches,quantizesthem\nintodiscretevisualwords,andthencomputesacompact\n\nhistogramrepresentationforsemanticimageclassication,\ne.g.objectrecognitionorscenecategorization.\nTheBoFapproachdiscardsthespatialorderoflocalde-\nscriptors,whichseverelylimitsthedescriptivepowerofthe\n\nimagerepresentation.Byovercomingthisproblem,one\nparticularextensionoftheBoFmodel,called\nspatialpyra-\nmidmatching\n(SPM)[\n12],hasmadearemarkablesuccess\nonarangeofimageclassicatio\nnbenchmarkslikeCaltech-\n101[\n14]andCaltech-256[\n8],andwasthemajorcompo-\nFigure1.SchematiccomparisonoftheoriginalnonlinearSPM\nwithourproposedlinearSPMbasedonsparsecoding(ScSPM).\nTheunderlyingspatialpoolingfunctionfornonlinearSPMis\nav-eraging\n,whilethespatialpoolingfunctioninScSPMis\nmaxpool-\ning\n.nentofthestate-of-the-artsystems,e.g.,[\n2].Themethod\npartitionsanimageinto\n2l2lsegmentsindifferentscales\nl=0,1,2,computestheBoFhistogramwithineachofthe\n21segments,andnallyconcat\nenatesallthehistogramsto\nformavectorrepresentationoftheimage.Incasewhere\nonlythescale\nl=0isused,SPMreducestoBoF.\nPeoplehaveempiricallyfoundthat,inordertoobtain\ngoodperformances,bothBoFandSPMmustbeappliedto-\ngetherwithaparticulartypeofnonlinearMercerkernels,\ne.g.the\nintersectionkernel\northe\nChi-squarekernel\n.Ac-\ncordingly,thenonlinearSVMhastopayacomputational\ncomplexity\nO(n3)andamemorycomplexity\nO(n2)inthe\ntrainingphase,where\nnisthetrainingsize.Furthermore,\nsincethenumberofsupportvectorsgrowslinearlywith\nn,thecomputationalcomplexityintestingis\nO(n).Thisscal-\nabilityimpliesaseverelimitationitisnontrivialtoapply\nthemtoreal-worldapplications,whosetrainingsizeistypi-\ncallyfarbeyondthousands.\n11794978-1-4244-3991-1/09/$25.00 2009 IEEE'b'Inthispaper,weproposeanextensionoftheSPMap-\nproach,whichcomputesaspatial-pyramidimagerepresen-\ntationbasedon\nsparsecodes\n(SC)ofSIFTfeatures,instead\noftheK-meansvectorquantization(VQ)inthetraditional\n\nSPM.Theapproachisnaturallyderivedbyrelaxingthere-\nstrictivecardinalityconstraintofVQ.Furthermore,unlike\ntheoriginalSPMthatperformsspatialpoolingbycomput-\ninghistograms,ourapproach,calledScSPM,uses\nmaxspa-\ntialpoolingthatismorerobusttolocalspatialtranslations\n\nandmorebiologicalplausible[\n24].Thenewimagerepre-\nsentationcapturesmoresalientpropertiesofvisualpatterns,\nandturnsouttoworksurprisinglywellwith\nlinear\nclassi-ers.OurapproachusingsimplelinearSVMsdramatically\nreducesthetrainingcomplexityto\nO(n),andobtainsacon-\nstantcomplexityintesting,whilestillachievinganeven\nbetterclassicationaccuracy\nincomparisonwiththetra-\nditionalnonlinearSPMappro\nach.Schematiccomparison\nbetweentheoriginalSPMwithScSPMisshowninFig.\n1.Therestofthepaperisorganizedasfollows.InSec.\n2wewilltalkaboutsomerelatedworks.Sec.\n3presentsthe\nframeworkofourproposedalgorithmandwegiveouref-\ncientimplementationinSec.\n4,followedbyexperiment\nresultsinSec.\n5.Finally,Sec.\n6concludesourpaper.\n2.RelatedWork\nOvertheyearsmanyworkshavebeendonetoim-\nprovethetraditionalBoFmodel,suchasgenerativemeth-\nodsin[\n7,21,3,1]formodelingtheco-occurrenceofthe\ncodewordsordescriptors,discriminativecodebooklearning\n\nin[\n10,5,19,27]insteadofstandardunsupervisedK-means\nclustering,andspatialpyramidmatchingkernel(SPM)[\n12]formodelingthespatiallayoutofthelocalfeatures,all\nbringingpromisingprogress.Amongtheseextensions,mo-\ntivatedbyGraumanandDarrellspyramidmatchinginthe\n\nfeaturespace,theSPMproposedbyLazebnik\netal.\nispar-\nticularsuccessful.\nAsbeingeasyandsimpletoconstruct,theSPMker-\nnelturnsouttobehighlyeffectiveinpractice.Itcon-\ntributesasthemajorcomponenttothestate-of-the-artsys-\ntems,e.g.,[\n2],andthesystemsofthetopperformersin\nPASCALChallenge2008[\n6].Despiteofsuchapopular-\nity,SPMhastoruntogetherwithnonlinearkernels,such\n\nastheintersectionkernelandtheChi-squarekernel,inor-\ndertoachieveagoodperformance,whichrequiresinten-\nsivecomputationandalargestorage.Realizingthis,Anna\n\nBosch\netal.\n[2]usedrandomizedtreesinsteadofSVMsfor\nfastertrainingandtesting.Mostrecently,Maji\netal.\n[16]showedthatonecanbuildhistogramintersectionkernel\nSVMsmuchefciently.However,theefciencycomes\nonlyforpre-trainednonlinearSVMs.Inrealapplications\n\nwhichinvolvesmorethantensofthousandsoftrainingex-\namples,linearkernelSVMsarefarmorefavoredasthey\nenjoybothmuchfastertrainingandtestingspeeds,withsig-\nnicantlylessmemoryrequirementscomparedtononlinear\nkernels.Therefore,ourproposedlinearSPMusingSIFT\nsparsecodesisverypromisinginrealapplications.\nSparsemodelingofimagepatcheshasbeensuccessfully\nappliedtotaskssuchasimageandvideodenoising,in-\n\npainting,demosaicing,super-resolution[\n5,17,26]andseg-\nmentation[\n18].Therearealreadysomeworksdevoting\ntoimagecategorizationthroughsparsecodingonrawim-\nagepatches[\n23,22].However,theirperformancesarestill\nbehindthestate-of-the-artachievedby[\n12,1,9]onpub-\nlicbenchmarks.Ourapproachdiffersfromthematusing\nsparsecodingonappearancedescriptorslikeSIFTfeatures,\nandthedevelopmentofthewholesystemthatachieves\n\nstate-of-the-art\nperformancesonseveralbenchmarks.\n3.LinearSPMUsingSIFTSparseCodes\n3.1.EncodingSIFT:FromVQtoSC\nLetXbeasetofSIFTappearancedescriptorsina\nD-dimensionalfeaturespace,i.e.\nX=[x1,...,xM]RMD.Thevectorquantization(VQ)methodappliesthe\nK-meansclusteringalgorithmtosolvethefollowingprob-\nlemminVMm=1mink=1...K\nxmvk2(1)whereV=[v1,...,vK]arethe\nKclustercenterstobe\nfound,called\ncodebook,and\n\ndenotesthe\nL2-normof\nvectors.Theoptimizationproblemcanbere-formulated\nintoamatrixfactorizationproblemwithclustermember-\nshipindicators\nU=[u1,...,\nuM],minU,VMm=1xmumV2(2)subjectto\nCard\n(um)=1\n,|um|=1,um0,mwhereCard\n(um)=1isacardinalityconstraint,meaning\nthatonlyoneelementof\numisnonzero,\num0meansthatalltheelementsof\numarenonnegative,and\n|um|isthe\nL1-normof\num,thesummationoftheabsolutevalue\nofeachelementin\num.Aftertheoptimization,theindex\noftheonlynonzeroelementin\numindicateswhichcluster\nthevector\nxmbelongsto.InthetrainingphaseofVQ,the\noptimizationEq.(\n2)issolvedwithrespecttoboth\nUandV.Inthecodingphase,thelearned\nVwillbeappliedfora\nnewsetof\nXandEq.(\n2)willbesolvedwithrespectto\nUonly.\nTheconstraint\nCard\n(um)=1maybetoorestrictive,\ngivingrisetooftenacoarsereconstructionof\nX.Wecan\nrelaxtheconstraintbyinsteadputtinga\nL1-normregular-\nizationon\num,whichenforces\numtohaveasmallnumber\n1795'b'ofnonzeroelements.ThentheVQformulationisturned\nintoanotherproblemknownas\nsparsecoding\n(SC):\nminU,VMm=1xmumV2+|um|(3)subjectto\nvk1,k=1,2,...,K\nwhereaunit\nL2-normconstrainton\nvkistypicallyap-\npliedtoavoidtrivialsolutions\n1.Normally,thecodebook\nVisan\novercomplete\nbasisset,i.e.\nK>D\n.Notethatwedrop\noutthenonnegativityconstraint\num0aswell,because\nthesignof\numisnotessentialitcanbeeasilyabsorbed\nbyletting\nV[V,V]andum[um+,um]sothattheconstraintcanbetriviallysatised,where\num+=min(0,um)andum=max(0\n,um).SimilartoVQ,SChasatrainingphaseandacoding\nphase.First,adescriptorset\nXfromarandomcollection\nofimagepatchesisusedtosolveEq.(\n3)withrespectto\nUandV,where\nVisretainedasthecodebook;Inthecoding\nphase,foreachimagerepresentedasadescriptorset\nX,the\nSCcodesareobtainedbyoptimizingEq.(\n3)withrespectto\nUonly.\nWechooseSCtoderiveimagerepresentationsbecauseit\nhasanumberofattractiveproperties.First,comparedwith\ntheVQcoding,SCcodingcanachieveamuchlowerrecon-\n\nstructionerrorduetothelessrestrictiveconstraint;Second,\nsparsityallowstherepresent\nationtobespecialize,andto\ncapturesalientpropertiesofimages;Third,researchinim-\nagestatisticsclearlyrevealsthatimagepatchesaresparse\nsignals.\n3.2.LinearSPM\nForanyimagerepresentedbyasetofdescriptors,wecan\ncomputeasinglefeaturevectorbasedon\nsomestatistics\nofthedescriptorscodes.Forexample,if\nUisobtainedvia\nEq.(\n2),apopularchoiceistocomputethehistogram\nz=1MMm=1um(4)Thebag-of-wordsapproachto\nimageclassicationcom-\nputessuchahistogram\nzforeachimage\nIrepresentedbyan\nunorderedsetoflocaldescriptors.Inthemoresophisticated\nSPMapproach,theimagesspatialpyramidhistogramrep-\nresentation\nzisaconcatenationoflo\ncalhistogramsinvari-\nouspartitionsofdifferentscales.Afternormalization\nzcanbeseenasagainahistogram.Let\nzidenotethehistogram\nrepresentationforimage\nIi.Forabinaryimageclassica-\ntionproblem,anSVMaimstolearnadecisionfunction\nf(z)=ni=1i(z,zi)+b(5)1Forexample,theobjectivecanbedecreasedbyrespectivelydividing\nandmultiplying\numandVbyaconstantfactor.\nFigure2.Theillustrationarchitectureofouralgorithmbasedon\nsparsecoding.Sparsecodingmeasurestheresponsesofeachlocal\ndescriptortothedictionarysvisualelements.Theseresponses\narepooledacrossdifferentspatiallocationsoverdifferentspatial\nscales.\nwhere{(zi,yi)}ni=1isthetrainingset,and\nyi\n1,+1}indicateslabels.Foratestimagerepresentedby\nz,if\nf(z)>0thentheimageisclassiedaspositive,otherwise\nasnegative.Intheory\n(,)canbeanyreasonableMer-\ncerkernelfunction,butinpracticetheintersectionkernel\nandChi-squarekernelhavebeenfoundthemostsuitableon\nhistogramrepresentations.Ourexperimentshowsthatlin-\n\nearkernelonhistogramsleadstoalwayssubstantiallyworse\nresults,partiallyduetothehighquantizationerrorofVQ.\nHowever,usingthesetwononlinearkernels,theSVMhas\ntopayahightrainingcost,i.e.\nO(n3)incomputation,and\nO(n2)instorage(forthe\nnnkernelmatrix).Thismeans\nthatitisdifculttoscaleupthealgorithmtothecasewhere\nnismorethantensofthousands.Furthermore,asthenum-\nberofsupportvectorsscaleslinearlytothetrainingsize,the\n\ntestingcostis\nO(n).Inthispaperweadvocateanapproachofusing\nlinear\nSVMsbasedSCofSIFT.Let\nUbetheresultofapplying\nthesparsecodingEq.(\n3)toadescriptorset\nX,assuming\nthecodebook\nVtobepre-learnedandxed,wecomputethe\nfollowingimagefeaturebyapre-chosenpoolingfunction\nz=F(U),(6)wherethepoolingfunction\nFisdenedoneachcolumnof\nU.Recallthateachcolumnof\nUcorrespondstothere-\nsponsesofallthelocaldescriptorstoonespecicitemin\ndictionary\nV.Therefore,differentpoolingfunctionscon-\nstructdifferent\nimagestatistics\n.Forexample,in\n4,theun-\nderlyingpoolingfunctionisdenedasthe\naveraging\nfunc-tion,yieldingthehistogramfeature.Inthiswork,wede-\nnedthepoolingfunction\nFasa\nmaxpoolingfunctionon\ntheabsolutesparsecodes\nzj=max\n{|u1j|,|u2j|,...,\n|u2M|},(7)wherezjisthej-thelementof\nz,uijisthematrixelement\nati-throwandj-thcolumnof\nU,and\nMisthenumberof\n1796'b'localdescriptorsintheregion.Thismaxpoolingproce-\ndureiswellestablishedbybiophysicalevidenceinvisual\ncortex(V1)[\n24]andisempiricallyjustiedbymanyalgo-\nrithmsappliedtoimagecategorization.Inourcase,wealso\n\nndthatmaxpoolingoutperformsotheralternativepooling\nmethods(seeSec.\n5.5.4).SimilartotheconstructionofhistogramsinSPM,wedo\nmaxpoolingEq.(\n7)onaspatialpyramidconstructedfor\nanimage.Bymaxpoolingacrossdifferentlocationsand\n\noverdifferentspatialscalesoftheimage,thepooledfeature\nismorerobusttolocaltransformationsthanmeanstatistics\ninhistogram.Fig.\n2illustratesthewholestructureofour\nalgorithmbasedonsparsecoding.Thepooledfeaturesfrom\nvariouslocationsandscalesarethenconcatenatedtoform\naspatialpyramidrepresentationoftheimage.\nLetimage\nIiberepresentedby\nzi,weuseasimplelinear\nSPMkernel\n(zi,zj)=zizj=2l=02ls=12lt=1zli(s,t\n),zlj(s,t\n)(8)wherezi,zj=zizj,and\nzli(s,t\n)isthemeansquare\nstatisticsofdescriptorsinthe\n(s,t\n)-thsegmentofimage\nIiinthescalelevel\nl.ThenthebinarySVMdecisionfunction\nbecomesf(z)=ni=1iziz+b=wz+b(9)Intheliterature,Eq.(\n5)iscalledthe\ndualformulation\nofSVMs,whileEq.(\n9)isthe\nprimalformulation\n.Asthema-\njoradvantageofthelinearkernel,nowwecandirectlywork\nintheprimal,whichmeansthatthetrainingcostis\nO(n)incomputation,andthetestin\ngcostforeachimageiseven\nconstant!InSec.\n4.2,wewilldescribeourlarge-scaleim-\nplementationforbinaryandmulti-classlinearSVMs.\nDespitethatthelinearSPMkernelbasedonhistograms\nleadstoverypoorperformances,wendthatthelin-\nearSPMkernelbasedonsparsecodingstatisticsalways\n\nachievesexcellentclassicationaccuracy.Thissuccessis\nlargelyduetothreefactors:(1)SChasmuchlessquantiza-\ntionerrorsthanVQ;(2)Itiswel\nlknownthatimagepatches\naresparseinnature,andthussparsecodingisparticularly\nsuitableforimagedata;(3)Thecomputedstatisticsbymax\npoolingaremoresalientandrobusttolocaltranslations.\n4.Implementation\n4.1.SparseCoding\nTheoptimizationproblemEq.(\n3)isconvexin\nV(withUxed)andconvexin\nU(withVxed),butnotinboth\nsimultaneously.Theconventionalwayforsuchaproblemis\ntosolveititerativelybyalternatinglyoptimizingover\nVorUwhilexingtheother.Fixing\nV,theoptimizationcanbe\nsolvedbyoptimizingovereachcoefcient\numindividually:\nminumxmVum22+|um|.(10)Thisisessentiallyalinearregressionproblemwith\nL1normregularizationonthecoefcients,wellknownasLassoin\ntheStatisticalliterature.Theoptimizationcanbesolved\nveryefcientlybyalgorithmssuchastherecentlyproposed\n\nfeature-signsearch\nalgorithm.[\n13].Fixing\nU,theproblem\nreducestoaleastsquareproblemwithquadraticconstraints:\nminVXVU2Fs.t.vk1,k=1,2,...,K.\n(11)TheoptimizationcanbedoneefcientlybytheLagrange\ndualasusedin[\n13].Inourexperiments,weuse\n50,000SIFTdescriptorsex-\ntractedfromrandompatchestotrainthecodebook,byiter-\natingthestepsEq.(\n10)andEq.(\n11).Oncewegetthecode-\nbookVinthisoff-linetraining,wecandoon-linesparse\ncodingefcientlyasinEq.(\n10)oneachdescriptorofan\nimage.4.2.Multi-classLinearSVM\nWeintroduceasimpleimplementationoflinearSVMs\nthatwasusedinourexperiments.Giventhetrainingdata\n{(zi,yi)}ni=1,yi={1,...,L\n},alinearSVMaimsto\nlearnLlinearfunctions\n{wcz|c}\n,suchthat,foratest\ndatum\nz,itsclasslabelispredictedby\n2y=max\ncwcz(12)Wetakeaone-against-allstrategytotrain\nLbinarylinear\nSVMs,eachsolvingthefollo\nwingunconstraintconvexop-\ntimizationproblem\nminwcJ(wc)=wc2+Cni=1(wc;yci,zi)(13)whereyci=1ifyi=c,otherwise\nyci=1,and\n(wc;yci,zi)isahingelossfunction.Thestandardhinge\nlossfunctionisnotdifferentiableeverywhere,whichham-\nperstheuseofgradient-basedoptimizationmethods.Here\nweadoptadifferentiablequadratichingeloss,\n(wc;yci,zi)=max\n0,wczyci12suchthatthetrainingcanbeeasilydonewithsimple\ngradient-basedoptimizationmethods.Inourworkweused\n2Themoregeneralformoflinearfunctions,i.e.\nf(z)=wz+b,canstillbewrittenas\nf(z)=wzbyadoptingthereparameterization\nw[w,b]andz[z,1].1797'b'LBFGS.Otherchoiceslikeconjugategradientarealsoap-\nplicable.Theonlyimplementationonoursideisproviding\nthecost\nJ(w)andthegradient\n(w)w.Thecomputa-\ntionlinearlyscansoverthetrainingexamplesandthushas\n\nthe\nlinear\ncomplexity\nO(n).InourexperimentinSec.\n5.4,theSVMtrainingonabout\n200,000exampleswith\n5376-dimensionalfeatureswasusuallynishedin\n5minutes.\n5.ExperimentsandResults\nIntheexperiments,weimplementedandevaluatedthree\nclassesofSPMmethodsonfourdiversedatasets:Caltech\n101[\n14],Caltech256[\n8],15Scenes[\n12],andTRECVID\n2008surveillancevideo.Thethreemethodsare\n1.KSPM:thepopularnonlinearkernelSPMthatuses\nspatial-pyramidhistogramsandChi-squarekernels;\n2.LSPM:thesimplelinear\nSPMthatuseslinearkernel\nonspatial-pyramidhistograms;\n3.ScSPM:thelinearSPMthatuseslinearkernelon\nspatial-pyramidpoolingofSIFTsparsecodes,\nBesidesourownimplementations,wealsoquotesomere-\nsultsdirectlyfromtheliterature,especiallythoseofKSPM\n\nfrom[\n12]and[\n8].Wenotethatsometimeswecouldnotre-\nproducetheirresults,largelyduetosubtleengineeringde-\ntails,e.g.thewayofdealingwithhigh-contrastandlow-\n\ncontrastpatches.Itthusmakesmoresensetocompareour\nownimplementations,sincetheywerebasedonexactlythe\nsamesetofdescriptors.\nOurimplementationsusedasingledescriptortype,the\npopularSIFTdescriptor,\n3asin[\n12,1,9].TheSIFTde-\nscriptorsextractedfrom\n1616pixelpatchesweredensely\nsampledfromeachimageonagr\nidwithstepsize8pixels.\nTheimageswereallpreprocessedintograyscale.Totrain\nthecodebooks,weusedstandardK-meansclusteringfor\nKSPMandLSPM,andthesparsecodingschemeforour\n\nproposedScSPMalgorithm.Foralltheexperimentsex-\nceptTRECVID2008,wexedthecodebooksizeas512\nforLSPMand1024forScSPM,toachieveoptimalperfor-\n\nmancesforboth.Fortrainingthelinearclassiers,weused\nourimplementedSVMdescribedin\n4.2.TheKSPMwas\ntrainedusingtheLIBSVM[\n4]package.\nFollowingthecommonbenchmarkingprocedures,we\nrepeattheexperimentalprocessby10timeswithdifferent\n\nrandomselectedtrainingandtestingimagestoobtainreli-\nableresults.Theaverageofper-classrecognitionrateswere\nrecordedforeachrun.Andwer\neportournalresultsbythe\nmeanandstandarddeviationoftherecognitionrates.\n3Itisstraightforwardthatthea\npproachcanbegeneralizedtohandle\notherdescriptorsandalsomultipledescriptors.\n5.1.Caltech-101Dataset\nTheCaltech-101datasetcontains101classes(including\nanimals,vehicles,owers,et\nc.)withhighshapevariabil-\nity.Thenumberofimagespercategoryvariesfrom31\nto800.Mostimagesaremediumresolution,i.e.about\n300300pixels.Wefollowedthecommonexperiment\nsetupforCaltech-101,trainingon15and30imagespercat-\negoryandtestingontherest.Detailedcomparisonresults\nareshowninTable\n1.Asshown,oursparsecodingscheme\noutperformslinearSPMbymorethan14percent,andeven\noutperformthenonlinearSPM[\n12]byalargemargin(about\n11percentfor15trainingand9percentfor30trainingper\ncategory).OneworkneedstomentionistheKernelCode-\nbooks[\n25],wheretheauthorassignedeachdescriptorinto\nmultiplebinsinsteadofhardassignment.Thisschemegen-\nerallyimprovestheirbaselineSPMby\n56percent4.However,theirmethodisstillbasedonnonlinearkernels.\nTable1.Classicationrate(%)comparisononCaltech-101.\nAlgorithms15training30training\nZhangetal.[\n28]59.100.6066\n.200.50KSPM[\n12]56.40\n64.400.80NBNN[\n1]65.001.1470.40ML+CORR[\n9]61.0069.60\nKC[\n25]64.141.18KSPM56.440.7863\n.990.88LSPM53.230.6558\n.811.51ScSPM67.00.4573\n.20.545.2.Caltech-256Dataset\nTheCaltech-256datasetholds29,780imagesfallinginto\n256categorieswithmuchhigherintra-classvariabilityand\nhigherobjectlocationvariabilitycomparedwithCaltech-\n\n101.Eachcategorycontainsatleast80images.Wetried\nouralgorithmon15,30,45,and60trainingimagesper\nclassrespectively.Ther\nesultsareshowninTable\n2.Forall\nthecases,ourScSPMoutperformsLSPMbymorethan15\npercent,andoutperformsourownKSPMbymorethan4\n\npercent.Inthecasesof45and60trainingimagespercate-\ngory,KSPMwasnottriedduetoitsveryhighcomputation\ncostfortraining.\n5.3.15ScenesCategorization\nWealsotriedouralgorithmonthe15-Scenesdataset\ncompiledbyseveralresearchers[\n20,7,12].Thisdataset\ncontainstotally4485imagesfallinginto15categories,with\nthenumberofimageseachcategoryrangingfrom200to\n400.The15categoriesvaryfromlivingroomandkitchen\n4Becausethecodebookbaselinescoresarelower,theimprovedabso-\nluteperformanceobtainedbythekernelcodebookisnotashighasmaybe\n\nobtainedwithabetterbaseline\n1798'b'Table2.Classicationrate(%)comparisononCaltech-256dataset.\nAlgorithms15train30train45train60train\nKSPM[\n8]34.10\nKC[\n25]27.170.46KSPM23.340.4229\n.510.52LSPM13.200.6215\n.450.3716\n.370.4716\n.571.01ScSPM27.730.5134\n.020.3537\n.460.5540\n.140.91tostreetandindustrial.Followingthesameexperimentpro-\ncedureofLazebniketal.[\n12],wetook100imagesperclass\nfortrainingandusedtheleftfortesting.Thedetailedcom-\nparisonresultsareshowninTable\n3.Inthisexperiment,\nourimplementationofkernelSPMwasnotabletorepro-\nducetheresultsreportedin[\n12],probablyduetotheSIFT\ndescriptorextractionandnormalizationprocess.Follow-\n\ningourownbaseline,theLinearScSPMalgorithmagain\nachievesmuchbetterperformancethanKSPMandKC[\n25].Table3.Classicationrate(%)comparisonon15scenes.\nAlgorithmsClassicationRate\nKSPM[\n12]81.400.50KC[\n25]76.670.39KSPM76.730.65LSPM65.321.02ScSPM80.280.935.4.TRECVID2008Sur\nveillanceVideo\nFigure3.ExamplesofEventsinTRECVIDSurveillanceVideo\nThistime,wetriedouralgorithmonthelarge-scaledata\nof2008TRECVIDSurveillanceEventDetectionEvalua-\ntion,sponsoredbyNationalInstituteofStandardandTech-\n\nnology(NIST).Thedataare100hoursofsurveillance\nvideos,10hourseachday,fromLondonGatwickInterna-\ntionalAirport.NISTdened10classesofeventstodetect,\nandprovided50hoursofannotatedvideosfortraining,as\nwellastheother50hoursvideosfortesting.Theproposed\nalgorithmofthispaperwasoneofthemaincomponentsina\nsystemparticipatingin3tasksoftheevaluation,i.e.detect-\n\ning\nCellToEar\n,ObjectPut,and\nPointing\n,andbeingamong\nthetopperformers.Somesampleframesoftheseeventsare\nshowninFig.\n3.Inadditiontotheeventdurationannotated\nbyNIST,wemanuallymarkedthelocationsofpersonsper-\nformingthe3eventsofinterests.\nThetasksareextremelychallengingintwoaspects:\n(1)Thepeoplesubjectshaveahugedegreeofvariances\ninviewpointsandappearances,andarealwaysinhighly\ncrowedandclutteredenvironm\nents;(2)Thedetectionsys-\ntemhastoprocess\n9millionsof\n720576framesthe\ncomputationloadisfarbeyondmostoftheresearchefforts\nknownfromtheliterature.Tomakethecomputationaf-\nfordable,oursystemtookasimpleframe-basedapproach:\n\nrstusedahumandetectorto\ndetectpeoplesubjectsoneach\nframe,andthenappliedclassiersoneachdetectedregion\ntofurtherdetecttheeventsofinterest.Foreachofthe\n3events,wetrainedabinaryclassier.\nTable4.AUCcomparisononTREC\nVID2008surveillancevideo.\nAlgorithmsCellToEarObjectPutPointing\nLSPM0.6880\n.7140\n.744ScSPM0.7440\n.7730\n.769Sincethetrainingvideoswererecordedin\n5different\ndays,weused5-foldcrossvalidationtodevelopandevalu-\nateourmethods,whereeachfoldcorrespondedtooneday.\nIntotal,wegot\n2114,2172,and\n8725positiveexamplesof\nCellToEar\n,ObjectPut,and\nPointing\n,respectively,andabout\n200,000negativeexamples(onlyasmallsubset!)inthe\n\ntrainingset.Eachexamplewasacroppedimagecontaining\nadetectedhumansubjectwiththeannotatedevent,resized\nintoa\n100100image.Foreachexample,weextracted\nSIFTdescriptorsforevery\n1616patchesonagridof\nstepsize\n8.ThecodebooksizesofbothVQandSCwereset\ntobe\n256.NonlinearSVMdoesnotworkonsuchalarge-\nscaletrainingset,thereforeweonlycomparedthetwolinear\nmethods,ScSPMandLSPM.Duetotheextremelyunbal-\n\nancedclassdistribution,weusedROCcurves,aswellas\ntheAUC(areaunderROCcurve)scorestoevaluatetheac-\ncuracy.TheaverageAUCresultsover5foldsareshown\n1799'b'inTable\n4.Typically,theSVMtrainingonabout\n200,000exampleswith\n5376-dimensionalfeatureswasusuallyn-\nishedin\n5minutes.\n5.5.ExperimentRevisit\n5.5.1PatchSize\nInourexperiments,weonlyusedonepatchsizetotoextract\nSIFTdescriptors,namely,\n1616pixelsasinSPM[\n12].In\nNBNN[1],theyusedfourpatchscalestoextractthedescrip-\ntorsinordertoboosttheirperformance.Inourexperiments,\nwedidntobserveanysubstantialimprovementsbypooling\novermultiplepatchscales,probablybecausemaxpooling\noversparsecodescancaptureth\nesalientpropertiesoflocal\nregionsthatareirrelevanttothescaleoflocalpatches.\n5.5.2CodebookSize\nWealsoinvestigatedtheeffectsofcodebooksizesonthese\nSPMalgorithms.Intuitively,ifthecodebooksizeistoo\nsmall,thehistogramfeatureloosesdiscriminantpower;if\n\nthecodebooksizeistoolarge,thehistogramsfromthesame\nclassofimageswillnevermatch.InLazebniketal.swork,\ntheyusedtwocodebooksizes200and400andreportedthat\ntherewaslittledifference.InourexperimentsonScSPM\nandLSPM,wetriedthreesizes:256,512and1024.As\n\nshowninTable\n5,theperformanceforLSPMincreasesini-\ntiallyandthendecreasesasthecodebooksizegrowsfurther.\nTheperformanceforScSPMcontinuestoincreasewhenthe\n\ncodebooksizegoesupto1024.\nTable5.TheeffectsofcodebooksizeonScSPMandLSPMre-\nspectivelyonCaltech101dataset.\nCodebooksize2565121024\n30trainScSPM68.2671.20\n73.20LSPM57.42\n58.8158.5615trainScSPM61.9763.23\n69.70LSPM51.84\n53.2351.745.5.3SparseCodingParameter\nThereisonefreeparameter\nasinEq.(\n10)weneedto\ndeterminewhenwedosparsecodingoneachfeaturevec-\n\ntor.\nenforcesthesparsityofthesolution;thebigger\nis,moresparsethesolutionwillbe.Empirically,wefoundthat\nkeepingthesparsitytobearound\n10%yieldsgoodresults.\nForallourexperiments,wesimplyxed\ntobe\n0.30.4andthemeannumberofsupports(non-zerocoefcients)is\naround10.\n5.5.4ComparisonofPoolingMethods\nWealsostudiedtwootherstraightforwardpoolingmethods,\nnamely,thesquarerootofmeansquaredstatistics(Sqrt)and\nthemeanofabsolutevalues(Abs),incomparisonwith\nmaxpooling.Tobemoreprecise,theothertwopoolingmethods\naredenedas\nSqrt\n:zj=\n1MMi=1u2ijAbs\n:zj=1MMi=1|uij|,(14)wherethemeaningsofthenotationsarethesameasinEqn.\n7.ExperimentsusingthreepoolingmethodsonCaltech-\n101for30trainingpercategoriesand15Scenesfor100\ntrainingarelistedinTable\n6.Asshown,\nmaxpoolingpro-\nducesthebestperformance,probablyduetoitsrobustness\n\ntolocalspatialvariations.\nTable6.Theperformancecomparisonusingdifferentpooling\nmethodsonCaltech-101and15ScenesforScSPM.\nSqrtAbsMax\nCaltech\n71.091.4766\n.680.6673.20.54Scenes76.200.7773\n.921.0380.40.455.5.5LinearKernelvs.NonlinearKernels\nTojustifytheuseoflinearclassiersinourapproach,we\ntriedthepopular\nintersectionkernel\nandChi-squareker-\nnelonoursparsecodingfeaturesforcomparison.We\nconductedtheexperimentsonCaltech-101(with15train-\n\ningexamples)and15Scenes\n,andtheresultsareshown\ninTable\n7.Asshown,ourScSPMbasedonlinearkernel\nachievesamuchbetterperformanceonbothCaltech-101\nand15Scenescomparedtothenonlinearcounterparts,not\ntomentionthatthenonlinearmethodsrequiremuchmore\n\ncomputation.ThecompatibilityoflinearmodelswithSIFT\nsparsecodesisaveryinterestingphenomenon.Oneintu-\nitiveexplanationisthat,patternswithsparsefeaturesare\n\nmorelinearlyseparable,whichisindeedthecasefortext\nclassication.Table7.Theperformancecomparisonbetweenlinearandnonlin-\nearkernelsonScSPM.\nDatasetLinearChi-SquareIntersection\nCaltech\n67.00.4560.70.1160\n.40.98Scene80.40.4577.30.7577\n.70.666.ConclusionandFutureWork\nInthispaperweproposedaspatialpyramidmatching\napproachbasedonSIFTsparsecodesforimageclassica-\n\ntion.Themethodusesselectivesparsecodinginsteadof\ntraditionalvectorquantizationtoextractsalientproperties\nofappearancedescriptorsofl\nocalimagepatches.Further\n1800'b'more,insteadof\naveraging\npoolinginthehistogram,sparse\ncodingenablesustooperatelocal\nmaxpoolingonmultiple\nspatialscalestoincorporatetra\nnslationandscaleinvariance.\nThemostencouragingresultofthispaperis,theobtained\nimagerepresentationworkssurprisinglywellwithsimple\nlinearSVMs,whichdramaticallyimprovesthescalability\noftrainingandthespeedoftesting,andevenimprovesthe\nclassicationaccuracy.Oure\nxperimentsonavarietyofim-\nageclassicationtasksdemons\ntratedtheeffectivenessof\nthisapproach.Sincethe\nnonlinearSPMbasedonvector\nquantizationisverypopularintop-performingimageclas-\nsicationsystems,webelievethesuggestedlinearSPMwill\n\ngreatlyimprove\nstate-of-the-art\nbyallowingtousemuch\nlargersetsoftrainingdata.\nAsanindicationfromourwork,thesparsecodesofSIFT\nfeaturesmightserveasabetterlocalappearancedescrip-\ntorforgeneralimageprocessingtasks.Furtherresearchof\n\nthisinempiricalstudyandtheoreticalunderstandingisan\ninterestingdirection.Anotherissueistheefciencyofen-\ncoding.Currentlyencoding\ntheSIFTdescriptorsofeach\nCaltechimagetakesabout\n1secondinaverage.Arecent\nworkshowsthatsparsecodingcanbedramaticallyaccel-\neratedbyusingafeed-forwardnetwork[\n11].Itwillbein-\nterestingtotrysuchmethodstomakeourapproachfaster.\nMoreover,theaccuracycouldbefurtherimprovedbylearn-\n\ningthecodebookinasupervisedfashion,assuggestedby\nanotherrecentwork[\n15].References\n[1]O.Boiman,E.Shechtman,andM.Irani.Indefenseof\nnearest-neighborbasedimageclassication.In\nCVPR,2008.\n2,5,7[2]A.Bosch,A.Zisserman,andX.Munoz.Imageclassication\nusingrandomforestsandferns.In\nICCV,2007.\n1,2[3]A.Bosch,A.Zisserman,andX.Munoz.Sceneclassication\nusingahybridgenerative/dicriminativeapproach.\nTPAMI\n,2008.2[4]C.-C.ChangandC.-J.Lin.\nLIBSVM:alibraryfor\nsupportvectormachines\n,2001.Softwareavailableat\nhttp://www.csie.ntu.edu.tw/cjlin/libsvm.5[5]M.EladandM.Aharon.Imagedenoisingviasparseand\nredundantrepresentationsoverlearneddictionaries.\nIEEETransactiononImageProcessing\n,2006.\n2[6]M.Everingham,L.V.Gool\n,C.Williams,J\n.Winn,and\nA.Zisserman.Thepascalvisualobjectclasseschallenge\n2008(voc2008).In\nECCVWorkshop\n,2008.\n2[7]L.Fei-FeiandP.Perona.Abayesianhierarchicalmodelfor\nlearningnaturalscenecategories.In\nCVPR,2005.\n2,5[8]Grifn,G.Holub,andP.AD.Perona.Caltech-256object\ncategorydataset.T\nechnicalReport7694,CaliforniaInstitute\nofTechnology,2007.\n1,5,6[9]P.Jain,B.Kullis,andK.Grauman.Fastimagesearchfor\nlearnedmetrics.In\nCVPR,2008.\n2,5[10]F.JurieandB.Triggs.Creatingefcientcodebooksforvi-\nsualrecognition.In\nICCV,2005.\n2[11]K.Kavukcuoglu,M.Ranzato,andY.LeCun.Fastinfer-\nenceinsparsecodingalgorithmswithapplicationstoobject\nrecognition.Technicalreport,ComputationalandBiological\nLearningLab,NYU,2008.\n8[12]S.Lazebnik,C.Schmid,andJ.Ponce.Beyondbagsof\nfeatures:Spatialpyramidmatchingforrecognizingnatural\n\nscenecategories.In\nCVPR,2006.\n1,2,5,6,7[13]H.Lee,A.Battle,R.Raina,andA.Y.Ng.Efcientsparse\ncodingalgorithms.In\nNIPS,2006.\n4[14]F.-F.Li,R.Fergus,andP.Perona.Learninggenerativevisual\nmodelsfromfewtrainingexamples:anincrementalbayesian\n\napproachtestedon101objectcategories.In\nCVPRWorkshop\nonGenerative-ModelBasedVision\n,2004.\n1,5[15]J.Mairal,F.Bach,J.Ponce,G.Sapiro,andA.Zisserman.\nSuperviseddictionarylearning.In\nNIPS,2009.\n8[16]S.Maji,A.C.Berg,andJ.Malik.Classicationusinginter-\nsectionkernelsupportvectormachineisefcient.In\nCVPR,2008.2[17]J.Malik,S.Belongie,T.Leung,andJ.Shi.Sparserepre-\nsentationforcolorimagerestoration.\nIEEETransactionon\nImageProcessing\n,2008.\n2[18]J.Mariral,F.Bach,J.Ponce,G.Sapiro,andA.Zisserman.\nDiscriminativelearneddictionariesforlocalimageanalysis.\nInCVPR,2008.\n2[19]F.Moosmann,B.Triggs,andF.Jurie.Randomizedcluster-\ningforestsforbuildingfastanddiscriminativevisualvocab-\nularies.In\nNIPS,2007.\n2[20]A.OlivaandA.Torraba.Modelingtheshapeofthescene:\nAholisticrepresentationofthespatialenvelop.\nIJCV,2001.\n5[21]P.Quelhas,F.Monay,J.Odobez,D.G.-P.T.Tuytelaars,and\nL.V.Gool.Modelingsceneswithlocaldescriptorsandlatent\n\naspects.In\nICCV,2005.\n2[22]R.Raina,A.Battle,H.Lee,B.Packer,andA.Y.Ng.Self-\ntaughtlearning:Transferlearningfromunlabeleddata.In\nICML,2007.\n2[23]M.Ranzato,F.Huang,Y.Boureau,andY.LeCun.Unsuper-\nvisedlearningofinvariantfeaturehierarchieswithapplica-\ntionstoobjectrecognition.In\nCVPR,2007.\n2[24]T.Serre,L.Wolf,andT.Poggio.Objectrecognitionwith\nfeaturesinspiredbyvisualcortex.In\nCVPR,2005.\n2,4[25]J.C.vanGemert,J.-M.Geusebroek,C.J.Veenman,and\nA.W.M.Smeulders.Kernelcodebooksforscenecatego-\nrization.In\nECCV,2008.\n5,6[26]J.Yang,J.Wright,T.Huang,andY.Ma.Imagesuper-\nresolutionassparserepresentationofrawimagepatches.In\n\nCVPR,2008.\n2[27]L.Yang,R.Jin,R.Sukthankar,andF.Jurie.Unifyingdis-\ncriminativevisualcodebookgenerationwithclassiertrain-\ningforobjectcategoryrecognition.In\nCVPR,2008.\n2[28]H.Zhang,A.Berg,M.Maire,andJ.Malik.Svm-knn:Dis-\ncriminativenearestheighborclassicationforvisualcate-\ngoryrecognition.In\nCVPR,2006.\n51801'