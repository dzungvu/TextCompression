b'FasterR-CNN:TowardsReal-TimeObjectDetection\nwithRegionProposalNetworks\nShaoqingRen\n\nKaimingHeRossGirshickJianSun\nMicrosoftResearch\nf\nv-shren,kahe,rbg,jiansun\ng\n@microsoft.com\nAbstract\nState-of-the-artobjectdetectionnetworksdependonregionproposalalgorithms\ntohypothesizeobjectlocations.AdvanceslikeSPPnet[\n7\n]andFastR-CNN[\n5\n]\nhavereducedtherunningtimeofthesedetectionnetworks,exposingregionpro-\nposalcomputationasabottleneck.Inthiswork,weintroducea\nRegionPro-\nposalNetwork\n(RPN)thatsharesfull-imageconvolutionalfeatureswiththede-\ntectionnetwork,thusenablingnearlycost-freeregionproposals.AnRPNisa\nfully-convolutionalnetworkthatsimultaneouslypredictsobjectboundsandob-\njectnessscoresateachposition.RPNsaretrainedend-to-endtogeneratehigh-\nqualityregionproposals,whichareusedbyFastR-CNNfordetection.Witha\nsimplealternatingoptimization,RPNandFastR-CNNcanbetrainedtoshare\nconvolutionalfeatures.FortheverydeepVGG-16model[\n19\n],ourdetection\nsystemhasaframerateof5fps(\nincludingallsteps\n)onaGPU,whileachieving\nstate-of-the-artobjectdetectionaccuracyonPASCALVOC2007(73.2%mAP)\nand2012(70.4%mAP)using300proposalsperimage.Codeisavailableat\nhttps://github.com/ShaoqingRen/faster_rcnn\n.\n1Introduction\nRecentadvancesinobjectdetectionaredrivenbythesuccessofregionproposalmethods(\ne.g.\n,[\n22\n])\nandregion-basedconvolutionalneuralnetworks(R-CNNs)[\n6\n].Althoughregion-basedCNNswere\ncomputationallyexpensiveasoriginallydevelopedin[\n6\n],theircosthasbeendrasticallyreduced\nthankstosharingconvolutionsacrossproposals[\n7\n,\n5\n].Thelatestincarnation,FastR-CNN[\n5\n],\nachievesnearreal-timeratesusingverydeepnetworks[\n19\n],\nwhenignoringthetimespentonregion\nproposals\n.Now,proposalsarethecomputationalbottleneckinstate-of-the-artdetectionsystems.\nRegionproposalmethodstypicallyrelyoninexpensivefeaturesandeconomicalinferenceschemes.\nSelectiveSearch(SS)[\n22\n],oneofthemostpopularmethods,greedilymergessuperpixelsbased\nonengineeredlow-levelfeatures.Yetwhencomparedtoefdetectionnetworks[\n5\n],Selective\nSearchisanorderofmagnitudeslower,at2sperimageinaCPUimplementation.EdgeBoxes\n[\n24\n]currentlyprovidesthebesttradeoffbetweenproposalqualityandspeed,at0.2sperimage.\nNevertheless,theregionproposalstepstillconsumesasmuchrunningtimeasthedetectionnetwork.\nOnemaynotethatfastregion-basedCNNstakeadvantageofGPUs,whiletheregionproposalmeth-\nodsusedinresearchareimplementedontheCPU,makingsuchruntimecomparisonsinequitable.\nAnobviouswaytoaccelerateproposalcomputationistore-implementitfortheGPU.Thismaybe\naneffectiveengineeringsolution,butre-implementationignoresthedown-streamdetectionnetwork\nandthereforemissesimportantopportunitiesforsharingcomputation.\nInthispaper,weshowthatanalgorithmicchangecomputingproposalswithadeepnetleads\ntoanelegantandeffectivesolution,whereproposalcomputationisnearlycost-freegiventhede-\n\nShaoqingReniswiththeUniversityofScienceandTechnologyofChina.Thisworkwasdonewhenhe\nwasaninternatMicrosoftResearch.\n1\n'b"tectionnetwork'scomputation.Tothisend,weintroducenovel\nRegionProposalNetworks\n(RPNs)\nthatshareconvolutionallayerswithstate-of-the-artobjectdetectionnetworks[\n7\n,\n5\n].Bysharing\nconvolutionsattest-time,themarginalcostforcomputingproposalsissmall(\ne.g.\n,10msperimage).\nOurobservationisthattheconvolutional(conv)featuremapsusedbyregion-baseddetectors,like\nFastR-CNN,canalsobeusedforgeneratingregionproposals.Ontopoftheseconvfeatures,we\nconstructRPNsbyaddingtwoadditionalconvlayers:onethatencodeseachconvmapposition\nintoashort(\ne.g.\n,256-d)featurevectorandasecondthat,ateachconvmapposition,outputsan\nobjectnessscoreandregressedboundsfor\nk\nregionproposalsrelativetovariousscalesandaspect\nratiosatthatlocation(\nk\n=9\nisatypicalvalue).\nOurRPNsarethusakindoffully-convolutionalnetwork(FCN)[\n14\n]andtheycanbetrainedend-to-\nendforthetaskforgeneratingdetectionproposals.TounifyRPNswithFastR-CNN[\n5\n]\nobjectdetectionnetworks,weproposeasimpletrainingschemethatalternatesbetween\nfortheregionproposaltaskandthenforobjectdetection,whilekeepingtheproposals\ned.Thisschemeconvergesquicklyandproducesanetworkwithconvfeaturesthatare\nsharedbetweenbothtasks.\nWeevaluateourmethodonthePASCALVOCdetectionbenchmarks[\n4\n],whereRPNswithFast\nR-CNNsproducedetectionaccuracybetterthanthestrongbaselineofSelectiveSearchwithFast\nR-CNNs.Meanwhile,ourmethodwaivesnearlyallcomputationalburdensofSSattest-timethe\neffectiverunningtimeforproposalsisjust10milliseconds.Usingtheexpensiveverydeepmodels\nof[\n19\n],ourdetectionmethodstillhasaframerateof5fps(\nincludingallsteps\n)onaGPU,and\nthusisapracticalobjectdetectionsystemintermsofbothspeedandaccuracy(73.2%mAPon\nPASCALVOC2007and70.4%mAPon2012).Codeisavailableat\nhttps://github.com/\nShaoqingRen/faster_rcnn\n.\n2RelatedWork\nSeveralrecentpapershaveproposedwaysofusingdeepnetworksforlocatingorclass-\nagnosticboundingboxes[\n21\n,\n18\n,\n3\n,\n20\n].IntheOverFeatmethod[\n18\n],afully-connected(fc)layer\nistrainedtopredicttheboxcoordinatesforthelocalizationtaskthatassumesasingleobject.The\nfclayeristhenturnedintoaconvlayerfordetectingmultipleobjects.TheMulti-\nBoxmethods[\n3\n,\n20\n]generateregionproposalsfromanetworkwhoselastfclayersimultaneously\npredictsmultiple(\ne.g.\n,800)boxes,whichareusedforR-CNN[\n6\n]objectdetection.Theirproposal\nnetworkisappliedonasingleimageormultiplelargeimagecrops(\ne.g.\n,224\n\n224)[\n20\n].Wediscuss\nOverFeatandMultiBoxinmoredepthlaterincontextwithourmethod.\nSharedcomputationofconvolutions[\n18\n,\n7\n,\n2\n,\n5\n]hasbeenattractingincreasingattentionforef\ncient,yetaccurate,visualrecognition.TheOverFeatpaper[\n18\n]computesconvfeaturesfroman\nimagepyramidforlocalization,anddetection.Adaptively-sizedpooling(SPP)[\n7\n]on\nsharedconvfeaturemapsisproposedforefregion-basedobjectdetection[\n7\n,\n16\n]andsemantic\nsegmentation[\n2\n].FastR-CNN[\n5\n]enablesend-to-enddetectortrainingonsharedconvfeaturesand\nshowscompellingaccuracyandspeed.\n3RegionProposalNetworks\nARegionProposalNetwork(RPN)takesanimage(ofanysize)asinputandoutputsasetof\nrectangularobjectproposals,eachwithanobjectnessscore.\n1\nWemodelthisprocesswithafully-\nconvolutionalnetwork[\n14\n],whichwedescribeinthissection.Becauseourultimategoalistoshare\ncomputationwithaFastR-CNNobjectdetectionnetwork[\n5\n],weassumethatbothnetssharea\ncommonsetofconvlayers.Inourexperiments,weinvestigatetheZeilerandFergusmodel[\n23\n]\n(ZF),whichhas5shareableconvlayersandtheSimonyanandZissermanmodel[\n19\n](VGG),which\nhas13shareableconvlayers.\nTogenerateregionproposals,weslideasmallnetworkovertheconvfeaturemapoutputbythelast\nsharedconvlayer.Thisnetworkisfullyconnectedtoan\nn\n\nn\nspatialwindowoftheinputconv\n1\nRegionisagenerictermandinthispaperweonlyconsider\nrectangular\nregions,asiscommonformany\nmethods(\ne.g.\n,[\n20\n,\n22\n,\n24\n]).Objectnessmeasuresmembershiptoasetofobjectclasses\nvs.\nbackground.\n2\n"b'Figure1:\nLeft\n:RegionProposalNetwork(RPN).\nRight\n:ExampledetectionsusingRPNproposals\nonPASCALVOC2007test.Ourmethoddetectsobjectsinawiderangeofscalesandaspectratios.\nfeaturemap.Eachslidingwindowismappedtoalower-dimensionalvector(256-dforZFand512-d\nforVGG).Thisvectorisfedintotwosiblingfully-connectedlayersabox-regressionlayer(\nreg\n)\nandalayer(\ncls\n).Weuse\nn\n=3\ninthispaper,notingthattheeffectivereceptive\nontheinputimageislarge(171and228pixelsforZFandVGG,respectively).Thismini-\nnetworkisillustratedatasinglepositioninFig.\n1\n(left).Notethatbecausethemini-networkoperates\ninasliding-windowfashion,thefully-connectedlayersaresharedacrossallspatiallocations.This\narchitectureisnaturallyimplementedwithan\nn\n\nn\nconvlayerfollowedbytwosibling\n1\n\n1\nconv\nlayers(for\nreg\nand\ncls\n,respectively).ReLUs[\n15\n]areappliedtotheoutputofthe\nn\n\nn\nconvlayer.\nTranslation-InvariantAnchors\nAteachsliding-windowlocation,wesimultaneouslypredict\nk\nregionproposals,sothe\nreg\nlayer\nhas\n4\nk\noutputsencodingthecoordinatesof\nk\nboxes.The\ncls\nlayeroutputs\n2\nk\nscoresthatestimate\nprobabilityofobject/not-objectforeachproposal.\n2\nThe\nk\nproposalsareparameterized\nrelative\nto\nk\nreferenceboxes,called\nanchors\n.Eachanchoriscenteredattheslidingwindowinquestion,andis\nassociatedwithascaleandaspectratio.Weuse3scalesand3aspectratios,yielding\nk\n=9\nanchors\nateachslidingposition.Foraconvfeaturemapofasize\nW\n\nH\n(typically\n\n2,400),thereare\nWHk\nanchorsintotal.Animportantpropertyofourapproachisthatitis\ntranslationinvariant\n,bothin\ntermsoftheanchorsandthefunctionsthatcomputeproposalsrelativetotheanchors.\nAsacomparison,theMultiBoxmethod[\n20\n]usesk-meanstogenerate800anchors,whichare\nnot\ntranslationinvariant.Ifonetranslatesanobjectinanimage,theproposalshouldtranslateandthe\nsamefunctionshouldbeabletopredicttheproposalineitherlocation.Moreover,becausethe\nMultiBoxanchorsarenottranslationinvariant,itrequiresa(4+1)\n\n800-dimensionaloutputlayer,\nwhereasourmethodrequiresa(4+2)\n\n9\n-dimensionaloutputlayer.Ourproposallayershaveanorder\nofmagnitudefewerparameters(27millionforMultiBoxusingGoogLeNet[\n20\n]\nvs.\n2.4millionfor\nRPNusingVGG-16),andthushavelessriskofovonsmalldatasets,likePASCALVOC.\nALossFunctionforLearningRegionProposals\nFortrainingRPNs,weassignabinaryclasslabel(ofbeinganobjectornot)toeachanchor.We\nassignapositivelabeltotwokindsofanchors:(i)theanchor/anchorswiththehighestIntersection-\nover-Union(IoU)overlapwithaground-truthbox,or(ii)ananchorthathasanIoUoverlaphigher\nthan0.7withanyground-truthbox.Notethatasingleground-truthboxmayassignpositivelabels\ntomultipleanchors.Weassignanegativelabeltoanon-positiveanchorifitsIoUratioislowerthan\n0.3forallground-truthboxes.Anchorsthatareneitherpositivenornegativedonotcontributetothe\ntrainingobjective.\nWiththeseweminimizeanobjectivefunctionfollowingthemulti-tasklossinFastR-\nCNN[\n5\n].Ourlossfunctionforanimageisas:\nL\n(\nf\np\ni\ng\n;\nf\nt\ni\ng\n)=\n1\nN\ncls\nX\ni\nL\ncls\n(\np\ni\n;p\n\ni\n)+\n\n1\nN\nreg\nX\ni\np\n\ni\nL\nreg\n(\nt\ni\n;t\n\ni\n)\n:\n(1)\n2\nForsimplicityweimplementthe\ncls\nlayerasatwo-classsoftmaxlayer.Alternatively,onemayuselogistic\nregressiontoproduce\nk\nscores.\n3\n'b'Here,\ni\nistheindexofananchorinamini-batchand\np\ni\nisthepredictedprobabilityofanchor\ni\nbeing\nanobject.Theground-truthlabel\np\n\ni\nis1iftheanchorispositive,andis0iftheanchorisnegative.\nt\ni\nisavectorrepresentingthe4parameterizedcoordinatesofthepredictedboundingbox,and\nt\n\ni\nisthat\noftheground-truthboxassociatedwithapositiveanchor.Theclassloss\nL\ncls\nisloglossover\ntwoclasses(object\nvs.\nnotobject).Fortheregressionloss,weuse\nL\nreg\n(\nt\ni\n;t\n\ni\n)=\nR\n(\nt\ni\n\nt\n\ni\n)\nwhere\nR\nistherobustlossfunction(smoothL\n1\n)in[\n5\n].Theterm\np\n\ni\nL\nreg\nmeanstheregressionloss\nisactivatedonlyforpositiveanchors(\np\n\ni\n=1\n)andisdisabledotherwise(\np\n\ni\n=0\n).Theoutputsof\nthe\ncls\nand\nreg\nlayersconsistof\nf\np\ni\ng\nand\nf\nt\ni\ng\nrespectively.Thetwotermsarenormalizedwith\nN\ncls\nand\nN\nreg\n,andabalancingweight\n\n.\n3\nForregression,weadopttheparameterizationsofthe4coordinatesfollowing[\n6\n]:\nt\nx\n=(\nx\n\nx\na\n)\n=w\na\n;t\ny\n=(\ny\n\ny\na\n)\n=h\na\n;t\nw\n=log(\nw=w\na\n)\n;t\nh\n=log(\nh=h\na\n)\n;\nt\n\nx\n=(\nx\n\n\nx\na\n)\n=w\na\n;t\n\ny\n=(\ny\n\n\ny\na\n)\n=h\na\n;t\n\nw\n=log(\nw\n\n=w\na\n)\n;t\n\nh\n=log(\nh\n\n=h\na\n)\n;\nwhere\nx\n,\ny\n,\nw\n,and\nh\ndenotethetwocoordinatesoftheboxcenter,width,andheight.Variables\nx\n,\nx\na\n,and\nx\n\nareforthepredictedbox,anchorbox,andground-truthboxrespectively(likewise\nfor\ny;w;h\n).Thiscanbethoughtofasbounding-boxregressionfromananchorboxtoanearby\nground-truthbox.\nNevertheless,ourmethodachievesbounding-boxregressionbyadifferentmannerfromprevious\nfeature-map-basedmethods[\n7\n,\n5\n].In[\n7\n,\n5\n],bounding-boxregressionisperformedonfeatures\npooledfrom\narbitrarily\nsizedregions,andtheregressionweightsare\nshared\nbyallregionsizes.In\nourformulation,thefeaturesusedforregressionareofthe\nsame\nspatialsize(\nn\n\nn\n)onthefeature\nmaps.Toaccountforvaryingsizes,asetof\nk\nbounding-boxregressorsarelearned.Eachregressor\nisresponsibleforonescaleandoneaspectratio,andthe\nk\nregressorsdo\nnot\nshareweights.Assuch,\nitisstillpossibletopredictboxesofvarioussizeseventhoughthefeaturesareofaedsize/scale.\nOptimization\nTheRPN,whichisnaturallyimplementedasafully-convolutionalnetwork[\n14\n],canbetrained\nend-to-endbyback-propagationandstochasticgradientdescent(SGD)[\n12\n].Wefollowtheimage-\ncentricsamplingstrategyfrom[\n5\n]totrainthisnetwork.Eachmini-batcharisesfromasingleimage\nthatcontainsmanypositiveandnegativeanchors.Itispossibletooptimizeforthelossfunctionsof\nallanchors,butthiswillbiastowardsnegativesamplesastheyaredominate.Instead,werandomly\nsample256anchorsinanimagetocomputethelossfunctionofamini-batch,wherethesampled\npositiveandnegativeanchorshavearatioof\nupto\n1:1.Iftherearefewerthan128positivesamples\ninanimage,wepadthemini-batchwithnegativeones.\nWerandomlyinitializeallnewlayersbydrawingweightsfromazero-meanGaussiandistribution\nwithstandarddeviation0.01.Allotherlayers(\ni.e.\n,thesharedconvlayers)areinitializedbypre-\ntrainingamodelforImageNet[\n17\n],asisstandardpractice[\n6\n].Wetunealllayersof\ntheZFnet,andconv3\n1\nandupfortheVGGnettoconservememory[\n5\n].Weusealearningrate\nof0.001for60kmini-batches,and0.0001forthenext20kmini-batchesonthePASCALdataset.\nWealsouseamomentumof0.9andaweightdecayof0.0005[\n11\n].OurimplementationusesCaffe\n[\n10\n].\nSharingConvolutionalFeaturesforRegionProposalandObjectDetection\nThusfarwehavedescribedhowtotrainanetworkforregionproposalgeneration,withoutcon-\nsideringtheregion-basedobjectdetectionCNNthatwillutilizetheseproposals.Forthedetection\nnetwork,weadoptFastR-CNN[\n5\n]\n4\nandnowdescribeanalgorithmthatlearnsconvlayersthatare\nsharedbetweentheRPNandFastR-CNN.\nBothRPNandFastR-CNN,trainedindependently,willmodifytheirconvlayersindifferentways.\nWethereforeneedtodevelopatechniquethatallowsforsharingconvlayersbetweenthetwonet-\nworks,ratherthanlearningtwoseparatenetworks.Notethatthisisnotaseasyassimply\nasinglenetworkthatincludesbothRPNandFastR-CNN,andthenoptimizingitjointlywithback-\npropagation.ThereasonisthatFastR-CNNtrainingdependson\n\nobjectproposalsanditis\n3\nInourearlyimplementation(asalsointhereleasedcode),\n\nwassetas10,andthe\ncls\nterminEqn.(\n1\n)was\nnormalizedbythemini-batchsize(\ni.e.\n,\nN\ncls\n=256\n)andthe\nreg\ntermwasnormalizedbythenumberofanchor\nlocations(\ni.e.\n,\nN\nreg\n\n2\n;\n400\n).Both\ncls\nand\nreg\ntermsareroughlyequallyweightedinthisway.\n4\nhttps://github.com/rbgirshick/fast-rcnn\n4\n'b'notclearaprioriiflearningFastR-CNNwhilesimultaneouslychangingtheproposalmechanism\nwillconverge.Whilethisjointoptimizingisaninterestingquestionforfuturework,wedevelopa\npragmatic4-steptrainingalgorithmtolearnsharedfeaturesviaalternatingoptimization.\nInthestep,wetraintheRPNasdescribedabove.ThisnetworkisinitializedwithanImageNet-\npre-trainedmodelandunedend-to-endfortheregionproposaltask.Inthesecondstep,we\ntrainaseparatedetectionnetworkbyFastR-CNNusingtheproposalsgeneratedbythestep-1RPN.\nThisdetectionnetworkisalsoinitializedbytheImageNet-pre-trainedmodel.Atthispointthetwo\nnetworksdonotshareconvlayers.Inthethirdstep,weusethedetectornetworktoinitializeRPN\ntraining,butwethesharedconvlayersandonlythelayersuniquetoRPN.Nowthetwo\nnetworksshareconvlayers.Finally,keepingthesharedconvlayersed,wethefclayers\noftheFastR-CNN.Assuch,bothnetworkssharethesameconvlayersandformanetwork.\nImplementationDetails\nWetrainandtestbothregionproposalandobjectdetectionnetworksonsingle-scaleimages[\n7\n,\n5\n].Were-scaletheimagessuchthattheirshortersideis\ns\n=600\npixels[\n5\n].Multi-scalefeature\nextractionmayimproveaccuracybutdoesnotexhibitagoodspeed-accuracytrade-off[\n5\n].Wealso\nnotethatforZFandVGGnets,thetotalstrideonthelastconvlayeris16pixelsonthere-scaled\nimage,andthusis\n\n10pixelsonatypicalPASCALimage(\n\n500\n\n375).Evensuchalargestride\nprovidesgoodresults,thoughaccuracymaybefurtherimprovedwithasmallerstride.\nForanchors,weuse3scaleswithboxareasof\n128\n2\n,\n256\n2\n,and\n512\n2\npixels,and3aspectratiosof\n1:1,1:2,and2:1.Wenotethatouralgorithmallowstheuseofanchorboxesthatarelargerthanthe\nunderlyingreceptivewhenpredictinglargeproposals.Suchpredictionsarenotimpossible\nonemaystillroughlyinfertheextentofanobjectifonlythemiddleoftheobjectisvisible.With\nthisdesign,oursolutiondoesnotneedmulti-scalefeaturesormulti-scaleslidingwindowstopredict\nlargeregions,savingconsiderablerunningtime.Fig.\n1\n(right)showsthecapabilityofourmethod\nforawiderangeofscalesandaspectratios.Thetablebelowshowsthe\nlearnedaverageproposal\nsize\nforeachanchorusingtheZFnet(numbersfor\ns\n=600\n).\nanchor\n128\n2\n,2:1\n128\n2\n,1:1\n128\n2\n,1:2\n256\n2\n,2:1\n256\n2\n,1:1\n256\n2\n,1:2\n512\n2\n,2:1\n512\n2\n,1:1\n512\n2\n,1:2\nproposal\n188\n\n111\n113\n\n114\n70\n\n92\n416\n\n229\n261\n\n284\n174\n\n332\n768\n\n437\n499\n\n501\n355\n\n715\nTheanchorboxesthatcrossimageboundariesneedtobehandledwithcare.Duringtraining,we\nignoreallcross-boundaryanchorssotheydonotcontributetotheloss.Foratypical\n1000\n\n600\nimage,therewillberoughly20k(\n\n60\n\n40\n\n9\n)anchorsintotal.Withthecross-boundaryanchors\nignored,thereareabout6kanchorsperimagefortraining.Iftheboundary-crossingoutliersarenot\nignoredintraining,theyintroducelarge,diftocorrecterrortermsintheobjective,andtraining\ndoesnotconverge.Duringtesting,however,westillapplythefully-convolutionalRPNtotheentire\nimage.Thismaygeneratecross-boundaryproposalboxes,whichwecliptotheimageboundary.\nSomeRPNproposalshighlyoverlapwitheachother.Toreduceredundancy,weadoptnon-\nmaximumsuppression(NMS)ontheproposalregionsbasedontheir\ncls\nscores.WetheIoU\nthresholdforNMSat0.7,whichleavesusabout2kproposalregionsperimage.Aswewillshow,\nNMSdoesnotharmtheultimatedetectionaccuracy,butsubstantiallyreducesthenumberofpro-\nposals.AfterNMS,weusethetop-\nN\nrankedproposalregionsfordetection.Inthefollowing,we\ntrainFastR-CNNusing2kRPNproposals,butevaluatedifferentnumbersofproposalsattest-time.\n4Experiments\nWecomprehensivelyevaluateourmethodonthePASCALVOC2007detectionbenchmark[\n4\n].\nThisdatasetconsistsofabout5ktrainvalimagesand5ktestimagesover20objectcategories.We\nalsoprovideresultsinthePASCALVOC2012benchmarkforafewmodels.FortheImageNet\npre-trainednetwork,weusethefastversionofZFnet[\n23\n]thathas5convlayersand3fclayers,\nandthepublicVGG-16model\n5\n[\n19\n]thathas13convlayersand3fclayers.Weprimarilyevalu-\natedetectionmeanAveragePrecision(mAP),becausethisistheactualmetricforobjectdetection\n(ratherthanfocusingonobjectproposalproxymetrics).\nTable\n1\n(top)showsFastR-CNNresultswhentrainedandtestedusingvariousregionproposal\nmethods.TheseresultsusetheZFnet.ForSelectiveSearch(SS)[\n22\n],wegenerateabout2kSS\n5\nwww.robots.ox.ac.uk/\n\nvgg/research/very_deep/\n5\n'b"Table1:Detectionresultson\nPASCALVOC2007testset\n(trainedonVOC2007trainval).The\ndetectorsareFastR-CNNwithZF,butusingvariousproposalmethodsfortrainingandtesting.\ntrain-timeregionproposals\ntest-timeregionproposals\nmethod#boxes\nmethod#proposals\nmAP(%)\nSS2k\nSS2k\n58.7\nEB2k\nEB2k\n58.6\nRPN+ZF,shared2k\nRPN+ZF,shared300\n59.9\nablationexperimentsfollowbelow\nRPN+ZF,unshared2k\nRPN+ZF,unshared300\n58.7\nSS2k\nRPN+ZF100\n55.1\nSS2k\nRPN+ZF300\n56.8\nSS2k\nRPN+ZF1k\n56.3\nSS2k\nRPN+ZF(noNMS)6k\n55.2\nSS2k\nRPN+ZF(no\ncls\n)100\n44.6\nSS2k\nRPN+ZF(no\ncls\n)300\n51.4\nSS2k\nRPN+ZF(no\ncls\n)1k\n55.8\nSS2k\nRPN+ZF(no\nreg\n)300\n52.1\nSS2k\nRPN+ZF(no\nreg\n)1k\n51.3\nSS2k\nRPN+VGG300\n59.2\nproposalsbythefastmode.ForEdgeBoxes(EB)[\n24\n],wegeneratetheproposalsbythedefault\nEBsettingtunedfor0.7IoU.SShasanmAPof58.7%andEBhasanmAPof58.6%.RPNwith\nFastR-CNNachievescompetitiveresults,withanmAPof59.9%whileusing\nupto\n300proposals\n6\n.\nUsingRPNyieldsamuchfasterdetectionsystemthanusingeitherSSorEBbecauseofsharedconv\ncomputations;thefewerproposalsalsoreducetheregion-wisefccost.Next,weconsiderseveral\nablationsofRPNandthenshowthatproposalqualityimproveswhenusingtheverydeepnetwork.\nAblationExperiments.\nToinvestigatethebehaviorofRPNsasaproposalmethod,weconducted\nseveralablationstudies.First,weshowtheeffectofsharingconvlayersbetweentheRPNandFast\nR-CNNdetectionnetwork.Todothis,westopafterthesecondstepinthe4-steptrainingprocess.\nUsingseparatenetworksreducestheresultslightlyto58.7%(RPN+ZF,unshared,Table\n1\n).We\nobservethatthisisbecauseinthethirdstepwhenthedetector-tunedfeaturesareusedto\ntheRPN,theproposalqualityisimproved.\nNext,wedisentangletheRPN'sontrainingtheFastR-CNNdetectionnetwork.Forthis\npurpose,wetrainaFastR-CNNmodelbyusingthe2kSSproposalsandZFnet.Wethisdetector\nandevaluatethedetectionmAPbychangingtheproposalregionsusedattest-time.Intheseablation\nexperiments,theRPNdoesnotsharefeatureswiththedetector.\nReplacingSSwith300RPNproposalsattest-timeleadstoanmAPof56.8%.ThelossinmAP\nisbecauseoftheinconsistencybetweenthetraining/testingproposals.Thisresultservesasthe\nbaselineforthefollowingcomparisons.\nSomewhatsurprisingly,theRPNstillleadstoacompetitiveresult(55.1%)whenusingthetop-\nranked100proposalsattest-time,indicatingthatthetop-rankedRPNproposalsareaccurate.On\ntheotherextreme,usingthetop-ranked6kRPNproposals(withoutNMS)hasacomparablemAP\n(55.2%),suggestingNMSdoesnotharmthedetectionmAPandmayreducefalsealarms.\nNext,weseparatelyinvestigatetherolesofRPN's\ncls\nand\nreg\noutputsbyturningoffeitherofthem\nattest-time.Whenthe\ncls\nlayerisremovedattest-time(thusnoNMS/rankingisused),werandomly\nsample\nN\nproposalsfromtheunscoredregions.ThemAPisnearlyunchangedwith\nN\n=1\nk\n(55.8%),butdegradesconsiderablyto44.6%when\nN\n=100\n.Thisshowsthatthe\ncls\nscoresaccount\nfortheaccuracyofthehighestrankedproposals.\nOntheotherhand,whenthe\nreg\nlayerisremovedattest-time(sotheproposalsbecomeanchor\nboxes),themAPdropsto52.1%.Thissuggeststhatthehigh-qualityproposalsaremainlydueto\nregressedpositions.Theanchorboxesalonearenotsufforaccuratedetection.\n6\nForRPN,thenumberofproposals(\ne.g.\n,300)isthemaximumnumberforanimage.RPNmayproduce\nfewerproposalsafterNMS,andthustheaveragenumberofproposalsissmaller.\n6\n"b'Table2:Detectionresultson\nPASCALVOC2007testset\n.ThedetectorisFastR-CNNandVGG-\n16.Trainingdata:07:VOC2007trainval,07+12:unionsetofVOC2007trainvalandVOC\n2012trainval.ForRPN,thetrain-timeproposalsforFastR-CNNare2k.\ny\n:thiswasreportedin[\n5\n];\nusingtherepositoryprovidedbythispaper,thisnumberishigher(68.0\n\n0.3insixruns).\nmethod#proposals\ndata\nmAP(%)\ntime(ms)\nSS2k\n07\n66.9\ny\n1830\nSS2k\n07+12\n70.0\n1830\nRPN+VGG,unshared300\n07\n68.5\n342\nRPN+VGG,shared300\n07\n69.9\n198\nRPN+VGG,shared300\n07+12\n73.2\n198\nTable3:Detectionresultson\nPASCALVOC2012testset\n.ThedetectorisFastR-CNNandVGG-\n16.Trainingdata:07:VOC2007trainval,07++12:unionsetofVOC2007trainval+test\nandVOC2012trainval.ForRPN,thetrain-timeproposalsforFastR-CNNare2k.\ny\n:\nhttp://\nhost.robots.ox.ac.uk:8080/anonymous/HZJTQA.html\n.\nz\n:\nhttp://host.robots.ox.ac.uk:8080/\nanonymous/YNPLXB.html\nmethod#proposals\ndata\nmAP(%)\nSS2k\n12\n65.7\nSS2k\n07++12\n68.4\nRPN+VGG,shared\ny\n300\n12\n67.0\nRPN+VGG,shared\nz\n300\n07++12\n70.4\nTable4:\nTiming\n(ms)onaK40GPU,exceptSSproposalisevaluatedinaCPU.Region-wise\nincludesNMS,pooling,fc,andsoftmax.Seeourreleasedcodefortheofrunningtime.\nmodel\nsystem\nconvproposalregion-wise\ntotal\nrate\nVGG\nSS+FastR-CNN\n1461510174\n1830\n0.5fps\nVGG\nRPN+FastR-CNN\n141\n10\n47\n198\n5fps\nZF\nRPN+FastR-CNN\n31\n3\n25\n59\n17fps\nWealsoevaluatetheeffectsofmorepowerfulnetworksontheproposalqualityofRPNalone.\nWeuseVGG-16totraintheRPN,andstillusetheabovedetectorofSS+ZF.ThemAPimproves\nfrom56.8%(usingRPN+ZF)to59.2%(usingRPN+VGG).Thisisapromisingresult,becauseit\nsuggeststhattheproposalqualityofRPN+VGGisbetterthanthatofRPN+ZF.Becauseproposalsof\nRPN+ZFarecompetitivewithSS(bothare58.7%whenconsistentlyusedfortrainingandtesting),\nwemayexpectRPN+VGGtobebetterthanSS.Thefollowingexperimentsjustifythishypothesis.\nDetectionAccuracyandRunningTimeofVGG-16.\nTable\n2\nshowstheresultsofVGG-16forboth\nproposalanddetection.UsingRPN+VGG,theFastR-CNNresultis68.5%for\nunshared\nfeatures,\nslightlyhigherthantheSSbaseline.Asshownabove,thisisbecausetheproposalsgeneratedby\nRPN+VGGaremoreaccuratethanSS.UnlikeSSthatistheRPNisactivelytrained\nandfrombetternetworks.Forthefeature-\nshared\nvariant,theresultis69.9%betterthan\nthestrongSSbaseline,yetwithnearlycost-freeproposals.WefurthertraintheRPNanddetection\nnetworkontheunionsetofPASCALVOC2007trainvaland2012trainval,following[\n5\n].ThemAP\nis\n73.2%\n.OnthePASCALVOC2012testset(Table\n3\n),ourmethodhasanmAPof\n70.4%\ntrained\nontheunionsetofVOC2007trainval+testandVOC2012trainval,following[\n5\n].\nInTable\n4\nwesummarizetherunningtimeoftheentireobjectdetectionsystem.SStakes1-2\nsecondsdependingoncontent(onaverage1.51s),andFastR-CNNwithVGG-16takes320mson\n2kSSproposals(or223msifusingSVDonfclayers[\n5\n]).OursystemwithVGG-16takesintotal\n198ms\nforbothproposalanddetection.Withtheconvfeaturesshared,theRPNaloneonlytakes\n10mscomputingtheadditionallayers.Ourregion-wisecomputationisalsolow,thankstofewer\nproposals(300).Oursystemhasaframe-rateof17fpswiththeZFnet.\nAnalysisofRecall-to-IoU.\nNextwecomputetherecallofproposalsatdifferentIoUratioswith\nground-truthboxes.ItisnoteworthythattheRecall-to-IoUmetricisjust\nloosely\n[\n9\n,\n8\n,\n1\n]relatedto\ntheultimatedetectionaccuracy.Itismoreappropriatetousethismetricto\ndiagnose\ntheproposal\nmethodthantoevaluateit.\n7\n'b'Figure2:Recall\nvs.\nIoUoverlapratioonthePASCALVOC2007testset.\nTable5:\nOne-StageDetection\nvs.\nTwo-StageProposal+Detection\n.Detectionresultsareonthe\nPASCALVOC2007testsetusingtheZFmodelandFastR-CNN.RPNusesunsharedfeatures.\nregions\ndetector\nmAP(%)\nTwo-Stage\nRPN+ZF,unshared300\nFastR-CNN+ZF,1scale\n58.7\nOne-Stage\ndense,3scales,3asp.ratios20k\nFastR-CNN+ZF,1scale\n53.8\nOne-Stage\ndense,3scales,3asp.ratios20k\nFastR-CNN+ZF,5scales\n53.9\nInFig.\n2\n,weshowtheresultsofusing300,1k,and2kproposals.WecomparewithSSandEB,and\nthe\nN\nproposalsarethetop-\nN\nrankedonesbasedonthegeneratedbythesemethods.\nTheplotsshowthattheRPNmethodbehavesgracefullywhenthenumberofproposalsdropsfrom\n2kto300.ThisexplainswhytheRPNhasagoodultimatedetectionmAPwhenusingasfewas300\nproposals.Asweanalyzedbefore,thispropertyismainlyattributedtothe\ncls\ntermoftheRPN.The\nrecallofSSandEBdropsmorequicklythanRPNwhentheproposalsarefewer.\nOne-StageDetection\nvs.\nTwo-StageProposal+Detection.\nTheOverFeatpaper[\n18\n]proposesa\ndetectionmethodthatusesregressorsandclassonslidingwindowsoverconvfeaturemaps.\nOverFeatisa\none-stage\n,\n\ndetectionpipeline,andoursisa\ntwo-stagecascade\nconsisting\nofclass-agnosticproposalsanddetections.InOverFeat,theregion-wisefeaturescome\nfromaslidingwindowofoneaspectratiooverascalepyramid.Thesefeaturesareusedtosimulta-\nneouslydeterminethelocationandcategoryofobjects.InRPN,thefeaturesarefromsquare(3\n\n3)\nslidingwindowsandpredictproposalsrelativetoanchorswithdifferentscalesandaspectratios.\nThoughbothmethodsuseslidingwindows,theregionproposaltaskisonlythestageofRPN\n+FastR-CNNthedetector\nattends\ntotheproposalstothem.Inthesecondstageofourcas-\ncade,theregion-wisefeaturesareadaptivelypooled[\n7\n,\n5\n]fromproposalboxesthatmorefaithfully\ncoverthefeaturesoftheregions.Webelievethesefeaturesleadtomoreaccuratedetections.\nTocomparetheone-stageandtwo-stagesystems,we\nemulate\ntheOverFeatsystem(andthusalso\ncircumventotherdifferencesofimplementationdetails)by\none-stage\nFastR-CNN.Inthissystem,\ntheproposalsaredenseslidingwindowsof3scales(128,256,512)and3aspectratios(1:1,1:2,\n2:1).FastR-CNNistrainedtopredictscoresandregressboxlocationsfromthese\nslidingwindows.BecausetheOverFeatsystemusesanimagepyramid,wealsoevaluateusingconv\nfeaturesextractedfrom5scales.Weusethose5scalesasin[\n7\n,\n5\n].\nTable\n5\ncomparesthetwo-stagesystemandtwovariantsoftheone-stagesystem.UsingtheZF\nmodel,theone-stagesystemhasanmAPof53.9%.Thisislowerthanthetwo-stagesystem(58.7%)\nby4.8%.Thisexperimentjustitheeffectivenessofcascadedregionproposalsandobjectdetec-\ntion.Similarobservationsarereportedin[\n5\n,\n13\n],wherereplacingSSregionproposalswithsliding\nwindowsleadsto\n\n6%degradationinbothpapers.Wealsonotethattheone-stagesystemisslower\nasithasconsiderablymoreproposalstoprocess.\n5Conclusion\nWehavepresentedRegionProposalNetworks(RPNs)forefandaccurateregionproposal\ngeneration.Bysharingconvolutionalfeatureswiththedown-streamdetectionnetwork,theregion\nproposalstepisnearlycost-free.Ourmethodenablesadeep-learning-basedobjectdetection\nsystemtorunat5-17fps.ThelearnedRPNalsoimprovesregionproposalqualityandthusthe\noverallobjectdetectionaccuracy.\n8\n'b"References\n[1]\nN.Chavali,H.Agrawal,A.Mahendru,andD.Batra.Object-ProposalEvaluationProtocolis'Gameable'.\narXiv:1505.05836\n,2015.\n[2]\nJ.Dai,K.He,andJ.Sun.Convolutionalfeaturemaskingforjointobjectandstuffsegmentation.In\nCVPR\n,2015.\n[3]\nD.Erhan,C.Szegedy,A.Toshev,andD.Anguelov.Scalableobjectdetectionusingdeepneuralnetworks.\nIn\nCVPR\n,2014.\n[4]\nM.Everingham,L.VanGool,C.K.I.Williams,J.Winn,andA.Zisserman.ThePASCALVisualObject\nClassesChallenge2007(VOC2007)Results,2007.\n[5]\nR.Girshick.FastR-CNN.\narXiv:1504.08083\n,2015.\n[6]\nR.Girshick,J.Donahue,T.Darrell,andJ.Malik.Richfeaturehierarchiesforaccurateobjectdetection\nandsemanticsegmentation.In\nCVPR\n,2014.\n[7]\nK.He,X.Zhang,S.Ren,andJ.Sun.Spatialpyramidpoolingindeepconvolutionalnetworksforvisual\nrecognition.In\nECCV\n.2014.\n[8]\nJ.Hosang,R.Benenson,P.Doll\n\nar,andB.Schiele.Whatmakesforeffectivedetectionproposals?\narXiv:1502.05082\n,2015.\n[9]\nJ.Hosang,R.Benenson,andB.Schiele.Howgoodaredetectionproposals,really?In\nBMVC\n,2014.\n[10]\nY.Jia,E.Shelhamer,J.Donahue,S.Karayev,J.Long,R.Girshick,S.Guadarrama,andT.Darrell.Caffe:\nConvolutionalarchitectureforfastfeatureembedding.\narXiv:1408.5093\n,2014.\n[11]\nA.Krizhevsky,I.Sutskever,andG.Hinton.Imagenetwithdeepconvolutionalneuralnet-\nworks.In\nNIPS\n,2012.\n[12]\nY.LeCun,B.Boser,J.S.Denker,D.Henderson,R.E.Howard,W.Hubbard,andL.D.Jackel.Backprop-\nagationappliedtohandwrittenzipcoderecognition.\nNeuralcomputation\n,1989.\n[13]\nK.LencandA.Vedaldi.R-CNNminusR.\narXiv:1506.06981\n,2015.\n[14]\nJ.Long,E.Shelhamer,andT.Darrell.Fullyconvolutionalnetworksforsemanticsegmentation.In\nCVPR\n,\n2015.\n[15]\nV.NairandG.E.Hinton.linearunitsimproverestrictedboltzmannmachines.In\nICML\n,2010.\n[16]\nS.Ren,K.He,R.Girshick,X.Zhang,andJ.Sun.Objectdetectionnetworksonconvolutionalfeature\nmaps.\narXiv:1504.06066\n,2015.\n[17]\nO.Russakovsky,J.Deng,H.Su,J.Krause,S.Satheesh,S.Ma,Z.Huang,A.Karpathy,A.Khosla,\nM.Bernstein,A.C.Berg,andL.Fei-Fei.ImageNetLargeScaleVisualRecognitionChallenge.\narXiv:1409.0575\n,2014.\n[18]\nP.Sermanet,D.Eigen,X.Zhang,M.Mathieu,R.Fergus,andY.LeCun.Overfeat:Integratedrecognition,\nlocalizationanddetectionusingconvolutionalnetworks.In\nICLR\n,2014.\n[19]\nK.SimonyanandA.Zisserman.Verydeepconvolutionalnetworksforlarge-scaleimagerecognition.In\nICLR\n,2015.\n[20]\nC.Szegedy,S.Reed,D.Erhan,andD.Anguelov.Scalable,high-qualityobjectdetection.\narXiv:1412.1441v2\n,2015.\n[21]\nC.Szegedy,A.Toshev,andD.Erhan.Deepneuralnetworksforobjectdetection.In\nNIPS\n,2013.\n[22]\nJ.R.Uijlings,K.E.vandeSande,T.Gevers,andA.W.Smeulders.Selectivesearchforobjectrecognition.\nIJCV\n,2013.\n[23]\nM.D.ZeilerandR.Fergus.Visualizingandunderstandingconvolutionalneuralnetworks.In\nECCV\n,\n2014.\n[24]\nC.L.ZitnickandP.Doll\n\nar.Edgeboxes:Locatingobjectproposalsfromedges.In\nECCV\n,2014.\n9\n"