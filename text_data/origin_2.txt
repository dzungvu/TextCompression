b"ImageNetwithDeepConvolutional\nNeuralNetworks\nAlexKrizhevsky\nUniversityofToronto\nkriz@cs.utoronto.ca\nIlyaSutskever\nUniversityofToronto\nilya@cs.utoronto.ca\nGeoffreyE.Hinton\nUniversityofToronto\nhinton@cs.utoronto.ca\nAbstract\nWetrainedalarge,deepconvolutionalneuralnetworktoclassifythe1.2million\nhigh-resolutionimagesintheImageNetLSVRC-2010contestintothe1000dif-\nferentclasses.Onthetestdata,weachievedtop-1andtop-5errorratesof37.5%\nand17.0%whichisconsiderablybetterthanthepreviousstate-of-the-art.The\nneuralnetwork,whichhas60millionparametersand650,000neurons,consists\nofveconvolutionallayers,someofwhicharefollowedbymax-poolinglayers,\nandthreefully-connectedlayerswitha1000-waysoftmax.Tomaketrain-\ningfaster,weusednon-saturatingneuronsandaveryefGPUimplemen-\ntationoftheconvolutionoperation.Toreduceovinthefully-connected\nlayersweemployedarecently-developedregularizationmethodcalleddropout\nthatprovedtobeveryeffective.Wealsoenteredavariantofthismodelinthe\nILSVRC-2012competitionandachievedawinningtop-5testerrorrateof15.3%,\ncomparedto26.2%achievedbythesecond-bestentry.\n1Introduction\nCurrentapproachestoobjectrecognitionmakeessentialuseofmachinelearningmethods.Toim-\nprovetheirperformance,wecancollectlargerdatasets,learnmorepowerfulmodels,andusebet-\ntertechniquesforpreventingovUntilrecently,datasetsoflabeledimageswererelatively\nsmallontheorderoftensofthousandsofimages(e.g.,NORB[16],Caltech-101/256[8,9],and\nCIFAR-10/100[12]).Simplerecognitiontaskscanbesolvedquitewellwithdatasetsofthissize,\nespeciallyiftheyareaugmentedwithlabel-preservingtransformations.Forexample,thecurrent-\nbesterrorrateontheMNISTdigit-recognitiontask(<0.3%)approacheshumanperformance[4].\nButobjectsinrealisticsettingsexhibitconsiderablevariability,sotolearntorecognizethemitis\nnecessarytousemuchlargertrainingsets.Andindeed,theshortcomingsofsmallimagedatasets\nhavebeenwidelyrecognized(e.g.,Pintoetal.[21]),butithasonlyrecentlybecomepossibletocol-\nlectlabeleddatasetswithmillionsofimages.ThenewlargerdatasetsincludeLabelMe[23],which\nconsistsofhundredsofthousandsoffully-segmentedimages,andImageNet[6],whichconsistsof\nover15millionlabeledhigh-resolutionimagesinover22,000categories.\nTolearnaboutthousandsofobjectsfrommillionsofimages,weneedamodelwithalargelearning\ncapacity.However,theimmensecomplexityoftheobjectrecognitiontaskmeansthatthisprob-\nlemcannotbeevenbyadatasetaslargeasImageNet,soourmodelshouldalsohavelots\nofpriorknowledgetocompensateforallthedatawedon'thave.Convolutionalneuralnetworks\n(CNNs)constituteonesuchclassofmodels[16,11,13,18,15,22,26].Theircapacitycanbecon-\ntrolledbyvaryingtheirdepthandbreadth,andtheyalsomakestrongandmostlycorrectassumptions\naboutthenatureofimages(namely,stationarityofstatisticsandlocalityofpixeldependencies).\nThus,comparedtostandardfeedforwardneuralnetworkswithsimilarly-sizedlayers,CNNshave\nmuchfewerconnectionsandparametersandsotheyareeasiertotrain,whiletheirtheoretically-best\nperformanceislikelytobeonlyslightlyworse.\n1\n"b"DespitetheattractivequalitiesofCNNs,anddespitetherelativeefyoftheirlocalarchitecture,\ntheyhavestillbeenprohibitivelyexpensivetoapplyinlargescaletohigh-resolutionimages.Luck-\nily,currentGPUs,pairedwithahighly-optimizedimplementationof2Dconvolution,arepowerful\nenoughtofacilitatethetrainingofinterestingly-largeCNNs,andrecentdatasetssuchasImageNet\ncontainenoughlabeledexamplestotrainsuchmodelswithoutsevereov\nThecontributionsofthispaperareasfollows:wetrainedoneofthelargestconvolutional\nneuralnetworkstodateonthesubsetsofImageNetusedintheILSVRC-2010andILSVRC-2012\ncompetitions[2]andachievedbyfarthebestresultseverreportedonthesedatasets.Wewrotea\nhighly-optimizedGPUimplementationof2Dconvolutionandalltheotheroperationsinherentin\ntrainingconvolutionalneuralnetworks,whichwemakeavailablepublicly\n1\n.Ournetworkcontains\nanumberofnewandunusualfeatureswhichimproveitsperformanceandreduceitstrainingtime,\nwhicharedetailedinSection3.Thesizeofournetworkmadeovaproblem,even\nwith1.2millionlabeledtrainingexamples,soweusedseveraleffectivetechniquesforpreventing\novwhicharedescribedinSection4.Ournetworkcontainsveconvolutionaland\nthreefully-connectedlayers,andthisdepthseemstobeimportant:wefoundthatremovingany\nconvolutionallayer(eachofwhichcontainsnomorethan1%ofthemodel'sparameters)resultedin\ninferiorperformance.\nIntheend,thenetwork'ssizeislimitedmainlybytheamountofmemoryavailableoncurrentGPUs\nandbytheamountoftrainingtimethatwearewillingtotolerate.Ournetworktakesbetweenve\nandsixdaystotrainontwoGTX5803GBGPUs.Allofourexperimentssuggestthatourresults\ncanbeimprovedsimplybywaitingforfasterGPUsandbiggerdatasetstobecomeavailable.\n2TheDataset\nImageNetisadatasetofover15millionlabeledhigh-resolutionimagesbelongingtoroughly22,000\ncategories.TheimageswerecollectedfromthewebandlabeledbyhumanlabelersusingAma-\nzon'sMechanicalTurkcrowd-sourcingtool.Startingin2010,aspartofthePascalVisualObject\nChallenge,anannualcompetitioncalledtheImageNetLarge-ScaleVisualRecognitionChallenge\n(ILSVRC)hasbeenheld.ILSVRCusesasubsetofImageNetwithroughly1000imagesineachof\n1000categories.Inall,thereareroughly1.2milliontrainingimages,50,000validationimages,and\n150,000testingimages.\nILSVRC-2010istheonlyversionofILSVRCforwhichthetestsetlabelsareavailable,sothisis\ntheversiononwhichweperformedmostofourexperiments.Sincewealsoenteredourmodelin\ntheILSVRC-2012competition,inSection6wereportourresultsonthisversionofthedatasetas\nwell,forwhichtestsetlabelsareunavailable.OnImageNet,itiscustomarytoreporttwoerrorrates:\ntop-1andtop-5,wherethetop-5errorrateisthefractionoftestimagesforwhichthecorrectlabel\nisnotamongthevelabelsconsideredmostprobablebythemodel.\nImageNetconsistsofvariable-resolutionimages,whileoursystemrequiresaconstantinputdimen-\nsionality.Therefore,wedown-sampledtheimagestoaedresolutionof\n256\n\n256\n.Givena\nrectangularimage,werescaledtheimagesuchthattheshortersidewasoflength256,andthen\ncroppedoutthecentral\n256\n\n256\npatchfromtheresultingimage.Wedidnotpre-processtheimages\ninanyotherway,exceptforsubtractingthemeanactivityoverthetrainingsetfromeachpixel.So\nwetrainedournetworkonthe(centered)rawRGBvaluesofthepixels.\n3TheArchitecture\nThearchitectureofournetworkissummarizedinFigure2.Itcontainseightlearnedlayers\nveconvolutionalandthreefully-connected.Below,wedescribesomeofthenovelorunusual\nfeaturesofournetwork'sarchitecture.Sections3.1-3.4aresortedaccordingtoourestimationof\ntheirimportance,withthemostimportant\n1\nhttp://code.google.com/p/cuda-convnet/\n2\n"b"3.1ReLUNonlinearity\nFigure1:\nAfour-layerconvolutionalneural\nnetworkwithReLUs\n(solidline)\nreachesa25%\ntrainingerrorrateonCIFAR-10sixtimesfaster\nthananequivalentnetworkwith\ntanh\nneurons\n(dashedline)\n.Thelearningratesforeachnet-\nworkwerechosenindependentlytomaketrain-\ningasfastaspossible.Noregularizationof\nanykindwasemployed.Themagnitudeofthe\neffectdemonstratedherevarieswithnetwork\narchitecture,butnetworkswithReLUsconsis-\ntentlylearnseveraltimesfasterthanequivalents\nwithsaturatingneurons.\nThestandardwaytomodelaneuron'soutput\nf\nas\nafunctionofitsinput\nx\niswith\nf\n(\nx\n)=tanh(\nx\n)\nor\nf\n(\nx\n)=(1+\ne\n\nx\n)\n\n1\n.Intermsoftrainingtime\nwithgradientdescent,thesesaturatingnonlinearities\naremuchslowerthanthenon-saturatingnonlinearity\nf\n(\nx\n)=max(0\n;x\n)\n.FollowingNairandHinton[20],\nwerefertoneuronswiththisnonlinearityas\nLinearUnits(ReLUs).Deepconvolutionalneuralnet-\nworkswithReLUstrainseveraltimesfasterthantheir\nequivalentswith\ntanh\nunits.Thisisdemonstratedin\nFigure1,whichshowsthenumberofiterationsre-\nquiredtoreach25%trainingerrorontheCIFAR-10\ndatasetforaparticularfour-layerconvolutionalnet-\nwork.Thisplotshowsthatwewouldnothavebeen\nabletoexperimentwithsuchlargeneuralnetworksfor\nthisworkifwehadusedtraditionalsaturatingneuron\nmodels.\nWearenotthetoconsideralternativestotradi-\ntionalneuronmodelsinCNNs.Forexample,Jarrett\netal.[11]claimthatthenonlinearity\nf\n(\nx\n)=\nj\ntanh(\nx\n)\nj\nworksparticularlywellwiththeirtypeofcontrastnor-\nmalizationfollowedbylocalaveragepoolingonthe\nCaltech-101dataset.However,onthisdatasetthepri-\nmaryconcernispreventingovting,sotheeffect\ntheyareobservingisdifferentfromtheaccelerated\nabilitytothetrainingsetwhichwereportwhenus-\ningReLUs.Fasterlearninghasagreatonthe\nperformanceoflargemodelstrainedonlargedatasets.\n3.2TrainingonMultipleGPUs\nAsingleGTX580GPUhasonly3GBofmemory,whichlimitsthemaximumsizeofthenetworks\nthatcanbetrainedonit.Itturnsoutthat1.2milliontrainingexamplesareenoughtotrainnetworks\nwhicharetoobigtoononeGPU.ThereforewespreadthenetacrosstwoGPUs.CurrentGPUs\nareparticularlywell-suitedtocross-GPUparallelization,astheyareabletoreadfromandwriteto\noneanother'smemorydirectly,withoutgoingthroughhostmachinememory.Theparallelization\nschemethatweemployessentiallyputshalfofthekernels(orneurons)oneachGPU,withone\nadditionaltrick:theGPUscommunicateonlyincertainlayers.Thismeansthat,forexample,the\nkernelsoflayer3takeinputfromallkernelmapsinlayer2.However,kernelsinlayer4takeinput\nonlyfromthosekernelmapsinlayer3whichresideonthesameGPU.Choosingthepatternof\nconnectivityisaproblemforcross-validation,butthisallowsustopreciselytunetheamountof\ncommunicationuntilitisanacceptablefractionoftheamountofcomputation.\nTheresultantarchitectureissomewhatsimilartothatofthecolumnarCNNemployedbyCiresan\netal.[5],exceptthatourcolumnsarenotindependent(seeFigure2).Thisschemereducesourtop-1\nandtop-5errorratesby1.7%and1.2%,respectively,ascomparedwithanetwithhalfasmany\nkernelsineachconvolutionallayertrainedononeGPU.Thetwo-GPUnettakesslightlylesstime\ntotrainthantheone-GPUnet\n2\n.\n2\nTheone-GPUnetactuallyhasthesamenumberofkernelsasthetwo-GPUnetintheconvolutional\nlayer.Thisisbecausemostofthenet'sparametersareinthefully-connectedlayer,whichtakesthelast\nconvolutionallayerasinput.Sotomakethetwonetshaveapproximatelythesamenumberofparameters,we\ndidnothalvethesizeoftheconvolutionallayer(northefully-connecedlayerswhichfollow).Therefore\nthiscomparisonisbiasedinfavoroftheone-GPUnet,sinceitisbiggerthanhalfthesizeofthetwo-GPU\nnet.\n3\n"b'3.3LocalResponseNormalization\nReLUshavethedesirablepropertythattheydonotrequireinputnormalizationtopreventthem\nfromsaturating.IfatleastsometrainingexamplesproduceapositiveinputtoaReLU,learningwill\nhappeninthatneuron.However,westillthatthefollowinglocalnormalizationschemeaids\ngeneralization.Denotingby\na\ni\nx;y\ntheactivityofaneuroncomputedbyapplyingkernel\ni\natposition\n(\nx;y\n)\nandthenapplyingtheReLUnonlinearity,theresponse-normalizedactivity\nb\ni\nx;y\nisgivenby\ntheexpression\nb\ni\nx;y\n=\na\ni\nx;y\n=\n0\n@\nk\n+\n\nmin(\nN\n\n1\n;i\n+\nn=\n2)\nX\nj\n=max(0\n;i\n\nn=\n2)\n(\na\nj\nx;y\n)\n2\n1\nA\n\nwherethesumrunsover\nn\nadjacentkernelmapsatthesamespatialposition,and\nN\nisthetotal\nnumberofkernelsinthelayer.Theorderingofthekernelmapsisofcoursearbitraryanddetermined\nbeforetrainingbegins.Thissortofresponsenormalizationimplementsaformoflateralinhibition\ninspiredbythetypefoundinrealneurons,creatingcompetitionforbigactivitiesamongstneuron\noutputscomputedusingdifferentkernels.Theconstants\nk;n;\n,and\n\narehyper-parameterswhose\nvaluesaredeterminedusingavalidationset;weused\nk\n=2\n,\nn\n=5\n,\n\n=10\n\n4\n,and\n\n=0\n:\n75\n.We\nappliedthisnormalizationafterapplyingtheReLUnonlinearityincertainlayers(seeSection3.5).\nThisschemebearssomeresemblancetothelocalcontrastnormalizationschemeofJarrettetal.[11],\nbutourswouldbemorecorrectlytermedbrightnessnormalization,sincewedonotsubtractthe\nmeanactivity.Responsenormalizationreducesourtop-1andtop-5errorratesby1.4%and1.2%,\nrespectively.WealsovtheeffectivenessofthisschemeontheCIFAR-10dataset:afour-layer\nCNNachieveda13%testerrorratewithoutnormalizationand11%withnormalization\n3\n.\n3.4OverlappingPooling\nPoolinglayersinCNNssummarizetheoutputsofneighboringgroupsofneuronsinthesamekernel\nmap.Traditionally,theneighborhoodssummarizedbyadjacentpoolingunitsdonotoverlap(e.g.,\n[17,11,4]).Tobemoreprecise,apoolinglayercanbethoughtofasconsistingofagridofpooling\nunitsspaced\ns\npixelsapart,eachsummarizinganeighborhoodofsize\nz\n\nz\ncenteredatthelocation\nofthepoolingunit.Ifweset\ns\n=\nz\n,weobtaintraditionallocalpoolingascommonlyemployed\ninCNNs.Ifweset\ns<z\n,weobtainoverlappingpooling.Thisiswhatweusethroughoutour\nnetwork,with\ns\n=2\nand\nz\n=3\n.Thisschemereducesthetop-1andtop-5errorratesby0.4%and\n0.3%,respectively,ascomparedwiththenon-overlappingscheme\ns\n=2\n;z\n=2\n,whichproduces\noutputofequivalentdimensions.Wegenerallyobserveduringtrainingthatmodelswithoverlapping\npoolingitslightlymorediftoov\n3.5OverallArchitecture\nNowwearereadytodescribetheoverallarchitectureofourCNN.AsdepictedinFigure2,thenet\ncontainseightlayerswithweights;theveareconvolutionalandtheremainingthreearefully-\nconnected.Theoutputofthelastfully-connectedlayerisfedtoa1000-waysoftmaxwhichproduces\nadistributionoverthe1000classlabels.Ournetworkmaximizesthemultinomiallogisticregression\nobjective,whichisequivalenttomaximizingtheaverageacrosstrainingcasesofthelog-probability\nofthecorrectlabelunderthepredictiondistribution.\nThekernelsofthesecond,fourth,andconvolutionallayersareconnectedonlytothosekernel\nmapsinthepreviouslayerwhichresideonthesameGPU(seeFigure2).Thekernelsofthethird\nconvolutionallayerareconnectedtoallkernelmapsinthesecondlayer.Theneuronsinthefully-\nconnectedlayersareconnectedtoallneuronsinthepreviouslayer.Response-normalizationlayers\nfollowtheandsecondconvolutionallayers.Max-poolinglayers,ofthekinddescribedinSection\n3.4,followbothresponse-normalizationlayersaswellastheconvolutionallayer.TheReLU\nnon-linearityisappliedtotheoutputofeveryconvolutionalandfully-connectedlayer.\nTheconvolutionallayerthe\n224\n\n224\n\n3\ninputimagewith96kernelsofsize\n11\n\n11\n\n3\nwithastrideof4pixels(thisisthedistancebetweenthereceptivecentersofneighboring\n3\nWecannotdescribethisnetworkindetailduetospaceconstraints,butitispreciselybythecode\nandparameterprovidedhere:http://code.google.com/p/cuda-convnet/.\n4\n'b"Figure2:\nAnillustrationofthearchitectureofourCNN,explicitlyshowingthedelineationofresponsibilities\nbetweenthetwoGPUs.OneGPUrunsthelayer-partsatthetopofthewhiletheotherrunsthelayer-parts\natthebottom.TheGPUscommunicateonlyatcertainlayers.Thenetwork'sinputis150,528-dimensional,and\nthenumberofneuronsinthenetwork'sremaininglayersisgivenby253,440186,62464,89664,89643,264\n409640961000.\nneuronsinakernelmap).Thesecondconvolutionallayertakesasinputthe(response-normalized\nandpooled)outputoftheconvolutionallayeranditwith256kernelsofsize\n5\n\n5\n\n48\n.\nThethird,fourth,andconvolutionallayersareconnectedtooneanotherwithoutanyintervening\npoolingornormalizationlayers.Thethirdconvolutionallayerhas384kernelsofsize\n3\n\n3\n\n256\nconnectedtothe(normalized,pooled)outputsofthesecondconvolutionallayer.Thefourth\nconvolutionallayerhas384kernelsofsize\n3\n\n3\n\n192\n,andtheconvolutionallayerhas256\nkernelsofsize\n3\n\n3\n\n192\n.Thefully-connectedlayershave4096neuronseach.\n4ReducingOv\nOurneuralnetworkarchitecturehas60millionparameters.Althoughthe1000classesofILSVRC\nmakeeachtrainingexampleimpose10bitsofconstraintonthemappingfromimagetolabel,this\nturnsouttobeinsuftolearnsomanyparameterswithoutconsiderableovting.Below,we\ndescribethetwoprimarywaysinwhichwecombatov\n4.1DataAugmentation\nTheeasiestandmostcommonmethodtoreduceovonimagedataistoenlarge\nthedatasetusinglabel-preservingtransformations(e.g.,[25,4,5]).Weemploytwodistinctforms\nofdataaugmentation,bothofwhichallowtransformedimagestobeproducedfromtheoriginal\nimageswithverylittlecomputation,sothetransformedimagesdonotneedtobestoredondisk.\nInourimplementation,thetransformedimagesaregeneratedinPythoncodeontheCPUwhilethe\nGPUistrainingonthepreviousbatchofimages.Sothesedataaugmentationschemesare,ineffect,\ncomputationallyfree.\nTheformofdataaugmentationconsistsofgeneratingimagetranslationsandhorizontal\ntions.Wedothisbyextractingrandom\n224\n\n224\npatches(andtheirhorizontalfromthe\n256\n\n256\nimagesandtrainingournetworkontheseextractedpatches\n4\n.Thisincreasesthesizeofour\ntrainingsetbyafactorof2048,thoughtheresultingtrainingexamplesare,ofcourse,highlyinter-\ndependent.Withoutthisscheme,ournetworksuffersfromsubstantialovwhichwouldhave\nforcedustousemuchsmallernetworks.Attesttime,thenetworkmakesapredictionbyextracting\nve\n224\n\n224\npatches(thefourcornerpatchesandthecenterpatch)aswellastheirhorizontal\n(hencetenpatchesinall),andaveragingthepredictionsmadebythenetwork'ssoftmax\nlayeronthetenpatches.\nThesecondformofdataaugmentationconsistsofalteringtheintensitiesoftheRGBchannelsin\ntrainingimages.,weperformPCAonthesetofRGBpixelvaluesthroughoutthe\nImageNettrainingset.Toeachtrainingimage,weaddmultiplesofthefoundprincipalcomponents,\n4\nThisisthereasonwhytheinputimagesinFigure2are\n224\n\n224\n\n3\n-dimensional.\n5\n"b"withmagnitudesproportionaltothecorrespondingeigenvaluestimesarandomvariabledrawnfrom\naGaussianwithmeanzeroandstandarddeviation0.1.ThereforetoeachRGBimagepixel\nI\nxy\n=\n[\nI\nR\nxy\n;I\nG\nxy\n;I\nB\nxy\n]\nT\nweaddthefollowingquantity:\n[\np\n1\n;\np\n2\n;\np\n3\n][\n\n1\n\n1\n;\n2\n\n2\n;\n3\n\n3\n]\nT\nwhere\np\ni\nand\n\ni\nare\ni\ntheigenvectorandeigenvalueofthe\n3\n\n3\ncovariancematrixofRGBpixel\nvalues,respectively,and\n\ni\nistheaforementionedrandomvariable.Each\n\ni\nisdrawnonlyonce\nforallthepixelsofaparticulartrainingimageuntilthatimageisusedfortrainingagain,atwhich\npointitisre-drawn.Thisschemeapproximatelycapturesanimportantpropertyofnaturalimages,\nnamely,thatobjectidentityisinvarianttochangesintheintensityandcoloroftheillumination.This\nschemereducesthetop-1errorratebyover1%.\n4.2Dropout\nCombiningthepredictionsofmanydifferentmodelsisaverysuccessfulwaytoreducetesterrors\n[1,3],butitappearstobetooexpensiveforbigneuralnetworksthatalreadytakeseveraldays\ntotrain.Thereis,however,averyefversionofmodelcombinationthatonlycostsabouta\nfactoroftwoduringtraining.Therecently-introducedtechnique,calleddropout[10],consists\nofsettingtozerotheoutputofeachhiddenneuronwithprobability0.5.Theneuronswhichare\ndroppedoutinthiswaydonotcontributetotheforwardpassanddonotparticipateinback-\npropagation.Soeverytimeaninputispresented,theneuralnetworksamplesadifferentarchitecture,\nbutallthesearchitecturesshareweights.Thistechniquereducescomplexco-adaptationsofneurons,\nsinceaneuroncannotrelyonthepresenceofparticularotherneurons.Itis,therefore,forcedto\nlearnmorerobustfeaturesthatareusefulinconjunctionwithmanydifferentrandomsubsetsofthe\notherneurons.Attesttime,weusealltheneuronsbutmultiplytheiroutputsby0.5,whichisa\nreasonableapproximationtotakingthegeometricmeanofthepredictivedistributionsproducedby\ntheexponentially-manydropoutnetworks.\nWeusedropoutinthetwofully-connectedlayersofFigure2.Withoutdropout,ournetworkex-\nhibitssubstantialovDropoutroughlydoublesthenumberofiterationsrequiredtoconverge.\nFigure3:\n96convolutionalkernelsofsize\n11\n\n11\n\n3\nlearnedbytheconvolutional\nlayeronthe\n224\n\n224\n\n3\ninputimages.The\ntop48kernelswerelearnedonGPU1while\nthebottom48kernelswerelearnedonGPU\n2.SeeSection6.1fordetails.\n5Detailsoflearning\nWetrainedourmodelsusingstochasticgradientdescent\nwithabatchsizeof128examples,momentumof0.9,and\nweightdecayof0.0005.Wefoundthatthissmallamount\nofweightdecaywasimportantforthemodeltolearn.In\notherwords,weightdecayhereisnotmerelyaregularizer:\nitreducesthemodel'strainingerror.Theupdaterulefor\nweight\nw\nwas\nv\ni\n+1\n:=0\n:\n9\n\nv\ni\n\n0\n:\n0005\n\n\n\nw\ni\n\n\n\n\n@L\n@w\n\n\nw\ni\n\nD\ni\nw\ni\n+1\n:=\nw\ni\n+\nv\ni\n+1\nwhere\ni\nistheiterationindex,\nv\nisthemomentumvariable,\n\nisthelearningrate,and\nD\n@L\n@w\n\n\nw\ni\nE\nD\ni\nis\ntheaverageoverthe\ni\nthbatch\nD\ni\nofthederivativeoftheobjectivewithrespectto\nw\n,evaluatedat\nw\ni\n.\nWeinitializedtheweightsineachlayerfromazero-meanGaussiandistributionwithstandardde-\nviation0.01.Weinitializedtheneuronbiasesinthesecond,fourth,andconvolutionallayers,\naswellasinthefully-connectedhiddenlayers,withtheconstant1.Thisinitializationaccelerates\ntheearlystagesoflearningbyprovidingtheReLUswithpositiveinputs.Weinitializedtheneuron\nbiasesintheremaininglayerswiththeconstant0.\nWeusedanequallearningrateforalllayers,whichweadjustedmanuallythroughouttraining.\nTheheuristicwhichwefollowedwastodividethelearningrateby10whenthevalidationerror\nratestoppedimprovingwiththecurrentlearningrate.Thelearningratewasinitializedat0.01and\n6\n"b"reducedthreetimespriortotermination.Wetrainedthenetworkforroughly90cyclesthroughthe\ntrainingsetof1.2millionimages,whichtookvetosixdaysontwoNVIDIAGTX5803GBGPUs.\n6Results\nOurresultsonILSVRC-2010aresummarizedinTable1.Ournetworkachievestop-1andtop-5\ntestseterrorratesof\n37.5%\nand\n17.0%\n5\n.ThebestperformanceachievedduringtheILSVRC-\n2010competitionwas47.1%and28.2%withanapproachthataveragesthepredictionsproduced\nfromsixsparse-codingmodelstrainedondifferentfeatures[2],andsincethenthebestpub-\nlishedresultsare45.7%and25.7%withanapproachthataveragesthepredictionsoftwoclassi-\ntrainedonFisherVectors(FVs)computedfromtwotypesofdensely-sampledfeatures[24].\nModel\nTop-1\nTop-5\nSparsecoding[2]\n47.1%\n28.2%\nSIFT+FVs[24]\n45.7%\n25.7%\nCNN\n37.5%\n17.0%\nTable1:\nComparisonofresultsonILSVRC-\n2010testset.In\nitalics\narebestresults\nachievedbyothers.\nWealsoenteredourmodelintheILSVRC-2012com-\npetitionandreportourresultsinTable2.Sincethe\nILSVRC-2012testsetlabelsarenotpubliclyavailable,\nwecannotreporttesterrorratesforallthemodelsthat\nwetried.Intheremainderofthisparagraph,weuse\nvalidationandtesterrorratesinterchangeablybecause\ninourexperiencetheydonotdifferbymorethan0.1%\n(seeTable2).TheCNNdescribedinthispaperachieves\natop-5errorrateof18.2%.Averagingthepredictions\nofvesimilarCNNsgivesanerrorrateof16.4%.TrainingoneCNN,withanextrasixthcon-\nvolutionallayeroverthelastpoolinglayer,toclassifytheentireImageNetFall2011release\n(15Mimages,22Kcategories),andthenitonILSVRC-2012givesanerrorrateof\n16.6%.AveragingthepredictionsoftwoCNNsthatwerepre-trainedontheentireFall2011re-\nleasewiththeaforementionedveCNNsgivesanerrorrateof\n15.3%\n.Thesecond-bestcon-\ntestentryachievedanerrorrateof26.2%withanapproachthataveragesthepredictionsofsev-\neraltrainedonFVscomputedfromdifferenttypesofdensely-sampledfeatures[7].\nModel\nTop-1(val)\nTop-5(val)\nTop-5(test)\nSIFT+FVs[7]\n\n\n26.2%\n1CNN\n40.7%\n18.2%\n\n5CNNs\n38.1%\n16.4%\n16.4%\n1CNN*\n39.0%\n16.6%\n\n7CNNs*\n36.7%\n15.4%\n15.3%\nTable2:\nComparisonoferrorratesonILSVRC-2012validationand\ntestsets.In\nitalics\narebestresultsachievedbyothers.Modelswithan\nasterisk*werepre-trainedtoclassifytheentireImageNet2011Fall\nrelease.SeeSection6fordetails.\nFinally,wealsoreportourerror\nratesontheFall2009versionof\nImageNetwith10,184categories\nand8.9millionimages.Onthis\ndatasetwefollowtheconvention\nintheliteratureofusinghalfof\ntheimagesfortrainingandhalf\nfortesting.Sincethereisnoes-\ntablishedtestset,oursplitneces-\nsarilydiffersfromthesplitsused\nbypreviousauthors,butthisdoes\nnotaffecttheresultsappreciably.\nOurtop-1andtop-5errorrates\nonthisdatasetare\n67.4%\nand\n40.9%\n,attainedbythenetdescribedabovebutwithanadditional,sixthconvolutionallayeroverthe\nlastpoolinglayer.Thebestpublishedresultsonthisdatasetare78.1%and60.9%[19].\n6.1QualitativeEvaluations\nFigure3showstheconvolutionalkernelslearnedbythenetwork'stwodata-connectedlayers.The\nnetworkhaslearnedavarietyoffrequency-andorientation-selectivekernels,aswellasvariouscol-\noredblobs.NoticethespecializationexhibitedbythetwoGPUs,aresultoftherestrictedconnec-\ntivitydescribedinSection3.5.ThekernelsonGPU1arelargelycolor-agnostic,whilethekernels\nononGPU2arelargelycolorThiskindofspecializationoccursduringeveryrunandis\nindependentofanyparticularrandomweightinitialization(moduloarenumberingoftheGPUs).\n5\nTheerrorrateswithoutaveragingpredictionsovertenpatchesasdescribedinSection4.1are39.0%and\n18.3%.\n7\n"b"Figure4:\n(Left)\nEightILSVRC-2010testimagesandthevelabelsconsideredmostprobablebyourmodel.\nThecorrectlabeliswrittenundereachimage,andtheprobabilityassignedtothecorrectlabelisalsoshown\nwitharedbar(ifithappenstobeinthetop5).\n(Right)\nFiveILSVRC-2010testimagesinthecolumn.The\nremainingcolumnsshowthesixtrainingimagesthatproducefeaturevectorsinthelasthiddenlayerwiththe\nsmallestEuclideandistancefromthefeaturevectorforthetestimage.\nIntheleftpanelofFigure4wequalitativelyassesswhatthenetworkhaslearnedbycomputingits\ntop-5predictionsoneighttestimages.Noticethatevenoff-centerobjects,suchasthemiteinthe\ntop-left,canberecognizedbythenet.Mostofthetop-5labelsappearreasonable.Forexample,\nonlyothertypesofcatareconsideredplausiblelabelsfortheleopard.Insomecases(grille,cherry)\nthereisgenuineambiguityabouttheintendedfocusofthephotograph.\nAnotherwaytoprobethenetwork'svisualknowledgeistoconsiderthefeatureactivationsinduced\nbyanimageatthelast,4096-dimensionalhiddenlayer.Iftwoimagesproducefeatureactivation\nvectorswithasmallEuclideanseparation,wecansaythatthehigherlevelsoftheneuralnetwork\nconsiderthemtobesimilar.Figure4showsveimagesfromthetestsetandthesiximagesfrom\nthetrainingsetthataremostsimilartoeachofthemaccordingtothismeasure.Noticethatatthe\npixellevel,theretrievedtrainingimagesaregenerallynotcloseinL2tothequeryimagesinthe\ncolumn.Forexample,theretrieveddogsandelephantsappearinavarietyofposes.Wepresentthe\nresultsformanymoretestimagesinthesupplementarymaterial.\nComputingsimilaritybyusingEuclideandistancebetweentwo4096-dimensional,real-valuedvec-\ntorsisinefbutitcouldbemadeefbytraininganauto-encodertocompressthesevectors\ntoshortbinarycodes.Thisshouldproduceamuchbetterimageretrievalmethodthanapplyingauto-\nencoderstotherawpixels[14],whichdoesnotmakeuseofimagelabelsandhencehasatendency\ntoretrieveimageswithsimilarpatternsofedges,whetherornottheyaresemanticallysimilar.\n7Discussion\nOurresultsshowthatalarge,deepconvolutionalneuralnetworkiscapableofachievingrecord-\nbreakingresultsonahighlychallengingdatasetusingpurelysupervisedlearning.Itisnotable\nthatournetwork'sperformancedegradesifasingleconvolutionallayerisremoved.Forexample,\nremovinganyofthemiddlelayersresultsinalossofabout2%forthetop-1performanceofthe\nnetwork.Sothedepthreallyisimportantforachievingourresults.\nTosimplifyourexperiments,wedidnotuseanyunsupervisedpre-trainingeventhoughweexpect\nthatitwillhelp,especiallyifweobtainenoughcomputationalpowertoincreasethe\nsizeofthenetworkwithoutobtainingacorrespondingincreaseintheamountoflabeleddata.Thus\nfar,ourresultshaveimprovedaswehavemadeournetworklargerandtraineditlongerbutwestill\nhavemanyordersofmagnitudetogoinordertomatchtheinfero-temporalpathwayofthehuman\nvisualsystem.Ultimatelywewouldliketouseverylargeanddeepconvolutionalnetsonvideo\nsequenceswherethetemporalstructureprovidesveryhelpfulinformationthatismissingorfarless\nobviousinstaticimages.\n8\n"b"References\n[1]\nR.M.BellandY.Koren.Lessonsfromtheprizechallenge.\nACMSIGKDDExplorationsNewsletter\n,\n9(2):7579,2007.\n[2]\nA.Berg,J.Deng,andL.Fei-Fei.Largescalevisualrecognitionchallenge2010.www.image-\nnet.org/challenges.2010.\n[3]\nL.Breiman.Randomforests.\nMachinelearning\n,45(1):532,2001.\n[4]\nD.Ciresan,U.Meier,andJ.Schmidhuber.Multi-columndeepneuralnetworksforimage\nArxivpreprintarXiv:1202.2745\n,2012.\n[5]\nD.C.Ciresan,U.Meier,J.Masci,L.M.Gambardella,andJ.Schmidhuber.High-performanceneural\nnetworksforvisualobject\nArxivpreprintarXiv:1102.0183\n,2011.\n[6]\nJ.Deng,W.Dong,R.Socher,L.-J.Li,K.Li,andL.Fei-Fei.ImageNet:ALarge-ScaleHierarchical\nImageDatabase.In\nCVPR09\n,2009.\n[7]\nJ.Deng,A.Berg,S.Satheesh,H.Su,A.Khosla,andL.Fei-Fei.\nILSVRC-2012\n,2012.URL\nhttp://www.image-net.org/challenges/LSVRC/2012/\n.\n[8]\nL.Fei-Fei,R.Fergus,andP.Perona.Learninggenerativevisualmodelsfromfewtrainingexamples:An\nincrementalbayesianapproachtestedon101objectcategories.\nComputerVisionandImageUnderstand-\ning\n,106(1):5970,2007.\n[9]\nG.GrifA.Holub,andP.Perona.Caltech-256objectcategorydataset.TechnicalReport7694,Cali-\nforniaInstituteofTechnology,2007.URL\nhttp://authors.library.caltech.edu/7694\n.\n[10]\nG.E.Hinton,N.Srivastava,A.Krizhevsky,I.Sutskever,andR.R.Salakhutdinov.Improvingneuralnet-\nworksbypreventingco-adaptationoffeaturedetectors.\narXivpreprintarXiv:1207.0580\n,2012.\n[11]\nK.Jarrett,K.Kavukcuoglu,M.A.Ranzato,andY.LeCun.Whatisthebestmulti-stagearchitecturefor\nobjectrecognition?In\nInternationalConferenceonComputerVision\n,pages21462153.IEEE,2009.\n[12]\nA.Krizhevsky.Learningmultiplelayersoffeaturesfromtinyimages.Master'sthesis,Departmentof\nComputerScience,UniversityofToronto,2009.\n[13]\nA.Krizhevsky.Convolutionaldeepbeliefnetworksoncifar-10.\nUnpublishedmanuscript\n,2010.\n[14]\nA.KrizhevskyandG.E.Hinton.Usingverydeepautoencodersforcontent-basedimageretrieval.In\nESANN\n,2011.\n[15]\nY.LeCun,B.Boser,J.S.Denker,D.Henderson,R.E.Howard,W.Hubbard,L.D.Jackel,etal.Hand-\nwrittendigitrecognitionwithaback-propagationnetwork.In\nAdvancesinneuralinformationprocessing\nsystems\n,1990.\n[16]\nY.LeCun,F.J.Huang,andL.Bottou.Learningmethodsforgenericobjectrecognitionwithinvarianceto\nposeandlighting.In\nComputerVisionandPatternRecognition,2004.CVPR2004.Proceedingsofthe\n2004IEEEComputerSocietyConferenceon\n,volume2,pagesII97.IEEE,2004.\n[17]\nY.LeCun,K.Kavukcuoglu,andC.Farabet.Convolutionalnetworksandapplicationsinvision.In\nCircuitsandSystems(ISCAS),Proceedingsof2010IEEEInternationalSymposiumon\n,pages253256.\nIEEE,2010.\n[18]\nH.Lee,R.Grosse,R.Ranganath,andA.Y.Ng.Convolutionaldeepbeliefnetworksforscalableunsuper-\nvisedlearningofhierarchicalrepresentations.In\nProceedingsofthe26thAnnualInternationalConference\nonMachineLearning\n,pages609616.ACM,2009.\n[19]\nT.Mensink,J.Verbeek,F.Perronnin,andG.Csurka.MetricLearningforLargeScaleImage\ncation:GeneralizingtoNewClassesatNear-ZeroCost.In\nECCV-EuropeanConferenceonComputer\nVision\n,Florence,Italy,October2012.\n[20]\nV.NairandG.E.Hinton.linearunitsimproverestrictedboltzmannmachines.In\nProc.27th\nInternationalConferenceonMachineLearning\n,2010.\n[21]\nN.Pinto,D.D.Cox,andJ.J.DiCarlo.Whyisreal-worldvisualobjectrecognitionhard?\nPLoScomputa-\ntionalbiology\n,4(1):e27,2008.\n[22]\nN.Pinto,D.Doukhan,J.J.DiCarlo,andD.D.Cox.Ahigh-throughputscreeningapproachtodiscovering\ngoodformsofbiologicallyinspiredvisualrepresentation.\nPLoScomputationalbiology\n,5(11):e1000579,\n2009.\n[23]\nB.C.Russell,A.Torralba,K.P.Murphy,andW.T.Freeman.Labelme:adatabaseandweb-basedtoolfor\nimageannotation.\nInternationaljournalofcomputervision\n,77(1):157173,2008.\n[24]\nJ.SnchezandF.Perronnin.High-dimensionalsignaturecompressionforlarge-scaleimage\nIn\nComputerVisionandPatternRecognition(CVPR),2011IEEEConferenceon\n,pages16651672.IEEE,\n2011.\n[25]\nP.Y.Simard,D.Steinkraus,andJ.C.Platt.Bestpracticesforconvolutionalneuralnetworksappliedto\nvisualdocumentanalysis.In\nProceedingsoftheSeventhInternationalConferenceonDocumentAnalysis\nandRecognition\n,volume2,pages958962,2003.\n[26]\nS.C.Turaga,J.F.Murray,V.Jain,F.Roth,M.Helmstaedter,K.Briggman,W.Denk,andH.S.Seung.Con-\nvolutionalnetworkscanlearntogenerateafgraphsforimagesegmentation.\nNeuralComputation\n,\n22(2):511538,2010.\n9\n"