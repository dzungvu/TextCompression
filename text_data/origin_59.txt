b'Face Recognition via Sparse RepresentationJohn Wright, Allen Y. Yang, Arvind, S. Shankar Sastry and Yi Ma\nIEEE Trans. PAMI, March 2008\n'b'2Research About FaceFace DetectionFace AlignmentFace Recognition'b'3Sparse RepresentationWhat is it?Represent a samplewith respect to an overcomplete dictionaryThe representation is sparse and linear\nAdvantagesMore concise (Compression)Naturally Discriminative (Classification)sampleOvercomplete dictionarySparse linear representation'b'4Classification via sparse representationSample test sampleOvercomplete dictionary training samplesThe test sample can be represented as a \nlinear combinationof training samples only from its class\nThis representation is naturally \nsparse, compared to the whole training samples.\nTest sample\nTraining samples\nSparse linear combinationThe representation can be recovered efficiently via \n1-normminimization\nSeeking the sparsest representation\nautomatically discriminates different \nclasses in the training set.\n'b'5Comparison with related approachesNearest Neighbor\nclassifies the test sample \nbased on the best representation in terms of a \nsingle training sample.Nearest Subspace\nclassifies the test samples \nbased on the best linear representation in terms \nof all the training samples in each classLinear Sparse Representation\nconsiders all possible supports (within each class or across \nmultiple classes) and adaptively chooses the \nminimal number of training samples needed to \n\nrepresent each test sample.\n121231?33322111NN33332222111111NS?123?321LSR'b'6New Insights 1: The Role of Feature ExtractionTraditionallyGood feature provide more information for classification\nVarious features have been investigated for projecting \nthe high-dimensional test image into low-dimensional \n\nfeature spacesE.g. Eigenface, Fisherface, LaplacianfaceLack of guidelinesto decide which feature to use\nRecently, with the theory of compressed sensingThe choice of features is no longer critical\nWhat is criticalThe dimension of the feature space is sufficiently \n\nlarge The sparse representation is correctly computed\n'b'7New Insights 2:Robustness to Occlusion\nTraditionallyOcclusion poses a significant obstacle to robust, real-world face recognition\nIn this workSince the error corrupts only a fractionof the image pixels, and is therefore Sparse in the standard basis given by individual pixels.It can be handled uniformly within the \n\nproposed frameworks.'b'8OutlineIntroductionClassification Based on Sparse \nRepresentationTwo Fundamental Issues in Face RecognitionExperimental ResultsConclusion & Future works'b'9SymbolsThere are kdistinct object classes in the training dataThe nigiven training samples, taken from the i-thclass, are arranged as columns of a matrix\nIn the context of face recognition, we will identify a             grayscale image with the vector\ngiven by stacking it columns; \nthe columns of \nAiare then the training face \nimages of the i-thsubject.'b'10A. Test Sample as a Sparse Linear Combination of Training SamplesObservation:The images of faces under varying lighting \nand expression lie on a special low-\n\ndimensional subspace face subspace.Assumption:The training samples from a single class do lie \n\non a subspace'b'11Given sufficient training samples of the \ni-thobject class,                                                    , any new (test) sample             from the same class, will \napproximately lie in the linear span of the training \nsamples associated with object i:(1)For some scalars  \n'b'12Since the membership iof the test sample is \ninitially unknown, we define a new matrix \nAfor the entire training set as the concatenation of the \nntraining samples of all \nkobject classes:(2)Then the linear representation of ycan be written in terms of all training samples as\n(3)where                       \n                                                 is a coefficient vector whose entries are zero \nexcept those associated with i-thclass.'b'13=A1A2A3x0yA00\n0\n21222324250\n0\n0\n0'b'14Since the entries of the x0encode the identity of the test sample y, we can find the associated class of yby solving the linear system of equations y=AxRemarks1.A more discriminative classifier from such a \nglobal representation can be obtai\nned, which demonstrate its \nsuperiority over those local methods (NN or NS) both \nfor identifyingobjects represented in the training set and for rejecting outlying samples that do not arise \nfrom any of the classes presented in the training set.\n2.These advantages can come without an increase in the order of growth of complexity: the complexity remains linearin the size of training set.'b'15To Solve y=AxIf m>n, the system of equations y=Axis over-determined and the correct x0can usually be \nfound as its unique solution.\nHowever, in robust face recognition, the system y=Axis typically \nunder-determined, and so its \nsolution is not unique.\nConventionally, this difficulty is resolved by \n\nchoosing the minimum \n2-normsolution,(4)'b'16Problems of 2Eqn.(4) can be easily solved (via the pseudo-inverse of A), the solution     is not especially \ninformative for recognizing the test image \ny.As shown in Example 1 (Fig.4),     is generally \ndense, with large nonzero entries corresponding \n\nto training samples from many different classes.\n'b'17ObservationsTo solve the above difficulty, the following \nsimple observation is exploited: A valid test sample \nycan be sufficiently represented using onlythe training samples \nfrom the same class.This representation is naturally sparse if the \nnumber of object classes kis reasonably large. The more sparsethe recovered x0is, the easierwill it be to \naccuratelydetermine the identity of the test sample y.'b'18To solve y=Axvia 0IdeaSeek the sparsest solution\nto y=AxMethodSolving the following optimization problem:\n(5)Where         denotes the \n0-norm, which counts the \nnumber of nonzero entries in a vectorReasonIt has been proven that whenever \ny=Axfor some xwith less than m/2nonzeros, xis the unique sparse \nsolution:             . \nProblemsolving \n(0)isNP-hard\n, and difficult even to \napproximate. combinatorial optimization!'b'19B. Sparse Solution via 1-MinimizationRecent development in the emerging theory of sparse representation and compressed sensing reveals that if the solutionx0sought is sparse enough, the solution of the Eqn.(5) is equal to the solution of the following\n1-minimizationproblem:(6)This problem can be solved in polynomial timeby standard linear programming method.'b'20As long as the number of nonzero entries of x0is a small fraction of the dimension m, 1-minimizaionwill recover x0.'b'21Dealing with noiseSince real data are noisy, the model(3) can be modified to explicitly account for small, possibly \ndense noise, by writing:(7)Where             is a noise term with bounded \nenergy             . The sparse solution \nx0can still be approximately recovered by solving the \nfollowing stable l1-minimizationproblem:(8)This convex optimization problem can be \nefficiently solved via second-order cone \n\nprogramming'b'22C. Classification Based on \nSparse RepresentationGiven a new test sample yfrom one of the classes in the training set, we first compute its sparse representation     via (6) or (8). Ideally, the nonzero entries in the estimate      will \nall be associated with the columns of \nAfrom a single object class i, and we can easily assign the test sample yto that class.However, noise and modeling error may lead to small nonzero entries associated with multipleobject classes (see Fig.3)'b'23Based on the global, sparse representation, one can design many possibly classifiers to resolve \nthis. For instance, we can \nsimply assignyto the object class with the single largest entryin     .However, such heuristics do not harness the subspace structure associated with images in \nface recognition.To better harness such linear structure, we \ninstead classify ybased on how well the coefficients associated with all training samples\nof each object reproduce \ny'b'24MethodsFor each class \ni, let                       be the characteristic function which selects the coefficients associated with the i-thclass. For                                is a new vector whose only nonzero entries are the entries in xthat are associated with class i. Using only the coefficients associated with the i-thclass, one can approximate the given test sample yas                     .We then classify \nybased on these \napproximations by assigning it to the object class \n\nthat minimizes the residual between \nyand     :\n(9)'b'25=A1A2A3A00\n0\na21a22a23a24a250\n0\n0\n0a11a22a33a21a22a23a24a25a31a32a33a34'b'26Algorithm 1: Sparse Representation-based Classification (SRC)\n1: Input: a matrix of training samples for kclasses, a test sample           , (and an optional error \ntolerance> 0.)2: Normalize the columns of \nAto have unit 2-norm.3: Solve the 1-minimizationproblem:(Or alternatively, solve\n)4: Compute the residuals \n5: Output: identity(y) = arg min\niri(y).'b'27Example 1 (l1-Minimizationversus l2-Minimization)Randomly select half of the 2,414 images in the \nExtended Yale B database\nas the training set, \nand the rest for testing.In this example, we sub-sample the images from \n\nthe original 192x168 to size 12x10. The pixel \n\nvalues of the down-sampled image are used as \n120-D features stacked as columns of the \nmatrix Ain the algorithm. Hence matrix Ahas size 120x1207, and the system \ny = Axis underdetermined. (See Fig. 3)\nAlgorithm 1 achieves an overall recognition rate \nof 92.1%across the Extended Yale B database'b'28Fig. 3'b'29Fig. 4'b'30D. Validation Based on Sparse \nRepresentationFactIt is important for recognition systems to detect and \nreject invalid test samples (\noutliers).ReasonThe input test image could be \neven of a subject which is \nnot not a face at all.Conventional classifiers methods (e.g. NN or NS)\nUse the residuals ri(y)for validationAccepts or rejects a test sample based on how small the \n\nsmallest residual is\nProblemEach residual ri(y)only measures similaritybetween the test sampleand eachindividual class.(Does not concern other classes'b'31Validation methods in this work\nIn the sparse representation paradigm, the \ncoefficients     are computed \nglobally, in terms of \nimages of all classes. In a sense, it can harness \nthe joint distribution of all classes for validation.the coefficients     are better statistics for validation than the residuals. (See Example 2)'b'32Example 2 (Concentration of Sparse Coefficients)Randomly select an irrelevant image from \nGoogle, and down-sample it to 12x10. Compute the sparse representation of the image \n\nagainst the same training data as in Example 1. \n(See Fig. 5)The distribution of the estimated sparse \n\ncoefficients     contains important information \n\nabout the validity of the test image:\nA valid test image should have a sparse representation \nwhose nonzero entries concentrate mostly on \nonesubject, whereas an\ninvalid image has sparse \ncoefficients spread widely among \nmultiplesubjects.'b'33Fig. 5'b'34Definition 1(Sparsity Concentration Index): a quantitative measure(10)For a solution    found by Algorithm 1, if                , the test image is represented using only images \nfrom a singleobject, and if          \n        , the sparse coefficients are spread evenly over all classes.We choose a threshold      \n         and accept a test image as valid if       \n           , and otherwise \nreject as invalid. In step 5 of Algorithm 1, one may choose to \noutput the identity of yonly if it passes this \ncriterion.'b'35OutlineIntroductionClassification Based on Sparse RepresentationTwo Fundamental Issues in Face \nRecognitionExperimental ResultsConclusion & Future works'b'36A. The Role of Feature ExtractionTraditional roleCrucialReasonThe choice of feature\ntransformation affects the \nsuccess of the classification algorithm. \nResultResearchers developed a wide variety of \nincreasingly complex feature extraction methods \n\n(including nonlinear and kernel features)\nThe role in this workTo reduce data dimension and \nTo reduce computational cost\n'b'37Methods to apply feature extractionSince most feature transformations involve only \nlinear operations(or approximately so), the projection from the \nimage spaceto the feature spacecan be represented as a matrix\nApplying Rto both sides of equation (3) yields:\n(11)In practice, the dimension \ndof the feature space is typically chosen to be much smaller than \nn. In this case, the system of equations\nis underdeterminedin the unknown              .\n'b'38Methods to apply feature extraction (Cond.)Nevertheless, as the desired solution x0is sparse, we can hope to recover it by solving the following \nreduced 1-minimization\nproblem:(12)for a given error tolerance          . \nThus, in Algorithm 1, the matrix A of training images is now replaced by the matrix                   of d-dimensional features; the test image \nyis replaced by its features    .\n'b'39A surprising phenomenon: (the blessing of dimensionality)If the solution \nx0is sparse enough, then with overwhelming probability, it can be correctly recovered via 1-minimizationfrom any sufficiently large number dof linear measurements                 . \nMore precisely, if x0has         \n  nonzeros, then with \noverwhelming probability, (13)random linear measurements are sufficient for 1-minimization(12) to recover the correct sparse solution x0Random features can be viewed as a less-structured counterpart to classical face features, \n\nsuch as Eigenfaces or Fisherfaces.'b'40Definition 2 (Randomfaces)Consider a transform matrix                    whose entries are independently sampled from a zero-mean normal distributionand each row is normalized to unit length. The \nrow vectors of Rcan be viewed as drandom faces in       .Randomfaces are extremely efficient to \ngenerate, as the transformation Ris independent of the training dataset.'b'41RemarkAs long as the correct sparse solution \nx0can be recovered, Algorithm 1 will always give the sameclassificationresult, regardless of the feature\nactually used. Thus, when the dimension of feature \ndexceeds the above bound (13), one should expect that the \nrecognition performance of Algorithm 1 with \n\ndifferent features quickly converges, and the choice of an optimal feature transformation is \nno longer critical: \nEven random projections or downsampled images \nshould perform as well as any other carefully \n\nengineered features.'b'42B. Robustness to Occlusion or CorruptionBy ECC theory,\nredundancy is essential to detecting and correcting errors \nIt is possible to correctly recognize face image\nthe number of image pixels\nis typically far greaterthan the number of subjects\nthat have generated the \nimages (redundant!)We should work with the highest possible \nresolutionfeature extraction discard useful information that \ncould help compensate for the occlusion\nThe most redundant, robust, or informative is the \n\noriginalimages.'b'43Previous methodsMotivationIt is difficult to extraction the information encoded in the \nredundant dataMethodsFocus on spatial localityComputedLocal featuresfrom only a small fraction of the image pixels that are clearly less likely to be corrupted by \nocclusion than holistic features.Examples: ICA, LNMF, Local Binary Patterns, Gabor wavelets, \netc.Problemno bases or features are more\n spatially localized than the \noriginalimage pixels themselves.The role of feature extraction in achieving spatial locality is \nquestionable'b'44In this workWe modified the above linear model (3) as\n(14)where               is a vector of errors a fraction, \n,of its entries are nonzero. The nonzero \nentries of e0model which pixels in yare corrupted or occluded.Assume that the corrupted pixels are a relatively \nsmall portion of the image. The error vector e0, like the vector x0, then has sparse nonzero \nentries. Since y0= Ax0, we can rewrite (14) as\n(15)Here,                                     , so the system y = Bwis always underdeterminedand does not have a unique solution for \nw. 'b'45The rate of occlusionFrom the above discussion about the sparsity of \nx0and e0, the correct generating \nw0= [x0 , e0]has at most                 nonzeros.We might therefore hope to recover \nw0as the sparsest solution to the system \ny = Bw. In fact, if the matrix Bis in general position, then \nas long as            \n  for some     with less than m/2nonzeros,     is the unique sparsest solution. \nThus, if the occlusion ecovers less than        pixels,  50% of the image, the sparsest solution         \nto y = Bwis the true generator, w0= [x0 , e0].'b'46More generally, one can assume that the corrupting error e0has a sparse representation with respect to some basis                         .That is, e0= Aeu0for some sparse vector              . \nHere, we have chosen the special case             \n\nas e0is assumed to be sparse with respect to the \nnatural pixel coordinates.If the error \ne0is instead more sparse w.r.t. \nanother basis, e.g., Fourier or Haar, we can \n\nsimply redefine the matrix Bby appending Ae(instead of the identity I) to Aand instead seek the sparsest solution w0to the equation:(16)'b'47In this way, the same formulation can handle \nmore general classes of (sparse) corruption. \n\nThat is(17)That is, in Algorithm 1, we replace the image \nmatrix Awith the extended matrix B = [A, I]and x with w = [x, e].'b'48If yis an image of subject \ni, the 1-minimization(17) cannot guarantee to correctly recover w0 = [x0, e0]if(18)Generally,              , so (18) implies that the \nlargest fraction of occlusion under which we can \nhope to still achieve perfect reconstruction is \n\n33%.'b'49Once the sparse solution      \n                is computed, setting           \n          recovers a clean image of the subject with occlusion or corruption \ncompensated for. To identify the subject, we slightly modify the \nresidual ri(y)in Algorithm 1, computing it against \nthe recovered image \nyr :(19)'b'50OutlineIntroductionClassification Based on Sparse RepresentationTwo Fundamental Issues in Face \nRecognitionExperimental ResultsConclusion & Future works'b'51DatabasesExtend Yale B\n2,414 frontal-face images of 38 individuals\nImages are cropped to 192x168 pixels\nCaptured under various laboratory controlled lighting conditionsAR Face Data BaseOver 4,000 frontal-face images of 126 \n\nindividualsIncluding illumination change,  different \n\nexpressions and facial disguiseCropped to 165x120 pixels\n'b'521. Feature extraction and classification methodsExtend Yale B\n'b'53AR Face Database'b'542.Recognition with partial featureTop: example features. Bottom: Recognition rates of SRC, \nNN, NS, and SVM on the Extended Yale B database.\n'b'553.Recognition under random corruption'b'56\n\n\n\n\n\n96x84 pixels12x10 pixels'b'574. Recognition under varying level of \ncontinguous occlusion'b'58\n\n\n\n\n\n96x84 pixels12x10 pixels'b'595. Partition scheme to tackle continguous disguise\n'b'60Remark: The use of holistic versus local features in Face RecognitionThe problem is not the choice of representing the \ntest image in terms of a holistic or local basis, but \n\nrather how the representation is computed.\nProperly harnessingredundancy \nand sparsityis the key to error correction and robustness. \nExtracting local or disjoint features can only \nreduce redundancy, resulting in inferior \nrobustness.W. Zhao, et al., \nFace recognition: A literature survey,\nACM Computing Surveys, pp. 399\n458, 2003.'b'616. Rejecting invalid test images'b'627. Robust training set designAn important consideration in designing \nrecognition systems is selecting the number of \n\ntraining images as well as the conditions (lighting, \nexpression, viewpoint etc.) under which they are \n\nto be taken. The training images should be extensive enough \n\nto span the conditions that might occur in the test \nset: they should be sufficient from a pattern \n\nrecognition standpoint.K. Lee, et al., \nAcquiring linear subspaces for face recognition under variable lighting,IEEE T-PAMI, vol. 27, no. 5, pp. 684\n698, 2005.'b'63This paper provides a different, quantitative measure for how robust the \ntraining set is: the amount of worst-case occlusion the \nalgorithm can tolerate is directly determined \nby how neighborly the associated polytope is.\nPolytope: please refer to Prof. David L. Donoho\ns website.'b'64'b'65Training sets with \nwider variation(such as lighting and expression) in the images allow \ngreater robustness to occlusion.However, the training set should not contain too \n\nmany similar images. In the language of signal representation, the \n\ntraining images should form an Incoherent \n\nDictionary.D. Donoho, For most large underdetermined systems of linear equations the \nminimal 1-norm solution is also the sparsest solution,Communucation on Pure and Applied Math , vol. 59, no. 6, pp. 797\n829, 2006.'b'66OutlineIntroductionClassification Based on Sparse RepresentationTwo Fundamental Issues in Face \nRecognitionExperimental ResultsConclusion & Future works\n'b'67Conclusion & Future worksConclusionWe proposed a high performance \nclassification of high-dimensional data (face \n\nimages) via sparsity.The choice of features become less important \n\nthan numbers of features dim.Occlusion and corruption can be handles \nuniformly and robustly in this workFuture worksExtend to object detectionVariations in object pose'b'Thank You!'