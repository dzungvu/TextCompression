b'Overview\nIntroduction\nModeldescription\nASRResults\nExtensions\nMTResults\nComparison\nMainoutcomes\nFuturework\nRecurrentneuralnetworkbasedlanguage\nmodel\nTom\n\na\n\nsMikolov\nBrnoUniversityofTechnology,JohnsHopkinsUniversity\n20.7.2010\n1/24\n'b'Overview\nIntroduction\nModeldescription\nASRResults\nExtensions\nMTResults\nComparison\nMainoutcomes\nFuturework\nOverview\nIntroduction\nModeldescription\nASRResults\nExtensions\nMTResults\nComparisonandmodelcombination\nMainoutcomes\nFuturework\n2/24\n'b'Overview\nIntroduction\nModeldescription\nASRResults\nExtensions\nMTResults\nComparison\nMainoutcomes\nFuturework\nIntroduction\nNeuralnetworkbasedLMsoutperformstandardbackoff\nn-grammodels\nWordsareprojectedintolowdimensionalspace,similar\nwordsareautomaticallyclusteredtogether.\nSmoothingissolvedimplicitly.\nBackpropagationisusedfortraining.\n3/24\n'b'Overview\nIntroduction\nModeldescription\nASRResults\nExtensions\nMTResults\nComparison\nMainoutcomes\nFuturework\nIntroduction\nRecurrentvsfeedforwardneuralnetworks\nInfeedforwardnetworks,historyisrepresentedbycontext\nof\nN\n\n1\nwords-itislimitedinthesamewayasinN-gram\nbackoffmodels.\nInrecurrentnetworks,historyisrepresentedbyneurons\nwithrecurrentconnections-historylengthisunlimited.\nAlso,recurrentnetworkscanlearntocompresswhole\nhistoryinlowdimensionalspace,whilefeedforward\nnetworkscompress(project)justsingleword.\nRecurrentnetworkshavepossibilitytoformshortterm\nmemory,sotheycanbetterdealwithpositioninvariance;\nfeedforwardnetworkscannotdothat.\n4/24\n'b'Overview\nIntroduction\nModeldescription\nASRResults\nExtensions\nMTResults\nComparison\nMainoutcomes\nFuturework\nModeldescription-feedforwardNN\nFigure:\nFeedforwardneuralnetworkbasedLMusedbyY.Bengioand\nH.Schwenk\n5/24\n'b'Overview\nIntroduction\nModeldescription\nASRResults\nExtensions\nMTResults\nComparison\nMainoutcomes\nFuturework\nModeldescription-recurrentNN\nFigure:\nRecurrentneuralnetworkbasedLM\n6/24\n'b'Overview\nIntroduction\nModeldescription\nASRResults\nExtensions\nMTResults\nComparison\nMainoutcomes\nFuturework\nModeldescription\nTherecurrentnetworkhasaninputlayer\nx\n,hiddenlayer\ns\n(alsocalledcontextlayerorstate)andoutputlayer\ny\n.\nInputvector\nx\n(\nt\n)\nisformedbyconcatenatingvector\nw\nrepresentingcurrentword,andoutputfromneuronsin\ncontextlayer\ns\nattime\nt\n\n1\n.\nToimproveperformance,infrequentwordsareusually\nmergedintoonetoken.\n7/24\n'b'Overview\nIntroduction\nModeldescription\nASRResults\nExtensions\nMTResults\nComparison\nMainoutcomes\nFuturework\nModeldescription-equations\nx\n(\nt\n)=\nw\n(\nt\n)+\ns\n(\nt\n\n1)\n(1)\ns\nj\n(\nt\n)=\nf\n \nX\ni\nx\ni\n(\nt\n)\nu\nji\n!\n(2)\ny\nk\n(\nt\n)=\ng\n0\n@\nX\nj\ns\nj\n(\nt\n)\nv\nkj\n1\nA\n(3)\nwhere\nf\n(\nz\n)\nissigmoidactivationfunction:\nf\n(\nz\n)=\n1\n1+\ne\n\nz\n(4)\nand\ng\n(\nz\n)\nissoftmaxfunction:\ng\n(\nz\nm\n)=\ne\nz\nm\nP\nk\ne\nz\nk\n(5)\n8/24\n'b'Overview\nIntroduction\nModeldescription\nASRResults\nExtensions\nMTResults\nComparison\nMainoutcomes\nFuturework\nComparisonofmodels\nModel\nPPL\nKN5gram\n93.7\nfeedforwardNN\n85.1\nrecurrentNN\n80.0\n4xRNN+KN5\n73.5\nSimpleexperiment:4MwordsfromSwitchboardcorpus\nFeedforwardnetworksusedhereareslightlydifferentthan\nwhatBengio&Schwenkuse\n9/24\n'b'Overview\nIntroduction\nModeldescription\nASRResults\nExtensions\nMTResults\nComparison\nMainoutcomes\nFuturework\nResults-WallStreetJournal\nPPL\nWER\nModel\nRNN\nRNN+KN\nRNN\nRNN+KN\nKN5-baseline\n-\n221\n-\n13.5\nRNN60/20\n229\n186\n13.2\n12.6\nRNN90/10\n202\n173\n12.8\n12.2\nRNN250/5\n173\n155\n12.3\n11.7\nRNN250/2\n176\n156\n12.0\n11.9\nRNN400/10\n171\n152\n12.5\n12.1\n3xRNNstatic\n151\n143\n11.6\n11.3\n3xRNNdynamic\n128\n121\n11.3\n11.1\nRNNationiswrittenas\nhidden/threshold\n-90/10means\nthatnetworkhas90neuronsinhiddenlayerandthresholdfor\nkeepingwordsinvocabularyis10.\nAllmodelsherearetrainedon6.4Mwords.\nThelargestnetworksperformthebest.\n10/24\n'b'Overview\nIntroduction\nModeldescription\nASRResults\nExtensions\nMTResults\nComparison\nMainoutcomes\nFuturework\nResults-WallStreetJournal\nModel\nDEVWER\nEVALWER\nBaseline-KN5\n12.2\n17.2\nDiscriminativeLM\n11.5\n16.9\nJointLM\n-\n16.7\nStatic3xRNN+KN5\n11.0\n15.5\nDynamic3xRNN+KN5\n10.7\n16.3\nDiscriminativeLMisdescribedinpaper\nPuyangXuandDamianos\nKarakosandSanjeevKhudanpur.Self-SupervisedDiscriminative\nTrainingofStatisticalLanguageModels.ASRU2009.\nModelsare\ntrainedon37Mwords.\nJointLMisdescribedinpaper\nDenisFilimonovandMaryHarper.2009.\nAjointlanguagemodelwithrainsyntactictags.InEMNLP.\nModels\naretrainedon70Mwords.\nRNNsaretrainedon6.4Mwordsandareinterpolatedwithbackoff\nmodeltrainedon37Mwords.\n11/24\n'b'Overview\nIntroduction\nModeldescription\nASRResults\nExtensions\nMTResults\nComparison\nMainoutcomes\nFuturework\nResults-RT05\nModel\nWERstatic\nWERdynamic\nRT05LM\n24.5\n-\nRT09LM-baseline\n24.1\n-\n3xRNN+RT09LM\n23.3\n22.8\nRNNsaretrainedonlyonin-domaindata(5.4Mwords).\nBackoffmodelsaretrainedonmorethan1300Mwords.\n12/24\n'b'Overview\nIntroduction\nModeldescription\nASRResults\nExtensions\nMTResults\nComparison\nMainoutcomes\nFuturework\nExtensions-Dynamicmodels\nLanguagemodelsareusually\nstatic\n.Testingdatadonot\nchangemodelsdirectly.\nBy\ndynamic\nlanguagemodelwedenotemodelthat\nupdatesitsparametersasitprocessesthetestingdata.\nInWSJresults,wecanseeimprovementonDEVsetand\ndegradationonEVALset.Currentexplanationisthat\ntestingdataneedtokeepnaturalorderofsentences,\nwhichistrueonlyforDEVdata.\n13/24\n'b'Overview\nIntroduction\nModeldescription\nASRResults\nExtensions\nMTResults\nComparison\nMainoutcomes\nFuturework\nCharacterbasedLMs-Results\nModel\nLogProbability\n5gram\n-175000\n9gram\n-153000\nbasicRNN640\n-170000\nBPTTRNN640\n-150000\nSimplerecurrentneuralnetworkcanlearnlongercontext\ninformation.However,itistogobeyond5-6grams.\nBackpropagationthroughtimealgorithmworksbetter:\nresultingnetworkisbetterthanthebestbackoffmodel.\nComputationalcostisveryhighashiddenlayersneedto\nbehugeandnetworkisevaluatedforeverycharacter.\n14/24\n'b'Overview\nIntroduction\nModeldescription\nASRResults\nExtensions\nMTResults\nComparison\nMainoutcomes\nFuturework\nResults-IWSLT2007Chinese\n!\nEnglish\nModel\nBLEU\nBaseline\n0.493\n+4xRNN\n0.510\nMachinetranslationfromChinesetoEnglish.\nRNNsareusedtoprovideadditionalscorewhenrescoring\nN-bestlists.\n400KwordsintrainingdatabothforbaselineandforRNN\nmodels.Smallvocabularytask.\n15/24\n'b'Overview\nIntroduction\nModeldescription\nASRResults\nExtensions\nMTResults\nComparison\nMainoutcomes\nFuturework\nResults-NISTMT05Chinese\n!\nEnglish\nModel\nBLEU\nNIST\nBaseline\n0.330\n9.03\nRNN3M\n0.338\n9.08\nRNN17M\n0.343\n9.15\nRNN17Mfull+c80\n0.347\n9.19\nNISTMT05:translationofnewspaper-styletext.Large\nvocabulary.\nRNNLMsaretrainedonupto17Mwords,baselinebackoff\nmodelsonmuchmore.\nRNNc80denotesneuralnetworkusingcompressionlayer\nbetweenhiddenandoutputlayers.\n16/24\n'b'Overview\nIntroduction\nModeldescription\nASRResults\nExtensions\nMTResults\nComparison\nMainoutcomes\nFuturework\nExtensions-compressionlayer\nModel\nBLEU\nRNN17M250/5full\n0.343\nRNN17M500/5c10\n0.337\nRNN17M500/5c20\n0.341\nRNN17M500/5c40\n0.341\nRNN17M500/5c80\n0.343\nHiddenlayerkeepsinformationaboutthewholehistory,\nsomeofthatmightnotbeneededtocomputeprobability\ndistributionofthenextword.\nByaddingsmall\ncompression\nlayerbetweenhiddenand\noutputlayers,amountofparameterscanbereducedvery\n(morethan10x).\nNetworkscanbetrainedindaysinsteadofweeks(witha\nsmalllossofaccuracy).\n17/24\n'b'Overview\nIntroduction\nModeldescription\nASRResults\nExtensions\nMTResults\nComparison\nMainoutcomes\nFuturework\nComparisonandmodelcombination-UPenn\nUPennTreebankportionoftheWSJcorpus.\n930Kwordsintrainingset,74Kindevsetand82Kintest\nset\nOpenvocabularytask,vocabularyisgivenandislimitedto\n10Kwords.\nStandardcorpususedbymanyresearcherstoreportPPL\nresults.\n18/24\n'b'Overview\nIntroduction\nModeldescription\nASRResults\nExtensions\nMTResults\nComparison\nMainoutcomes\nFuturework\nBackpropagationthroughtime-UPenncorpus\nSteps\n1\n2\n3\n4\n5\n6\n7\n8\nPPL\n145.9\n140.7\n141.2\n135.1\n135.0\n135.0\n134.7\n135.1\nTableshowsperplexitiesfordifferentamountofstepsfor\nwhicherrorispropagatedbackintime(1stepcorresponds\ntobasictraining).\nBPTTextendstrainingofRNNsbypropagatingerror\nthroughrecurrentconnectionsintime.\nResultsareshownondevsetofUPenncorpus(930K\nwordsintrainingset)\nResultsareaveragesfrom4modelstoavoidnoise.\nBPTTprovides7.5%improvementinPPLoverbasic\ntrainingforthisset.\nWithmoredata,thedifferenceshouldbegettingbigger.\n19/24\n'b'Overview\nIntroduction\nModeldescription\nASRResults\nExtensions\nMTResults\nComparison\nMainoutcomes\nFuturework\nComparisonandmodelcombination-UPenn\nModel\nPPL\nEntropyreduction\nGT3\n165.2\n-2.2%\nKN5\n147.8\n0%\nKN5+cache\n133.1\n2.1%\nStructuredLM(Chelba)\n148.9\n-0.1%\nStructuredLM(Roark)\n137.2\n1.5%\nStructuredLM(Filimonov)\n127.2\n3%\nRandomForest(PengXu)\n131.9\n2.3%\nPAQ8o10t\n131.1\n2.3%\nSyntacticNN(Emami,baselineKN4141)\n107\n5.5%\n8xRNNstatic\n105.4\n6.8%\n8xRNNdynamic\n104.5\n6.9%\nstatic+dynamic\n97.4\n8.3%\n+KN5\n93.9\n9.1%\n+KN5(cache)\n90.4\n9.8%\n+Randomforest(PengXu)\n87.9\n10.4%\n+StructuredLM(Filimonov)\n87.7\n10.4%\n20/24\n'b"Overview\nIntroduction\nModeldescription\nASRResults\nExtensions\nMTResults\nComparison\nMainoutcomes\nFuturework\nUPenn:datasampling:KN5n-grammodel\ncokecommonclosedat$Nashareincludingmodesthighbackedbywithitsproposed\nfordeniedbyequivalenttoibmthetheybuilda\n<\nunk\n>\nusedinoctoberNrepublics\naustralia'sdomesticaffairsand\n<\nunk\n>\nbutbyprivatepracticeofthegovernmenttothe\ntechnologytraderssay\nruralbusinessbuoyedbyimproved\n<\nunk\n>\nso\n<\nunk\n>\nthat\n<\nunk\n>\nup\n<\nunk\n>\nprogress\nspendingwentintonielsenvisitedwereissuedsoaringsearchingforanequitygiving\nvaluedat$Nto$N\nbutamodestwhattodoit\ntheeffortintoits\n<\nunk\n>\nspentby\n<\nunk\n>\nin\nachanceaffectingpriceafter-taxlegislatorboardcloseddownNcents\nsircouldbesoldprimarilybecauseoflowoverthe\n<\nunk\n>\nforthestudyillustratesthe\ncompanyone-thirdtoexecutivesnotecrossthatwillsellbymr.investments\nwhichnewhoweversaid\nhe\n<\nunk\n>\nup\nmr.rosencontendsthatvaccinenearbyinplanstotakeandwilliamgray\nbuthiscapital-gainsprovision\nabigengaginginotherandnewpreferredstockwasn'tchevroletbiddersanswered\nwhatiasbigwereimprovementsinauntillasttheontheeconomy\n<\nunk\n>\nappearance\nengineeredandporteranaustraliandollarshaltedtoboostsagging\n<\nunk\n>\nwhich\npreviouslyannouncedacceptedacheaperpersonalindustriesthedownwarditsN\nsupportthesameperiod\nthestatedepartmentsayis$N\n21/24\n"b"Overview\nIntroduction\nModeldescription\nASRResults\nExtensions\nMTResults\nComparison\nMainoutcomes\nFuturework\nUPenn:datasampling:RNNmixture\nmeanwhileamericanbrandsissuedanewrestructuringmixto\n<\nunk\n>\nfromcontinuing\noperationsinthewest\npeter\n<\nunk\n>\nchiefexecutivesaysthefamilyariz.isleftgettobeworkingwith\nthedollar\nitgrewthesomewhat\n<\nunk\n>\nthatdidn'tblameanyovercapacity\niftheoriginalalsoapparentlymightbeabletoshow\nitwasonnov.N\nthestockoverthemostresultsofthisisverylowbecausehecouldn'tdevelopthe\nsenatesaysrep.edwardbradleyabros.vowedtosuittheunit'slatestminister\nihelpsyouknowwhodidn'tsomehowhegotacourseandnowarrived\nthattherewon'tbedrawnprovidesima\n<\nunk\n>\ntobetterinformationmanagementin\nseveralmonths\nthe\n<\nunk\n>\nworld-widebayareaalthoughdecliningstockthatwereplanningbythat\nreservescontinuesasworkersataspeciallevelofseveralgoldslowly\n<\nunk\n>\nand\n<\nunk\n>\nminingstocksandweren'tdisclosed\nsilverarefortax-freecollegedetailsandtheuniversityofhawaii\ncellularclaimshoweverthatwentintobuildingmanufacturinghuge\n<\nunk\n>\nweneedtomoveupwithliquidityandlittleasmuchasprogramsthat\n<\nunk\n>\nadopted\nforcescannecessary\nstockpricesrecoveredpaidtowardaseconddiscounttoevenaboveNN\nthelatest10-yearinterbankmisstated\n<\nunk\n>\ninnewyorkarizonapeak\nmerrilllynchcapitalmarkets\n22/24\n"b'Overview\nIntroduction\nModeldescription\nASRResults\nExtensions\nMTResults\nComparison\nMainoutcomes\nFuturework\nMainoutcomes\nRNNLMisprobablythesimplestlanguagemodeltoday.\nAndverylikelyalsothemostintelligent.\nIthasbeenexperimentallyproventhatRNNLMscanbe\ncompetitivewithbackoffLMsthataretrainedonmuch\nmoredata.\nResultsshowinterestingimprovementsbothforASRand\nMT.\nSimpletoolkithasbeendevelopedthatcanbeusedtotrain\nRNNLMs.\nThisworkprovidesclearconnectionbetweenmachine\nlearning,datacompressionandlanguagemodeling.\n23/24\n'b'Overview\nIntroduction\nModeldescription\nASRResults\nExtensions\nMTResults\nComparison\nMainoutcomes\nFuturework\nFuturework\nClusteringofvocabularytospeeduptraining\nParallelimplementationofneuralnetworktraining\nalgorithm\nEvaluationofBPTTalgorithmforalotoftrainingdata\nGobeyondBPTT?\nComparisonagainstthelargestpossiblebackoffmodels\n24/24\n'