b'IEEE TRANSACTIONS ON \nPATTERN ANALYSIS AND MACHINE INTELLIGENCE. VOL. 12. NO. 5. MAY 1990 REFERENCES H. Freeman and \nL. Garder, Apictorial jigsaw \npuzzles: The com- \nputer solution \nof a problem in pattern recognition, IEEE Trans. Electron. Compui., vol. EC-13, pp. 118-127, Apr. 1964. \nG. M. Radack and N. I. Badler, Jigsaw \npuzzle matching using \na boundary-centered polar encoding, Comput. Graphics \nImage Pro- cessing, vol. 19, \npp. 1-17, 1982. H. Wolfson, E. Schonberg, A. Kalvin, \nand Y. Lamdan, Solving jigsaw puzzles using \ncomputer vision, Ann. Oper. Res., vol. 12, pp. 51-64, 1988. N. Ayache and \n0. D. Faugeras, HYPER: \nA new \napproach for the recognition and positioning of two-dimensional \nobjects, IEEE Trans. Pattern Anal. \nMachine Intell., vol. PAMI-8. pp. 44-54, Jan. 1986. J. L. Turney, T. N. Mudge, and R. A. Volz, Recognizing \npartially occluded parts, IEEE Trans. Pattern Anal. \nMachine Intell., vol. PAMI-7, no. 4, pp. 410-421, \nJuly 1985. A. Kalvin, \nE. Schonberg, J. T. Schwartz. and \nM. Sharir, Two di- mensional model based boundary matching using \nfootprints,  Int. J. Robotics Res., vol. 5, no. 4, pp. 38-55, 1986. H. Wolfson, On curve matching, in Proc. IEEE Comput. Soc. Workshop Computer Vision, \nMiami Beach, FL. Nov. 1987, \npp. 307- 310. M. D. Ernst and B. E. Flinchbaugh, Image/map correspondence using curve \nmatching, presented at the \nAAA1 Robot Navigation \nSymp., Mar. 28-30, 1989. J. T. Schwartz and \nM. Sharir, \nIdentification of partially obscured objects in two dimensions \nby matching of noisy \ncharacteristic curves, Int. J. Robotics Res., vol. 6, no. 2, pp. 29-44, 1987. \nH. Freeman, Shape description \nvia the use of critical points. Pat- tern Recogn., vol. IO, pp. 159-166, 1978. J. Hong and H. J. Wolfson, An improved model-based matching \nmethod using footprints, in Proc. Int. Con$ Pattern Recognition, Rome, Italy, Nov. 1988, pp. 72-78. P. Weiner, Linear \npattern matching algorithms, in Proc. /4/h Annu. Symp. Switching and Automata Theory, IEEE Comput. Soc., M. McCreight, A space-economical suffix tree construction \nalgo- rithm, J. ACM, vol. 23, no. 2, pp. 262-272, Apr. 1976. \nD. T. Lee and F. P. Preparata, Euclidean \nshortest paths \nin the pres- \nence of rectilinear bariers, Networks, vol. 14, pp. \n393-410, 1984. I. J. Stoker, Diferential Geometry. \nNew York: Wiley-Interscience, 1969. E. Kishon and \nH. Wolfson, 3-D curve matching, \nin Proc. AAAI Workshop Spaiial Reasoning \nand Multisensor Fusion, \nSt. Charles, 1973, pp. 1-11. IL, Oct. 1987, pp. 250-261. Invariant Image Recognition \nby Zernike Moments \nALIREZA KHOTANZAD AND YAW HUA HONG Abstract-This correspondence addresses the problem of rotation, \nscale, and translation invariant recognition of images. \nA new set of \nManuscript received August \n29, 1988; revised October 13, 1989. Rec- ommended for acceptance by C. Y. Suen. This \nwork was \nsupported in part by the Defense Advanced Research \nProjects Agency under Grant MDA- \nA. Khotanzad is with \nthe Image Processing and Analysis Laboratory, \nDepartment of \nElectrical Engineering, Southern Methodist \nUniversity, Dallas, TX 75275. \nY. H. Hong was with \nthe Image Processing and Analysis \nLaboratory, Department of Electrical Engineering, Southern Methodist \nUniversity, Dallas, TX 75275. He is now with Texas Instruments Incorporated, Dallas, TX . 903-86-C-0182. IEEE Log Number 8933763. __ 489 rotation invariant features are introduced. They are the magnitudes \nof a set of orthogonal complex moments of the image known \nas Zernike moments. Scale \nand translation invariance are obtained \nby first nor- malizing the image with respect to these parameters using its regular \ngeometrical moments. \nA systematic reconstruction-based method for \ndeciding the highest order of Zernike moments required in a classifi- \ncation problem is developed. The \nquality of the reconstructed image \nis examined through its comparison to the original \none. More moments \nare included until the reconstructed image from them is close enough \nto the \noriginal picture. \nThe orthogonality property of the Zernike mo- \nments which simplifies the process of image reconstruction makes the \nsuggested feature selection approach practical. Furthermore, features \nof each order can also \nbe weighted according \nto their contribution (their \nimage representation ability) to the reconstruction process. The method \nis tested using clean and noisy images from a 26-class character data \nset and a 4-class lake data \nset. The superiority of Zernike moment fea- \ntures over regular moments \nand moment invariants is experimentally \nverified. Index Terms-Feature selection, image recognition, image recon- \nstruction, invariant pattern recognition, moment invariants, Zernike \nmoments. I. INTRODUCTION An important problem \nin pattern analysis is the automatic rec- ognition of an object \nin a scene regardless of its position, size, \nand orientation. They \narise in a variety \nof situations such \nas inspection and packaging \nof manufactured parts \n[ 141, classification of \nchro- mosomes [3], \ntarget identification \n[2], [15], and scene analysis [5]. \nThe current approaches to \ninvariant two-dimensional \nshape recog- nition include extraction \nof global image information using regular \nmoments [ 151, boundary-based analysis via Fourier descriptors [ 121, [ 141, [ 151, [ 181, or autoregressive \nmodels [9], image repre- sentation by circular harmonic expansion \n[6], and syntactic \nap- proaches [3]. \nA fundamental element of all these \nschemes is defi- nition of a set \nof features for image representation and \ndata reduction. Normally additional transformations \nare needed to achieve the \ndesired invariant properties \nfor the selected features. \nAfter invariant \nfeatures are computed, they are input to a designed \nclassification rule \nto decide \na labeling \nfor the underlying image. \nThe utilization of good \nfeatures is not \nthe only decisive factor \nin the success \nof these methods. An additional parameter to \nbe de- cided upon \nis the number \nof such features \nto be used. However, \nthe majority \nof the existing techniques \nuse an ad hoc procedure \nfor arriving at such a \ndecision. The aim \nof this work is \nto develop new features along with a systematic method for selection of the re- quired number of features needed. Moments and functions \nof moments have \nbeen utilized \nas pattern features in a number of \napplications [I], 121, [7], [15]. Such fea- \ntures capture global information \nabout the image \nand do not require closed boundaries \nas boundary-based methods such \nas Fourier de- scriptors do. Regular moments have by far been the most popular \ntype of moments. They are defined as where mpq is the \n( p + q)th order moment of the continuous image \nfunction f (x, y). For digital images the integrals \nare replaced by summations and \nmpq becomes Hu [7] introduced seven \nnonlinear functions defined \non regular mo- ments which \nare translation, scale, and \nrotation invariant. These \n0162-8828/90/0500-0489$01 .OO 0 1990 IEEE 'b'490 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE. VOL. 12. NO. 5. MAY 1990 seven so called moment invariants were \nused in a number \nof pattern recognition problems \n[2], [ 131. The definition of \nregular moments has the form \nof projection of \nf (x, y) function onto the \nmonomial xpy4. Unfortunately, the basis set xpyq is not orthogonal. Consequently, \nthe recovery \nof image from these moments is quite \ndifficult and computationally expen- \nsive. Moreover, \nit implies that the information content of \nmpq\'s have a certain \ndegree of redundancy. Teague [I61 \nhas suggested \nthe orthogonal moments based on the theory \nof orthogonal poly- \nnomials to overcome \nthe problems associated with \nthe regular \nmo- ments. Zernike moments \nused in this study are a class of such or- \nthogonal moments. The \nreason for selecting them from \namong the other orthogonal moments is that they possess a useful rotation in- \nvariance property. Rotating the image \ndoes not change the \nmagni- tudes of its Zernike moments. Hence, \nthey could be used \nas rota- tion invariant features \nfor image representation. \nThese features could easily be constructed \nto an arbitrary \nhigh order. Another main property of Zernike moments \nis the ease \nof image reconstruction \nfrom them. \nThe orthogonality property enables \none to \nseparate out \nthe individual contribution of each \norder moment (its information \ncontent) to the reconstruction process. Simple addition \nof these in- dividual contributions generates \nthe reconstructed image. Taking advantage of this characteristic, a method \nfor selection of the re- quired number \nof features (maximum order of moments) is devel- oped. This \ntechnique evaluates the image representation ability \nof features of each order moments \nthrough comparison of the recon- \nstructed image by them with \nthe original one. The maximum order required is \nthe one for which the reconstructed image \nis close to \nthe original \none. Furthermore, \none can weight the features accord- \ning to their relative contribution \nto the \nreconstruction process. \nThe defined features on the \nZernike moments are \nonly rotation invariant. To obtain scale and translation invariance, the image \nis first subjected to a normalization process using \nits regular mo- ments. The rotation invariant \nZernike features are then extracted \nfrom the scale and translation normalized image. \nTeh and Chin \n[17] examined noise sensitivity and information \nredundancy of \nZernike moments along \nwith five other moments. \nThey concluded that \nhigher order moments are \nmore sensitive to noise. It was \nalso shown \nthat orthogonal moments including Zer- \nnike moments are better \nthan other types \nof moments in terms of information redundancy and \nimage representation. The organization of this correspondence is as follows. Section I1 defines the Zernike moments and their \nproperties. In Section 111, the image reconstruction from its \nZernike moments \nis shown. Sec- \ntion IV \ndiscusses the \nrotation invariant features obtained from Zer- \nnike moments. \nSection V describes the \nsynthesis based feature se- \nlection method. Section VI contains \nthe scale and translation \nnormalization approach and examines the \nperformance of the pro- \nposed features and \nthe accompanying feature selection method \nthrough experimental studies \ninvolving a 26-class English \ncharac- ter data set \nand a \n4-class lake data set. Performance comparisons \nto moment invariants and regular moments \nare also \npresented in this section. Section VI1 gives the \nconclusion of our study. 11. ZERNIKE MOMENTS In [19], Zernike introduced a set of complex polynomials which \nform a \ncomplete orthogonal set over \nthe interior of the unit circle, i.e., x2 + y2 = 1. Let the set \nof these polynomials be denoted by { Vnm(x, y) } . The form \nof these polynomials is: \nwhere n Positive integer or zero. m P e Positive and negative integers subject \nto constraints n Length of vector from \norigin to (x, y) pixel. Angle between \nvector p and x axis in counterclockwise - 1m( even, Iml I n. direction. R,, ( p) Radial polynomial defined \nas n - 1 in 1 /2 (n - s)! Pn-2r Note that \nRn, ( P) = Rnm( P ). These polynomials are orthogonal and satisfy \n1 a=b 0 otherwise with 6ob = Zernike moments are the projection \nof the image function \nonto these orthogonal basis \nfunctions. The Zernike \nmoment of order n with repetition m for a continuous image \nfunctionf (x, y) that vanishes \noutside the unit circle \nis For a digital \nimage, the \nintegrals are replaced by summations to get n+l A,, = - c Cf(x, y) V,*,(p, e), x2 + y2 5 1. (5) T 11 TO compute the Zernike moments \nof a given \nimage, the center of the image \nis taken as the \norigin and pixel coordinates \nare mapped to the range \nof unit circle, i.e., x2 + y2 5 1. Those pixels falling \noutside the unit circle \nare not used in \nthe computation. Also note \nthat A:", = A,,-,,. 111. IMAGE RECONSTRUCTION FROM ZERNIKE MOMENTS \nSuppose that one \nknows all \nmoments A,, off (x, y) up to? given order nmax. It is \ndesired to reconstruct a \ndiscrete function f (x, y) whose moments exactly match those \noff(x, y) up to the given \norder nmax. Zernike moments are the coefficients of \nthe image ex- pansion into orthogonal \nZemike polynomials. By orthogonality of \nthe Zernike basis \nnmdl n=O m PCx, Y> = C AnmVnrn(p3 6) (6) with m having similar constraints as \nin (3). Note that \nas nmax ap- proaches infinityf(x, y) will approachf(x, y). Since it is \neasier to work with real-valued \nfunctions, one \ncan expand (6) noting that \nV,*,( p, 0) = Vn,-m( P, 0) r 1 'b'IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 12. NO. 5. MAY 1990 f(x, y) = C C (c,, cos me + s,,, sin me) R,,(P) n m>O (7) with . R,,( p) cos me dx dy . Rn,( p) sin me dr dy. Indeed, C,, and S,, expressions could be used in \nplace of (4) to compute A,, as well. This reconstruction process is illustrated \nin Fig. 1 for two 64 X 64 binary images \nof letters E and F. The reconstructed binary im- \nages are generated by using (7) followed by mapping to [0, 2551 range, histogram equalization \n[SI, and binarization using threshold \nof 128. It is evident that lower order \nmoments capture gross shape \ninformation and high frequency \ndetails are \nfilled in by \nhigher order moments. In \nthis example moments \nof order 2 through \n12 are used. \nThe reason for omitting orders 0 and 1 is due to \nthe nature \nof pre- processing done on the original images which \nwill be discussed \nlater. Iv. ROTATION INVARIANT FEATURES \nDERIVED FROM ZERNIKE MOMENTS Consider a rotation \nof the image through angle \na. If the rotated image is denoted \nby fr, the relationship between \nthe original and \nrotated images in the same polar coordinates \nis The Zernike moment expression can be mapped from the xy-plane \ninto the \npolar coordinates by changing the variables \nin double in- \ntegral form \nof (4). This \ncan be seen from [8] where a (x, y)/a ( p, e) denotes the Jacobian \nof the transformation and is the determinant of the matrix \nLap aeJ For this case where x = p cos 0 and y = p sin 0, the Jacobian \nbecomes p. Hence = 1:\' SI f ( p, 0) R,, ( p) exp ( -jme) P dp do. ( 1 1 \n) The Zernike moment of the rotated image in the \nsame coordinate \nis 49 1 Fig. 1. The reconstructed \nimages of \ncharacters E and F. From top row and left to right: original \nimage, reconstructed image with up to second order moment through up to twelfth order moment. \nBy a change of variable = 8 - a . exp ( -jmOl) p dp del exp (-jma) (13) I = A,, exp (-jma). Equation (13) shows that \nZernike moments have \nsimple rotational \ntransformation properties; each Zernike moment merely acquires a phase shift on rotation. \nThis simple \nproperty leads to the \nconclu- sion that the magnitudes \nof the Zernike moments of a rotated image \nfunction remain identical \nto those before rotation. Thus 1 A,, 1, the magnitude of the Zernike moment, can be \ntaken as a rotation \nin- variant feature of \nthe underlying image \nfunction. Note that since \nA,, -, = A,*,, then 1 A,, 1 = I An, -, 1 ; thus, one can concentrate \non 1 A,, I with m 2 0 as far as the defined \nZernike features are con- \ncerned. Table I lists the rotation invariant Zernike features and their \ncorresponding numbers \nfrom order \n0 to order 12. This rotation invariancy property is illustrated \nby an experiment. Fig. 2 shows a 64 x 64 binary image of character A and five ro- tated versions \nof it, with rotation angles of 30, 60, \n150", 180", and 300", respectively. Table I1 is the list \nof the magnitudes \nof their Zernike moments for orders 2 and \n3, their respective sample \nmean p, sample standard deviation \nU, and u/p%, which indicates \nthe percentage \nof spread of the 1 A,, 1 values from their \ncorrespond- ing means. It is observed that rotation invariancy \nis very well \nachieved. For example, u/p% is 0.30% and 0.90% for 1 A,, I and 1 A33 1, respectively. These are to be compared to the exact \ninvari- ances of 0%. The reason for not obtaining exact invariances \nis the discrete form of \nthe image function rather \nthan being a continuous \none. V. FEATURE SELECTION VIA RECONSTRUCTION Having shown that the magnitudes of Zernike \nmoments can \nbe taken as rotation invariant \nfeatures, a main \nquestion to be \nanswered is; how big should \nn be? In \nother words, \nup to what order moments are needed for a good classification \nof a given \ndatabase. In fact, a major shortcoming of \nmany previously developed \nfeature sets \nfor image representation \nis the lack of a systematic method for auto- \nmatic selection \nof this number. A good set of features is \none that can characterize \nand represent \nthe image \nwell. The \ndifference between an image and its recon- \nstructed version \nfrom a finite set \nof its moments is a good measure \n'b'492 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 12. NO. 5. MAY 1990 TABLE I LIST OF ZERNIKE MOMENTS AND THEIR CORRESPONDING NUMBER OF FEATURES FROM ORDER ZERO \nTO ORDER TWELVE images. If H( A, f ) 5 E, where E is a preselected \nthreshold, then it can be concluded that enough information \nis extracted and no \nadditional order of moments needs to be computed, \ni.e., n* = i. The above \nprocedure not only specifies \nthe highest order needed for a prototype, but also provides a means \nfor treating features \nof each order differently. It is \napparent that different \norder moments capture different characteristics \nof the image. One can \nisolate the \ncontribution of ith \norder moments \nto the reconstruction process and \nuse its relative strength \nto weight the corresponding features. The \ncontribution of ith \norder moments to \nthe reconstruction process \ncan be measured \nby computing how much \ncloser& is to f compared to fi ~, . Hamming distance is again employed \nto carry out this task. The contribution of the ith order moments denoted \nby C( i ), is computed as A large positive value of \nC( i ) indicates that \nthe ith order moments do capture a lot \nof important information about \nthe shape. \nOn the other hand, a small positive \nor a negative \nC( i ) is an indication that \nthe corresponding moments \nfocus on unimportant \naspects of the image under study. Consequently, \nit makes sense to weight impor- \ntant (in the \nsense of reconstruction) moments and their correspond- ing features more and \nvice versa. Thus, one can \nintroduce a \nweighting mechanism \nfor the ith order features based on their cor- \nresponding C( i )S. All ith order features \ncould be weighted \nby wi during classification \nstage where, \nQQQQOO Fig. 2. The image of character A and five \nrotated versions of it. From left to right rotation \nangles are: 0. 30, 60, 150, 180, and 300. TABLE 11 SHOWN IN FIG. 2 AND THEIR CORRESPONDING STATISTICS \nMAGNITUDES OF SOME OF THE ZERNIKE MOMENTS FOR ROTATED IMA .GES of the image representation ability of the considered set \nof mo- \nments. The ease \nof image reconstruction from \nZernike moments makes it practical to base the feature selection process on such a \nmeasure. The idea is that \nn*, the maximum needed order, is one which can \ngenerate a reconstructed image which is \nsimilar to \nthe original in \nthe sense \nof a defined \nthreshold. In the \nfollowing dis- \ncussion, we will concentrate on binary images. \nHowever, exten- sion to ray level images is straightforward. \nLet f denote the \nbinary image reconstructed \nby using moments \nof order 0 through i extracted from the original \nimage, f. is gen- erated using where F represents mapping \nto [0, 2551 gray level \nrange, histo- gram equalization, and thresholding at \n128. A simple measure of image representation ability is the difference between \nfr and the original binary \nimagef. The Hamming distance between the \ntwo, H( A, f) is employed \nto quantify this difference. \nThe Hamming distance is the total number of pixels that \nare different in \nthe two and D(n*) = C(j). J = I. c( 1) B 0 If C( i ) is negative, w, is set \nto zero. Note that \nthe w,s sum up to 1 .O. Fig. 3 and Table 111 show the synthesized images and \nthe cor- responding C(i) and w, values for character A when E = 300 pixels. Again \nnote that the zeroth order and first order moments are not used \ndue to the preprocessing explained later. \nThe weight for the second \norder moment is set to zero since there \nis no previous \nimage (i.e., fi ) for comparison. Also, \nnote that \nthe unit circle \npart of a 64 x 64 image which \nconsists of 3096 pixels is the basis \nfor comparison and \nHamming distance calculation. \nIf more than \none prototype exists for a class, each \none may give rise to a different \nn* and w,. In that \ncase, the highest \nn* and the average of w, are used. The same \nprocedure can be \nused in \nthe case \nof gray level images. \nThe only needed modification is \nchanging the difference measure \nfrom the \nHamming distance to either a correlation type measure \nor mean squared error. Up to now, \nthe discussion has centered \non how to select the right order of moments and feature \nweights for a single class \nfrom its \ngiven training samples. \nIn a multiclass \nproblem, the \nhighest order moment to be extracted from an unknown image \nis nn*laX where is the maximum value among all the classes to \nbe considered. VI. EXPERIMENTAL STUDY In this \nsection, the classification \npower of the proposed Zernike moment features and the accompanying feature \nselection method \nis experimentally tested and the results \nare reported. Furthermore, the performance of these features is compared to those of moment in- variants and regular \nmoments. Noise sensitivity \nof Zernike features is also examined. \nA. The Utilized Data Sets Two different data sets \nof shapes are generated. The \nfirst data set consists of 26 upper case \nEnglish characters \nfrom A to Z. Twelve different 64 x 64 binary images from each character (for \na total \nof 314 images) are considered. Four slightly different sil- houettes of each character are \ngenerated and three scaled, \nrotated, 'b'IEEE TRANSACTIONS ON \nPATTERN ANALYSIS \nAND MACHINE INTELLIGENCE. VOL 12. NO. 5. MAY 1990 493 Order 1 H(f ,,f ) I QOOOQQO C(i) w, Fig. 3. The reconstructed \nimages of character A. From left to right: orig- inal image and reconstructed images with up to second order moment through up to seventh order moment. n* = 7 is selected with t = 300. TABLE I11 STATISTICS FOR RECONSTRUCTED IMAGES SHOWN IN FIG. 3 I I ,6328 492 ,0460 Fig. 4. The twelve scaled, translated, \nand rotated images of letter A in the character data \nset. Note the slight intraclass variations \nin shape. and translated versions \nof each silhouette are considered to \nmake up the twelve images per class. \nFig. 4 shows the twelve generated \nimages of character "A". Note the \nwithin class differences \nof shapes and their \nscale, orientation, \nand translation. \nIn Fig. 5 four (unrotated) out \nof twelve images \nof each of the other characters are \nshown. The second data set consists \nof four classes of shapes which are the aerial views \nof lakes Erie, Huron, Michigan, and Superior. Again twelve differently oriented 64 X 64 binary images of each lake are generated. Fig. \n6 shows these images. \nNote that no within class shape difference is considered for this data set. \nAs discussed before, the proposed Zernike features are \nonly ro- tation invariant. But, the considered images \nhave scale and trans- lation differences as well. \nTherefore, prior to extraction \nof Zernike features, these images should \nbe normalized with respect \nto scaling and translation. A regular moment-based approach \nis taken toward this stage \nwhich is discussed in the next section. B. Scale and Translation Normalization \nTo achieve scale \nand translation uniformity, the regular \nmo- ments (i.e., mpq) of each image are utilized. Translation invariancy \nis achieved by transforming the image into a \nnew one whose \nfirst order moments, \nmol and mlO, are both equal to zero. \nThis is done \nby transforming the \noriginalf (x, y) image into \nanother one \nwhich isf (x + X, y + j ), where X and j are the centroid location \nof the original image computed \nfrom Fig. 5. Four out of the twelve images \nof letters B to 2 in the character data set. The remaining eight images per character \nare rotated, scaled, \nand translated versions \nof the shown four. In other words, the origin \nis moved to the centroid before \nmoment calculation. Scale invariancy \nis accomplished by enlarging or reducing each shape such \nthat its zeroth \norder moment moo is set equal to a \npre- determined value \np. Note that in the case of binary \nimages moo is the total number \nof shape pixels \nin the image. \nLetf(x/a, y/a) represent a scaled \nversion of \nthe image \nfunctionf(x, y). Then, the regular moment \nmPq off (x. p) and mLq, the regular \nmoment of 'b'494 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 12. NO. 5. MAY 1990 Fig. 6. The twelve \nscaled, translated, \nand rotated images \nof Lakes Erie, Huron, Michigan, and Superior. f(x/a, y/a), are related by Since the \nobjective is to have rnh = 0, one can let a = Jp/moo. Substituting a = into m&, one obtains mb = a2moo = p. Thus scale invariancy is achieved by transforming \nthe original im- \nage functionf(x, y) into a new functionf(x/a, y/a), with a = In summary, an image functionf \n(x, y) can be normalized with respect to scale and translation by transforming \nit into g(x, y), where Jp/moo. with (X, y) being the centroid off(x, y) and a = Jp/moo, with 0 a predetermined value. Wherever (x/a + X, y/a + j) does not correspond to a grid location, the \nfunction value associated with it is interpolated from the values of \nthe four \nnearest grid locations \naround it. Fig. 7 shows the effect of this normalization \nstage on images of character "A" using 0 = 800. Fig. 8 shows one normalized image \nof each of the lakes. This scale \nand translation invariancy stage \ndoes affect two of the Zernike features. \nThose features are 1 Aoo I and 1 A, I 1. I A, I is going to be the same for all images and \nIA,, I is equal to zero. This is Fig. 7. The scale \nand translation normalized images \nof those shown \nin Fig. 4. Fig. 8. The scale \nand translation normalized images \nof lakes. One out of the twelve images per lake \nis shown. and s, = 0. (20) Since moo = 0 it is evident \nthat l&-l = I   COO/^) - j(Soo/2) I = P/a for all the normalized images. \nTherefore, I A, I is not taken \nas one of \nthe features utilized in the \nclassification. For 1 A, 1, = \' = \' 1 i g(x, y)xd*-dy = -mlo (21) g(x, y)p cos 0 du dy a r?+,?s I 4 a r?+i?sI and since rnlo = mol = 0 for all normalized \nimages, then lAll I = l(Cl1/2) - j(Sll/2)1 = 0 for them, and IAll I will not \nbe in- cluded as one of the utilized feature. Thus in the following exper- iments, the extracted Zernike features start \nfrom the second order moments. C. Classification Rules Two different classifiers namely nearest-neighbor \n(NN) and min- \nimum-mean-distance (MMD) are used in this study. The descrip- tion of \neach follows. The nearest-neighbor classifier labels \nan unknown image \nrepre- sented by \nan rn-dimensional feature vector \nX = [xI, x2, . . . \n, x,] with the label of \nthe nearest neighbor of \nX among all the \ntraining 'b'IEEE TRANSACTIONS ON \nPATTERN ANALYSIS AND MACHINE INTELLIGENCE. VOL. \n12. NO. 5. MAY 1990 495 samples. The distance \nbetween X and a training sample is measured \nusing Euclidean distance. This \nis a mapping from m-dimensional \nfeature space onto a one-dimensional Euclidean \nspace. However, \nto prevent the domination of a subgroup of features, one \nhas to normalize the \nfeatures. The normalization consists \nof subtracting sample mean and dividing \nby standard deviation \nof the correspond- ing class. In a c class problem let t:) = [ti:), ti:), . . . \n, t;:,)] and N, denote the kth m-dimensional training feature vector \nof the ith class and \nthe number of available training samples \nof class i, respectively. The unknown test sample \nX is classified to class \ni*, where with ;:I and U::) representing the sample mean and standard \ndevia- tion of the jth element of the m-dimensional training feature vector \nof class i. The second classifier \nis a weighted minimum-mean-distance rule. \nEach class is represented by the sample \nmeans and variances \nof its training features. The utilized classifier measures \nthe sum of the squared distance between the feature vector \nof the test image \nX and the mean of the feature \nvectors of each of the classes \nweighted by the inverse of the corresponding variances. The \npurpose of weight- \ning is to balance the \neffect of \neach of the \nm components of the feature vector. \nLet d(X, i ) be the weighted \ndistance between test \nimage X and representation of class i. Then Xis classified to class \ni* for which the \ndistance d(t, i ) is minimum among {d(X, i) i = 1, 2, . . . , c}. D. Error Estimation Schemes To estimate the error \nrate associated with the selected \nfeatures, the available samples must be divided \ninto two sets, one \nfor train- ing (design) and \none for testing. Three \ndifferent partitioning \nschemes are considered. \nThe first method is known as leave-one- out. This means that \nout of N samples from c classes per \ndata- base, N - l of them are used to train the classifier and the remain- \ning one to \ntest it. This \nprocess is repeated \nN times, each time leav- \ning a different \nsample out. Therefore, all \nof samples are ultimately used for testing. The \nratio of the number of misclassifications to the total number \nof tested samples yields an upper bound \nof the classification error for the considered set \n[4]. Since \nthe number \nof samples in the lake data \nset is rather small, it is \nonly tested with this scheme. But, for the character data \nset two additional parti- \ntionings are considered. \nIn a leave-one-out scheme the classifier \nis trained on rotated \nim- ages. To test the rotation invariance \npower of the features, \na second \npartitioning method is \nconsidered. In this \nscheme called trained on unrotated, the \nclassifier is trained on \nthe four \nunrotated images \nper character and \ntested using the remaining rotated \nimages. This \ntranslates into 104 training and \n208 test samples. In the above two cases the \nclassifier sees all \nfour silhouettes of each character during the \ntraining phase. \nThe third partitioning \nscheme is designed to test the sensitivity \nof the method \nto slight variations in shape. The \nclassifier is trained \non three images \nof only \none of the four silhouettes \nper character and is tested using the \nremaining nine images from \nthe other three silhouettes. This \nmeans 78 training and \n234 test samples \nare considered. This \nmethod is \nrefered to as train \non one silhouette. \nE. Classijication \nResults To decide on n*, eight images per \nclass are used. In the case \nof character data set, two \nimages from each \nof the four silhouettes per \ncharacter are used. The \nselected threshold \nis E = 300 pixels. The rationale for selection of this number is that \nit represents around \n10% difference between the original and the reconstructed image \nwhich is a good \ndegree of closeness. The relation between \nE and n* is shown \nlater in this section. Tables \nIV apd V list the obtained n* along with the Hamming distance of H( fn., f) for each of the eight considered images \nin the two data sets. \nA slight modification \nto the algorithm was allowed which limits the \nmaximum order to \n12. Therefore, for some \nof the eight images \nof three characters, \nB, P, and R which do not satisfy the closeness criterion for n 5 12, n* = 12 is selected and no \nhigher order \nis considered. Based on these results, it is \nconcluded that \nfor the character data set, \none needs to extract \nup to twelfth order Zernike \nmoments correspond- \ning to 47 \nfeatures. For the lake data set, \nup to eighth order moments corresponding to 23 \nfeatures are sufficient. The features are not weighted. The effect of weighting is investigated \nlater. The classification accuracy rates obtained \nfor the character data \nset using \n47 features and the three described error estimation schemes are listed \nin the first row of \nTable VI. \nIn the same table classification results using \norders lower \nthan 12 are also presented. Table VI1 provides a relationship between \nE and the selected order. For the lake data \nset only leave-one-out method \nalong with the minimum-mean-distance classifier \nis considered. A perfect classi- \nfication accuracy is obtained. F. Effect of Feature Weighting \nIn the \nnext set of experiments, the \nutilized features \nare weighted according to the \nscheme described \nearlier. The \nresulting recogni- \ntion accuracies \nare very close \nto those obtained using the \nun- weighted features. In each \ncase either \nno improvement is observed or at most the \nerror is decreased \nby one sample. This \nis expected since for the considered \ndata sets, the \nunweighted results \nare very accurate to start with, leaving very little \nroom for improvement. However, it can be argued that the \nabsence of any \nperformance degradation validates \nthe proposed feature weighting \nscheme. G. Effect of Missing Phase Information \nThe considered features \nare the \nmagnitudes of complex Zernike moments. The phase information is dropped \nto obtain rotation in- \nvariance. The effect of deletion \nof phase information on classifi- \ncation is investigated through a set \nof experiments involving four unrotated images of each \ncharacter. These are \nthe images shown in the first \ncolumn of Fig. 4 and \nimages presented in Fig. 5. The first two images per \ncharacter are \nused for training and the remaining \ntwo for testing. The \nnearest neighbor classifier \nis utilized. Two sets \nof experiments are carried out. In the \nfirst set, both magnitudes and \nphases are used as features while \nin the second set \nonly magnitudes are considered. Perfect recognition accuracies \nare obtained for both cases when the maximum allowable order \nis 7 I n* I 12. When n* = 6, magnitudes and phases \ngive perfect accuracy while mag- \nnitudes only yield a \n98% rate (one error \nin 52 tests). Thus it can be inferred that the influence \nof loss of phase information in clas- \nsification is rather insignificant especially \nwhen high order mo- ments are included. H. Performance Comparisons In this \nsection, the performance of \nZernike features is compared to those of moment invariants and \nregular moments. In \nthe case \nof moment invariants six features \nare utilized since generating a big- \nger number of them is not a trivial \ntask. These \nfeatures are log,, \n16; 1, i = 1, 2, . . . \n, 6 where the \n6is are defined in \n[7]. Note that \nin this case there is no need \nto normalize the images \nsince the dis are not onty invariant \nto rotation but \nalso to \nscale and translation. \nThe same \nclassifiers and error \nestimation schemes \nare utilized and \nthe results \nare listed in the third row of \nTable VIII. \nWe also ex- \n'b'496 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE. VOL. 12, NO. 5, MAY 1990 A B TABLE IV n*s AND H( in., f)s FOR EIGHT PROTOTYPE IMAGES OF \nCHARACTER DATA EIGHT) FOR EACH CLASS \nSET. THE LAST COLUMN SHOWS THE SELECTED n* (MAXIMUM n* AMONG 12345678 n,~ n,~ n,n n,H n,H n,H n,H n,H n 7,225 10,285 7,25 7,285 7.219 10,264 7,216 1n.m in 12,375 12,429 \n12,242 12,371 12.372 12,431 12,237 12,377 \n12 M 9,267 11,272 N 8,214 8,298 o i2,i.w 12,220 P 12,365 12,389 11,272 9,288 \n9,280 11.278 11,287 10,279 \n11 8,290 11,207 8,224 8,299 8.273 11,191 11 \n12,226 12,207 12,191 12,1611 12,205 12,220 12 12,345 12,249 12,303 12,338 12,326 12,224 12 Q I 12,316 I 11,247 I 12,297 I 12.311 I 12,284 I 11,272 I 12.289 I 12,255 I 12 R I 10,245 I 12.385 I 12.443 I 12,437 I 10,248 I 12.410 I 12,476 I 12,327 I 12 ~~ S 10,227 11,271 12,185 11,149 11,146 11,230 11,289 12,255 \n12 T 9.291 8,280 9,296 9.286 8,290 9,202 10,141 g.295 in U 10,164 8,240 10,170 9,290 10,147 8,253 10,139 1o.185 IO v 7,253 10,174 10,208 10,175 9,277 10,176 10,213 10,185 IO w Y X Z 11,280 11,261 12,266 \n12,219 1z.1~8 11.22s 12,246 11.250 \n12 g9m 9,225 10.239 11,247 10,217 9,208 10,270 11,291 \n11 12,253 12,272 12,266 12,253 12,240 12,247 12.237 \n12.281 12 12,287 12,137 12,258 12,270 12,163 12,142 12,237 12,268 \n12 TABLE V n*s AND H( A*, f )S FOR EIGHT PROTOTYPE IMAGES OF \nLAKE DATA SET. THE LAST COLUMN SHOWS \nTHE SELECTED \nn* (MAXIMUM n* AMONG EIGHT) FOR EACH CLASS perimented with such features extracted from translation and scaled \nnormalized images \nand got \nvery similar classification results. \nUnlike moment invariants, regular moments \nmpq could be con- \nstructed for any positive p and q. However, as noted earlier, they are not rotation invariant. To experiment with them, the images \nneed to be corrected for rotation as well. This is done by the method \nof principal axis described \nin [ 151. After translation and \nscale nor- malization, the \nprincipal axis of the image \nis found \nand it is rotated \nsuch that this \naxis lines \nup with the horizontal \naxis. However, there \nare problems associated with \nthis technique. \nIf the image \nis n-fold symmetric, there \nwill be multiple possible \nsets of principal axes. In the case of a \ncharacter data set, \nsuch a problem happened \nfor some of the symmetric characters like \nC. In addition, the pres- \nence of even a moderate \namount of noise significantly affects the \naccuracy of rotation correction. To do a fair comparison, \n47 regular moments are extracted from translation, scale, and rotation normalized images. \nThe pq orders are the same as mn orders of the considered Zernike moments. The \nsame classifiers and error estimation schemes \nare utilized and \nthe results are presented in the second \nrow of \nTable VIII. The entries of Table VI11 clearly verify the \nassertion regarding superiority of Zernike \nmoment features over moment invariants and \nregular moments. The same relative strength is preserved \nif less than twelfth \norders are considered. \nI. Performance on Noisy Images In this section, the \nnoise tolerance and \nsensitivity of \nthe Zernike \nfeatures are studied experimentally. Eight \nnoisy images with \ndif- TABLE VI RECOGNITION ACCURACY RATES FOR THE CHARACTEI \nZERNIKE FEATURES. \nNN AND MMD STAND FOR NE AND MINIMUM-MEAN-DISTANCE CLASSIFIERS, I Leave one out Train on unrotated NN MMD DATA SET USING \\REST-NEIGHBOR ESPECTIVELY Train on one silhouette TABLE VI1 THE RELATION BETWEEN THRESHOLD  (IN PIXELS) AND THE SELECTED MAXIMUM ORDER 410 470 560 TABLE VI11 PERFORMANCE COMPARISONS AMONG ZERNIKE, REGULAR MOMENTS, AND MOMENT INVARIANT FEATURES Zernike Features 1 58 40 ferent orientations \nare generated for each character \nby randomly selecting some of the 4096 \npixels of a normalized noise-free binary \nimage and reversing \ntheir values from 0 to 1 or vice versa. \nThe random pixel selection is \ndone according to \na uniform distribution \nbetween 1 and 4096. Different sets with different noise levels \nare generated. The \nsignal-to-noise ratio \n(SNR) of the generated sets are 30 dB, 25 dB, and \n17 dB. Fig. 9 shows one of the eight noisy images of characters \nA and B for different SNRs. Although the square images are shown, only the unit circle portion \nof them are used in \nthe experiments. \nUsing the noise-free normalized im- \nages as training samples, and the noisy images \nas test samples, the \nperformance of the \nfeatures are \ntested in \nthree sets \nof experiments. All twelve \nclean translation and \nscale normalized images, only the four unrotated images, and \nonly the three from \none silhouette im- \nage are \nused for training in the first, second, and \nthird sets of ex- periments, respectively. \nThe results are tabulated in \nTables IX, X, and XI. In [17] it is shown that higher order moments are \nmore sensitive to noise. \nOur experiments verify this point \nsince including them in many \ncases degrades \nthe accuracy. Overall, the \nperfor- mance is rather good \nwhen SNR is 25 dB or higher. VII. CONCLUSION In this correspondence, \na new set of features defined on the \nZer- nike moments which are a mapping of \nan image function onto \na set \n'b'IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE. VOL. 12. NO. 5. MAY 1990 Fig. 9. One out of the eight noisy images \nof characters A and B. From \nleft to right SNR is 30 dB, 25 dB, and 17 dB. TABLE IX RECOGNITION ACCURACY RATES \nFOR THE NOISY CHARACTER \nDATA SET. THE CLASSIFIERS ARE TRAINED ON ALL TWELVE CLEAN IMAGES PER CHARACTER TABLE X RECOGNITION ACCURACY RATES FOR THE NOISY CHARACTER DATA SET. THE CLASSIFIERS ARE TRAINED ON FOUR UNROTATED CLEAN IMAGES \nPER CHARACTER of orthogonal basis functions \nover the unit circle has \nbeen devel- oped. These \nfeatures are the magnitudes \nof complex Zernike \nmo- ments and \nare proven \nto be rotation invariant. \nThe orthogonality property of Zernike moments \nmakes the image reconstruction \nfrom its moments computationally \nsimple. Moreover, it enables one \nto evaluate the image representation ability \nof each order moment as well as its contribution to the reconstruction process. \nBased on \nthis property a systematic method \nfor selection of the required number \nof features in a classification problem is developed. \nThe selected number for the highest \norder moment is the \none which yields a \nreconstructed image which \nis close to the original \none. The dis- crimination power \nof the proposed Zernike moment features \nand the developed feature selection method are tested \nby a series \nof experiments on two different data sets using a nearest-neighbor \nas well as a minimum-mean-distance classifier. The considered \nim- ages have \ndifferences in scale, translation, and rotation. They \nare first normalized with respect to scale and translation using regular moment based \ntechniques. The obtained classification accuracy \nfor a 26-class character data set is \n99%, while a perfect recognition \nrate is reported \nfor 4-class lake data set. Thus one can conclude \nthat the proposed features and the accompanying feature selection \nmethod are quite effective \nfor the image classification problem. \nIn 497 TABLE XI RECOGNITION ACCURACY RATES \nFOR THE NOISY CHARACTER \nDATA SET. THE CLASSIFIERS ARE TRAINED ON THREE IMAGES OF ONE OF THE SILHOUETTES PER CHARACTER addition the superiority \nof Zernike moment features \nover regular moments and moment invariants is \nshown. Finally, the noise sen- \nsitivity of \nZernike features is studied and \nit is concluded that \nthey can perform well in the presence \nof a moderate level \nof noise. REFERENCES [l] Y. S. Abu-Mostafa and D. Psaltis, Image normalization by complex moments, IEEE Trans. Pattern Anal. \nMachine Intell., vol. PAMI- 7, no. 1, pp. 46-55, Jan. 1985. [2] S. A. Dudani, \nK. J. Breeding, and R. \nB. McGhee, Aircraft identi- fication by moment invariants, IEEE Trans. Comput., vol. c-26, no. I, pp. 39-45, Jan. 1983. [3] K. S. Fu, Syntactic Pattern Recognition and Application. \nEngle- wood Cliffs, NJ: Prentice-Hall, 1982. [4] K. Fukunaga, Introduction to Statistical Pattern Recognition. \nNew York: Academic, 1972. [5] R. C. Gonzalez and \nP. Wintz, Digital Image Processing. Reading, MA: Addison-Wesley, 1977. [6] Y. N. \nHsu, H. H. Arsenault, and G. April, Rotational invariant digital pattern recognition \nusing circular harmonic expansion, Appl. Opt., vol. 21, pp. 4012-4015, 1982. \n[7] M. K. Hu, Visual pattern recognition \nby moment invariants, IRE Trans. Inform. Theory, vol. IT-8, pp. 179-187, Feb. 1962. [8] R. E. Johnson, F. L. Kiokemeister, and E. S. Wolk, Calculus with \nAnalytic Geometry, 6th ed. [9] R. L. Kashyap and R. Chellappa, Stochastic models \nfor closed boundary analysis: Representation and reconstruction, \nIEEE Trans. Inform. Theory, vol. IT-27, pp. 627-637, Sept. 1981. [IO] A. Khotanzad and \nY. H. Hong, Rotation and \nscale invariant features \nfor texture classification, in \nProc. Robotics and Automation. \nIASTED, Santa Barbara, \nCA, May 1987, pp. 16-17. [I 11 -, Rotation invariant pattern recognition \nusing Zernike mo- ments, in Proc. 9th ICPR, Rome, Italy, Nov. 1988, pp. 326-328. 1121 A. Krzyzak, \nS. Y. Leung, and \nC. Y. Suen, Reconstruction \nof two- \ndimensional patterns by Fourier descriptors, \nin Proc. 9th ICPR, \nRome, Italy, Nov. 1988, pp. 555-558. [13] S. Maitra, Moment invariants, Proc. IEEE, vol. 67, no. 4, pp. 697-699, Apr. 1979. [14] E. Persoon and K. S. Fu, Shape discrimination \nusing Fourier de- \nscriptors, IEEE Trans. Syst., Man, and Cybern., vol. SMC-7, pp. \n170-179, Mar. 1977. [I51 A. P. Reeves, R. \nJ. Prokop, S. E. Andrews, and F. Kuhl, Three- \ndimensional shape analysis \nusing moments and Fourier descriptors, \nIEEE Trans. Pattern Anal. \nMachine Intell., vol. 10, no. 6, pp. 937- 943, Nov. 1988. [16] M. Teague, Image analysis via the general \ntheory of \nmoments, J. Opt. Soc. Amer., vol. 70, no. 8, pp. 920-930, Aug. 1980. 1171 C. H. Teh and R. T. \nChin, On image analysis \nby the methods of \nmoments, IEEE Trans. Pattern Anal. \nMachine Intell., vol. IO, no. 1181 C. T. Zhan and C. T. Roskies, Fourier descriptors \nfor plane closed \ncurves, IEEE Trans. Comput., vol. (2-21, pp. 269-281, Mar. 1972. [I91 F. Zernike, Physica, vol. 1, p. 689, 1934. Boston, MA: Allyn and Bacon, 1978. 4, pp. 496-513, July 1988. '