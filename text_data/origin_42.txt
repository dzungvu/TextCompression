b'RecurrentConvolutionalNeuralNetworksforSceneLabeling\nPedroO.Pinheiro\n1\n;\n2\nPEDRO\n.\nPINHEIRO\n@\nIDIAP\n.\nCH\nRonanCollobert\n2\nRONAN\n@\nCOLLOBERT\n.\nCOM\n1\nEcolePolytechniqueF\n\ned\n\neraledeLausanne(EPFL),Lausanne,Switzerland\n2\nIdiapResearchInstitute,Martigny,Switzerland\nAbstract\nThegoalofthescenelabelingtaskistoassigna\nclasslabeltoeachpixelinanimage.Toensure\nagoodvisualcoherenceandahighclassaccu-\nracy,itisessentialforamodeltocapturelong\nrange(pixel)labeldependenciesinimages.In\na\nfeed-forward\narchitecture,thiscanbeachieved\nsimplybyconsideringasufylargeinput\ncontextpatch,aroundeachpixeltobelabeled.\nWeproposeanapproachthatconsistsofare-\ncurrentconvolutionalneuralnetworkwhichal-\nlowsustoconsideralargeinputcontextwhile\nlimitingthecapacityofthemodel.Contraryto\nmoststandardapproaches,ourmethoddoesnot\nrelyonanysegmentationtechniquenoranytask-\nfeatures.Thesystemistrainedinan\nend-to-endmanneroverrawpixels,andmod-\nelscomplexspatialdependencieswithlowinfer-\nencecost.Asthecontextsizeincreaseswiththe\nbuilt-inrecurrence,thesystemandcor-\nrectsitsownerrors.Ourapproachyieldsstate-of-\nthe-artperformanceonboththeStanfordBack-\ngroundDatasetandtheSIFTFlowDataset,while\nremainingveryfastattesttime.\n1.Introduction\nInthecomputervision\nscenelabeling\nisthetaskof\nfullylabelinganimagepixel-by-pixelwiththeclassofthe\nobjecteachpixelbelongsto.Thistaskisverychalleng-\ning,asitimpliessolvingjointlydetection,segmentation\nandmulti-labelrecognitionproblems.\nTheimagelabelingproblemismostcommonlyaddressed\nwithsomekindof\nlocal\nconstrainedinitspre-\ndictionswithagraphicalmodel(\ne.g.\nconditionalrandom\nProceedingsofthe\n31\nst\nInternationalConferenceonMachine\nLearning\n,Beijing,China,2014.JMLR:W&CPvolume32.Copy-\nright2014bytheauthor(s).\nmarkovrandominwhich\nglobal\ndecisions\naremade.Theseapproachesusuallyconsistofsegment-\ningtheimageintosuperpixelsorsegmentregionstoas-\nsureavisibleconsistencyofthelabelingandalsototake\nintoaccountsimilaritiesbetweenneighborsegments,giv-\ningahighlevelunderstandingoftheoverallstructureof\ntheimage.Eachsegmentcontainsaseriesofinputfea-\nturesdescribingitandcontextualfeaturesdescribingspa-\ntialrelationbetweenthelabelofneighborsegments.These\nmodelsarethentrainedtomaximizethelikelihoodofcor-\nrectgiventhefeatures(\nVerbeek&Triggs\n,\n2008\n;\nGouldetal.\n,\n2009\n;\nLiuetal.\n,\n2011\n;\nKumar&Koller\n,\n2010\n;\nSocheretal.\n,\n2011\n;\nLempitskyetal.\n,\n2011\n;\nTighe&\nLazebnik\n,\n2013\n).Themainlimitationofscenelabelingap-\nproachesbasedongraphicalmodelsisthecomputational\ncostattesttime,whichlimitsthemodeltosimplecontex-\ntualfeatures.\nInthiswork,weconsidera\nfeed-forwardneuralnetwork\napproachwhichcantakeintoaccountlongrangelabelde-\npendenciesinthesceneswhilecontrollingthecapacityof\nthenetwork.Weachievestate-of-the-artaccuracywhile\nkeepingthecomputationalcostlowattesttime,thanksto\nthecompletefeed-forwarddesign.Ourmethodreliesona\nrecurrentarchitectureforconvolutionalneuralnetworks:a\nsequentialseriesofnetworkssharingthesamesetofpa-\nrameters.EachinstancetakesasinputbothanRGBimage\nandthepredictionsofthepreviousinstanceof\nthenetwork.Thenetworkautomaticallylearnstosmooth\nitsownpredictedlabels.Asaresult,theoverallnetwork\nperformanceisincreasedasthenumberofinstancesin-\ncreases.\nComparedtographicalmodelapproachesrelyingonimage\nsegmentation,oursystemhasseveraladvantages:(i)itdoes\nnotrequireanyengineeredfeatures,sincedeeplearningar-\nchitecturestrain(hopefully)adequatediscriminative\ninanend-to-endmanner,(ii)thepredictionphasedoesnot\nrelyonanylabelspacesearching,sinceitrequiresonlythe\nforwardevaluationofafunction\n.\nThepaperisorganizedasfollows.Section\n2\n\n'b"RecurrentConvolutionalNeuralNetworksforSceneLabeling\nTable1.\nComparisonbetweendifferentmethodsforfullscenelabeling.Theadvantageofourproposedmethodisthesimplicityof\ninference,notrelyingonanyfeatureextractionnorsegmentationmethod.\nMETHODTASK\n-\nSPECIFICFEATURES\n(\nG\nOULDETAL\n.\n,\n2009\n)17-\nDIMENSIONALCOLORANDTEXTUREFEATURES\n,9\nGRIDLOCATIONSAROUNDTHE\nPIXELANDTHEIMAGEROW\n,\nREGIONSEGMENTATION\n.\n(\nM\nUNOZETAL\n.\n,\n2010\n)G\nIST\n,\nPYRAMIDHISTOGRAMOFORIENTEDGRADIENTS\n,\nCOLOR\nH\nISTOGRAM\nCIEL\nAB\n,\nRELATIVERELOCATION\n,\nHIERARCHICALREGIONREPRESENTATION\n.\n(\nK\nUMAR\n&K\nOLLER\n,\n2010\n)C\nOLOR\n,\nTEXTURE\n,\nSHAPE\n,\nPERCENTAGEPIXELSABOVEHORIZONTAL\n,\nREGION\n-\nBASED\nSEGMENTATION\n.\n(\nS\nOCHERETAL\n.\n,\n2011\n)S\nAMEAS\n(\nG\nOULDETAL\n.\n,\n2009\n).\n(\nL\nEMPITSKYETAL\n.\n,\n2011\n)H\nISTOGRAMOFVISUAL\nSIFT,\nHISTOGRAMOF\nRGB,\nHISTOGRAMOFLOCATIONS\n,\nCON\n-\nTOURSHAPE\n\nDESCRIPTOR\n.\n(\nT\nIGHE\n&L\nAZEBNIK\n,\n2013\n)G\nLOBAL\n,\nSHAPE\n,\nLOCATION\n,\nTEXTURE\n/SIFT,\nCOLOR\n,\nAPPEARANCE\n,MRF.\n(\nF\nARABETETAL\n.\n,\n2013\n)L\nAPLACIANPYRAMID\n,\nSUPERPIXELS\n/CRF/\nTREESEGMENTATION\n,\nDATAAUGMENTATION\n.\nO\nUR\nR\nECURRENT\nCNNR\nAWPIXELS\npresentsrelatedworks.Section\n3\ndescribestheproposed\nstrategy.Section\n4\npresentstheresultsofourexperiments\nintwostandarddatasets:theStanfordBackgroundDataset\n(8classes)andtheSIFTFlowDataset(33classes)and\ncomparetheperformancewithothersystems.Finally,Sec-\ntion\n5\nprovidesadiscussionfollowedbyaconclusion.\n2.RelatedWork\nRecurrentNeuralNetworks(RNNs)datebackfromthelate\n80's.Alreadyin(\nJordan\n,\n1986\n),thenetworkwasfed(ina\ntimeseriesframework)withtheinputofthecurrenttime\nstep,plustheoutputofthepreviousone.Severalvari-\nantshavebeenlaterintroduced,suchasin(\nElman\n,\n1990\n).\nRNNshavebeensuccessfullyappliedtowidevarietyof\ntasks,includinginnaturallanguageprocessing(\nStoianov\netal.\n,\n1997\n),speechprocessing(\nRobinson\n,\n1994\n)andim-\nageprocessing(\nGraves&Schmidhuber\n,\n2008\n).Ourap-\nproachcanbeviewedasaparticularinstanceoftheJor-\ndan'srecurrentnetworkadaptedtoimageprocessing(we\nuseaconvolutionalneuralnetworkinstead).Providing\nfeedbackfromtheoutputintotheinputallowsthenetwork\ntomodellabeldependencies,andcorrectitsownprevious\npredictions.\nInapreliminarywork,(\nGrangieretal.\n,\n2009\n)proposed\naninnovativeapproachtoscenelabelingwithouttheuse\nofanygraphicalmodel.Theauthorsproposedasolution\nbasedondeepconvolutionalnetworksrelyingona\nsuper-\nvised\ngreedylearningstrategy.Thesenetworkarchitectures\nwhenfedwithrawpixelsareabletocapturetexture,shape\nandcontextualinformation.\n(\nSocheretal.\n,\n2011\n)alsoconsideredtheuseofdeeplearn-\ningtechniquestodealwithscenelabeling,whereoff-the-\nshelffeaturesofsegmentsarerecursivelymergedtoas-\nsignasemanticcategorylabel.Incontrast,ourapproach\nusestherecurrentarchitecturetoparsethescenewitha\nsmootherclassannotation.\nIn(\nSocheretal.\n,\n2012\n),theauthorsproposedanapproach\nwhichcombinesconvolutionalandrecursivenetworksfor\nclassifyingRGB-Dimages.Theapproachextractsfea-\nturesusingaconvolutionalnetworkwhichisthenfedtoa\nstandardrecurrentnet.Inthatrespect,ourapproachismore\nend-to-end.\nMorerecently,(\nFarabetetal.\n,\n2013\n)investigatedtheuse\nofconvolutionalnetworkstoextractfeaturesfromamul-\ntiscalepyramidofimages.Thissolutionyieldssatisfac-\ntoryresultsforthecategorizationofthepixels,butpoorvi-\nsualcoherence.Inordertoimprovevisualcoherence,three\ndifferentover-segmentationapproacheswereproposed:(i)\nthesceneissegmentedinsuperpixelsandasingleclassis\nassignedtoeachofthesuperpixels,(ii)aconditionalran-\ndomisoverasetofsuperpixelstomodeljoint\nprobabilitiesbetweenthemandcorrectaberrantpixelclas-\n(suchasroadpixelsurroundedbysky),and\n(iii)theselectionofasubsetoftreenodesthatmaximize\ntheaveragepurityoftheclassdistribution,hencemax-\nimizingtheoveralllikelihoodthateachsegmentwillcon-\ntainasingleobject.Incontrast,ourapproachissimplerand\ncompletelyfeed-forward,asitdoesnotrequireanyimage\nsegmentationtechnique,northehandlingofamultiscale\npyramidofinputimages.\nSimilarto(\nFarabetetal.\n,\n2013\n),(\nSchulz&Behnke\n,\n2012\n)\nproposedasimilarmultiscaleconvolutionalarchitecture.In\ntheirapproach,theauthorssmoothoutthepredictedlabels\nwithpairwiseclass\nComparedtoexistingapproaches,ourmethoddoesnotrely\nonanyfeature(seeTable\n1\n).Furthermore,our\nscenelabelingsystemisabletoextractrelevantcontextual\ninformationfromrawpixels.\n3.SystemsDescription\nWeformallyintroduceconvolutionalneuralnetworks\n(CNNs)inSection\n3.1\nandwediscusshowtocapturelong\n"b'RecurrentConvolutionalNeuralNetworksforSceneLabeling\nFigure1.\nAsimpleconvolutionalnetwork.Givenanimagepatchprovidingacontextaroundapixeltoclassify(hereblue),aseriesof\nconvolutionsandpoolingoperationsslidthroughinputplanes)areapplied(here,ve\n4\n\n4\nconvolutions,followedbyone\n2\n\n2\npooling,followedbytwo\n2\n\n2\nconvolutions.Each\n1\n\n1\noutputplaneisinterpretedasascoreforagivenclass.\nrangelabeldependencieswiththesetypesofmodels,while\nkeepingatightcontroloverthecapacity.Section\n3.2\nintro-\nducesourrecurrentnetworkapproachforscenelabeling.\nFinally,inSection\n3.3\n,weshowhowtoinferthefullscene\nlabelinginanefmanner.\n3.1.ConvolutionalNeuralNetworksforSceneLabeling\nConvolutionalneuralnetworks(\nLeCun\n,\n1989\n)areanatu-\nralextensionofneuralnetworksfortreatingimages.Their\narchitecture,somewhatinspiredbythebiologicalvisual\nsystem,possessestwokeypropertiesthatmakethemex-\ntremelyusefulforimageapplications:spatiallyshared\nweightsandspatialpooling.Thesekindofnetworkslearn\nfeaturesthatareshift-invariant,\ni.e.\n,thatareuseful\nacrosstheentireimage(duetothefactthatimagestatistics\narestationary).Thepoolinglayersareresponsibleforre-\nducingthesensitivityoftheoutputtoslightinputshiftand\ndistortions.Thistypeofneuralnetworkhasbeenshownto\nbeveryefinmanyvisionapplications,suchasobject\nrecognition,segmentationand(\nLeCunetal.\n,\n1990\n;\nJarrettetal.\n,\n2009\n;\nTuragaetal.\n,\n2010\n;\nKrizhevsky\netal.\n,\n2012\n).\nAtypicalconvolutionalnetworkiscomposedofmultiple\nstages,asshowninFigure\n1\n.Theoutputofeachstageis\nmadeofasetof2Darrayscalledfeaturemaps.Eachfea-\nturemapistheoutcomeofoneconvolutional(orpooling)\nappliedoverthefullimage.Anon-linearactivation\nfunction(suchasahyperbolictangent)alwaysfollowsa\npoolinglayer.\nInthecontextofscenelabeling,givenanimage\nI\nk\nweare\ninterestedinthelabelofeachpixelatlocation\n(\ni;j\n)\nintheimage.Moreprecisely,thenetworkisfedwitha\nsquared\ncontextpatch\nI\ni;j;k\nsurroundingthepixelatloca-\ntion\n(\ni;j\n)\ninthe\nk\nth\nimage.Itcanbeshown(seeFigure\n1\n)\nthattheoutputplanesize\nsz\nm\nofthe\nm\nth\nconvolutionor\npoolinglayeriscomputedas:\nsz\nm\n=\nsz\nm\n\n1\n\nkW\nm\ndW\nm\n+1\n;\n(1)\nwhere\nsz\n0\nistheinputpatchsize,\nkW\nm\nisthesizeofthe\nconvolution(orpooling)kernelsinthe\nm\nth\nlayer,and\ndW\nm\nisthepixelstepsizeusedtoslidetheconvolution(orpool-\ning)kernelsovertheinputplanes.\n1\nGivenanetworkarchi-\ntectureandaninputimage,onecancomputetheoutputim-\nagesizebysuccessivelyapplying(\n1\n)oneachlayerofthe\nnetwork.Duringthetrainingphase,thesizeoftheinput\npatch\nI\ni;j;k\nischosencarefullysuchthattheoutputlayers\nproduces\n1\n\n1\nplanes,whicharetheninterpretedasscores\nforeachclassofinterest.\nAdoptingthesamenotationas(\nFarabetetal.\n,\n2013\n),the\noutputofanetwork\nf\nwith\nM\nstagesandtrainableparam-\neters\n(\nW\n;\nb\n)\n,foragiveninputpatch\nI\ni;j;k\ncanbeformally\nwrittenas:\nf\n(\nI\ni;j;k\n;(\nW\n;\nb\n))=\nW\nM\nH\nM\n\n1\n;\n(2)\nwiththeoutputofthe\nm\nth\nhiddenlayercomputedas:\nH\nm\n=tanh(\npool\n(\nW\nm\nH\nm\n\n1\n+\nb\nm\n))\n;\n(3)\nfor\nm\n=\nf\n1\n;:::;M\ng\nanddenoting\nH\n0\n=\nI\ni;j;k\n.\nb\nm\nis\nthebiasvectoroflayer\nm\nand\nW\nm\nistheToeplitzma-\ntrixofconnectionbetweenlayer\nm\n\n1\nandlayer\nm\n.The\npool\n(\n\n)\nfunctionisthemax-poolingoperatorand\ntanh(\n\n)\nis\nthepoint-wisehyperbolictangentfunctionappliedateach\npointofthefeaturemap.\nThenetworkistrainedbytransformingthescores\nf\nc\n(\nI\ni;j;k\n;(\nW\n;\nb\n))\n(foreachclassofinterest\nc\n2\nf\n1\n;:::;N\ng\n)intoconditionalprobabilities,byapplyinga\nsoftmax\nfunction:\np\n(\nc\nj\nI\ni;j;k\n;(\nW\n;\nb\n))=\ne\nf\nc\n(\nI\ni;j;k\n;(\nW\n;\nb\n))\nP\nd\n2f\n1\n;:::;N\ng\ne\nf\nd\n(\nI\ni;j;k\n;(\nW\n;\nb\n))\n;\n(4)\n1\nMostpeopleuse\ndW\n=1\nforconvolutionallayers,and\ndW\n=\nkW\nforpoolinglayers.\n'b'RecurrentConvolutionalNeuralNetworksforSceneLabeling\nandmaximizingthelikelihoodofthetrainingdata.More\n,theparameters\n(\nW\n;\nb\n)\nofthenetwork\nf\n(\n\n)\nare\nlearnedinanend-to-endsupervisedway,byminimizing\nthenegativelog-likelihoodoverthetrainingset:\nL\nf\n(\nW\n;\nb\n)=\n\nX\nI\n(\ni;j;k\n)\nln\np\n(\nl\ni;j;k\nj\nI\ni;j;k\n;(\nW\n;\nb\n))\n;\n(5)\nwhere\nl\ni;j;k\nisthecorrectpixellabelclassatposition\n(\ni;j\n)\ninimage\nI\nk\n.Theminimizationisachievedwiththe\nStochasticGradientDescent(SGD)algorithmwithaed\nlearningrate\n\n:\nW\n \nW\n\n\n@L\nf\n@\nW\n;\nb\n \nb\n\n\n@L\nf\n@\nb\n:\n(6)\nScenelabelingsystemsleveragelongrangelabeldepen-\ndenciesinsomeway.Themostcommonapproachisto\naddsomekindofgraphicalmodel(\ne.g.\naconditionalran-\ndomoverlocaldecisions,suchthatacertainglobal\ncoherenceismaintained.Inthecaseofconvolutionalnet-\nworks,anobviouswaytoefcapturelongrangede-\npendencieswouldbetoconsiderlargeinputpatcheswhen\nlabelingapixel.However,thisapproachmightfacegener-\nalizationissues,asconsideringlargercontextoftenimplies\nconsideringlargermodels(\ni.e.\nhighercapacity).\nInTable\n2\n,wereviewpossiblewaystocontrolthecapacity\nofaconvolutionalneuralnetworkbyassumingalargeinput\ncontext.Theeasiestwayisprobablytoincreasethe\nsizesinpoolinglayers,reducingtheoverallnumberofpa-\nrametersinthenetwork.However,performinglargepool-\ningsdecreasesthenetworklabeloutputresolution(\ne.g.\n,if\noneperformsa\n1\n=\n8\npooling,thelabeloutputplanesizewill\nbeabout\n1\n=\n8\nth\noftheinputimagesize).Asshownlaterin\nSection\n3.3\nthisproblemcouldbeovercomedatthecostof\naslowinferenceprocess.\nYetanotherapproachwouldbetheuseofa\nmultiscale\ncon-\nvolutionalnetwork(\nFarabetetal.\n,\n2013\n).Largecontexts\nareintegratedintolocaldecisionswhilemakingthemodel\nstillmanageableintermsofparameters/dimensionality.\nLabelcoherencecanthenbeincreasedbyleveraging,for\ninstance,superpixels.\nAnotherwaytoconsideralargeinputcontextsizewhile\ncontrollingthecapacityofthemodelistomakethenet-\nworkrecurrent.Inthiscase,thearchitecturemightbevery\ndeep(withmanyconvolutionlayers),butparametersbe-\ntweenseverallayersatvariousdepthsare\nshared\n.Wewill\nnowdetailourrecurrentnetworkapproach.\n3.2.RecurrentNetworkApproach\nTherecurrentarchitecture(seeFigure\n2\n)consistsofthe\ncomposition\nof\nP\ninstancesoftheplainconvolutional\nnetwork\nf\n(\n\n)\nintroducedinSection\n3.1\n.Eachinstancehas\nTable2.\nLongrangepixellabeldependenciesintegrationinCNN-\nbasedscenelabelingsystems.Methodstocontrolcapacityand\nspeedofeacharchitectureisreported.\nM\nEANS\nC\nAPACITYCONTROL\nS\nPEED\nGRAPHICAL\nMODEL\n\nSLOW\nMULTISCALE\nSCALEDOWNINPUTIMAGEFAST\nLARGEINPUT\nPATCHES\nINCREASEPOOLING\nRECURRENTARCHITECTURE\nSLOW\nFAST\nidentical\n(shared)trainableparameters\n(\nW\n;\nb\n)\n.Forclar-\nity,wedropthe\n(\nW\n;\nb\n)\nnotationinsubsequentparagraphs.\nThe\np\nth\ninstanceofthenetwork(\n1\n\np\n\nP\n)isfedwith\naninputimage\nF\np\nof\nN\n+3\nfeaturesmaps\nF\np\n=[\nf\n(\nF\np\n\n1\n)\n;I\np\ni;j;k\n]\n;\nF\n1\n=[\n0\n;I\ni;j;k\n]\n:\nwhicharetheoutputlabelplanesofthepreviousinstance,\nandthescaled\n2\nversionoftherawRGBsquaredpatchsur-\nroundingthepixelatlocation\n(\ni;j\n)\nofthetrainingimage\nk\n.\nNotethatthenetworkinstancetakes0labelmapsas\npreviouslabelpredictions.\nAsshowninFigure\n2\n,thesizeoftheinputpatch\nI\ni;j;k\nneededtolabelonepixelincreaseswiththenumberof\ncompositionsof\nf\n.However,thecapacityofthesystem\nremainsconstant,sincetheparametersofeachnetworkin-\nstanceareshared.\nThesystemistrainedbymaximizingthelikelihood\nL\n(\nf\n)+\nL\n(\nf\n\nf\n)+\n:::\n+\nL\n(\nf\n\nP\nf\n)\n;\n(7)\nwhere\nL\n(\nf\n)\nisashorthandforthelikelihoodintroduced\nin(\n5\n)inthecaseoftheplainCNN,and\n\np\ndenotesthe\ncompositionoperationperformed\np\ntimes.Thisway,we\nensurethateachnetworkinstanceistrainedtooutputthe\ncorrectlabelatlocation\n(\ni;j\n)\n.Inthatrespect,thesys-\ntemisabletolearnto\ncorrectitsownmistakes\n(madeby\nearlierinstances).Itcanalsolearn\nlabeldependencies\n,as\naninstancereceivesasinputthelabelpredictionsmadeby\nthepreviousinstancearoundlocation\n(\ni;j\n)\n(seeFigure\n2\n).\nNotethatmaximizing(\n7\n)isequivalenttorandomlyalter-\nnating(withequalweight)themaximizationofeachlikeli-\nhood\nL\n(\nf\n\np\nf\n)\n(for\n1\n\np\n\nP\n).Wechosethisapproach\nforsimplicityofimplementation.\nThelearningprocedureisthesameasforastandardCNN\n(stochasticgradientdescent),wheregradientsarecom-\nputedwiththe\nbackpropagationthroughtime\n(BPTT)al-\ngorithmthenetworkisunfoldedasshowninFig-\nure\n2\nandthenthestandardbackpropagationalgorithmis\napplied.\n2\nI\np\ni;j;k\nis\nI\ni;j;k\nscaledtothesizeof\nf\n(\nF\np\n\n1\n)\n.\n'b'RecurrentConvolutionalNeuralNetworksforSceneLabeling\nFigure2.\nSystemconsideringone(\nf\n),two(\nf\n\nf\n)andthree\n(\nf\n\nf\n\nf\n)instancesofthenetwork.Inallthreecases,thear-\nchitectureproduces\nlabels\n(\n1\n\n1\noutputplanes)\ncorrespondingto\nthepixelatthecenteroftheinputpatch\n.Eachnetworkinstance\nisfedwiththepreviouslabelpredictions,aswellasaRGBpatch\nsurroundingthepixelofinterest.Forspaceconstraints,wedonot\nshowthelabelmapsoftheinstances,astheyarezeromaps.\nAddingnetworkinstancesincreasesthecontextpatchsizeseenby\nthearchitecture(bothRGBpixelsandpreviouspredictedlabels).\n3.3.SceneInference\nGivenatestimage\nI\nk\n,foreachpixelatlocation\n(\ni;j\n)\nthe\nnetworkpredictsalabelas:\n^\nl\ni;j;k\n=argmax\nc\n2\nclasses\np\n(\nc\nj\nI\ni;j;k\n;(\nW\n;\nb\n))\n;\n(8)\nconsideringthecontextpatch\nI\ni;j;k\n.Notethatthisim-\npliespaddingtheinputimagewheninferringlabelofpix-\nelsclosetotheimageborder.Inpractice,simplyextracting\npatches\nI\ni;j;k\nandthenfeedingthemthroughthenetwork\nforallpixelsofatestimageiscomputationallyveryinef-\nInstead,itisbettertofeedthefulltestimage(also\nproperlypadded)totheconvolutionalnetwork:\napplying\noneconvolutiontoalargeimageismuchfasterthanap-\nplyingthesameconvolutionmanytimestosmallpatches\n.\nWhenfedwiththefullinputimage,thenetworkwilloutput\naplaneoflabelscores.However,following(\n1\n),theplane\nsizeissmallerthantheinputimagesize:thisismainlydue\ntopoolinglayers,butalsoduetobordereffectswhenapply-\ningtheconvolution.Forexample,ifthenetworkincludes\ntwo\n2\n\n2\npoolinglayers,only1every4pixelsoftheinput\nimagewillbelabeled.Mostconvolutionalnetworkusers\n(seefore.g.\nFarabetetal.\n,\n2013\n)upscalethelabelplaneto\ntheinputimagesize.\nInfact,itispossibletocomputeefthelabelplane\nwitharesolutionbyfeedingtothenetworkseveralver-\nsionsoftheinputimage,shiftedonthe\nX\nand\nY\naxis.Fig-\nure\n3\nshowsanexampleforanetworkwhichwouldhave\nonlyone\n2\n\n2\npoolinglayer,andoneoutputplane:low\nresolutionlabelplanes(comingoutofthenetworkforthe\ninputimageshiftedby\n(0\n;\n0)\n,\n(0\n;\n1)\n,\n(1\n;\n0)\nand\n(1\n;\n1)\npix-\nels)aremergedtoformthehighresolutionlabelplane.\nMergingisasimplecopyoperationwhichmatchesapixel\ninalowresolutionlabelplanewiththelocationofthecor-\nrespondingoriginalpixeltolabelinthe(highresolution)\ninputplane.Thenumberofforwardsisproportionalto\nthenumberofpoolinglayers.However,thiswouldbestill\nmuchfasterthanforwardingpatchesateachlocationofthe\ntestimage.WewillseeinSection\n4.3\nthathavingala-\nbelresolutioncanincreasetheperformance.\n4.Experiments\nWetestedourproposedmethodontwodifferentfully-\nlabeleddatasets:theStanfordBackground(\nGouldetal.\n,\n2009\n)andtheSIFTFlowDataset(\nLiuetal.\n,\n2011\n).The\nStanforddatasethas715imagesfromruralandurban\nscenescomposedof8classes.Thesceneshaveapproxi-\nmately\n320\n\n240\npixels.Asin(\nGouldetal.\n,\n2009\n),we\nperformeda5-foldcross-validationwiththedatasetran-\ndomlysplitinto572trainingimagesand143testimagesin\neachfold.TheSIFTFlowisalargerdatasetcomposedof\n2688imagesof\n256\n\n256\npixelsand33semanticlabels.\nAllthealgorithmsandexperimentswereimplementedus-\ningTorch7(\nCollobertetal.\n,\n2012\n).\nEachimageofthetrainingsetwasproperlypaddedand\nnormalizedsuchthattheyhavezeromeanandunitvari-\nance.Allnetworksweretrainedbysamplingpatchessur-\nroundingarandomlychosenpixelfromarandomlychosen\nimagefromthetrainingset.Contraryto(\nFarabetetal.\n,\n2013\n)(i)wedidnotconsideradditionofanydistortionon\ntheimages\n3\n,(ii)wedidnotusecontrastivenormalization\nand(iii)wedidnotsampletrainingpatchesaccordingto\nbalancedclassfrequencies.\n3\nWhichisknowntoimprovethegeneralizationaccuracyby\nfewextrapercents.\n'b'RecurrentConvolutionalNeuralNetworksforSceneLabeling\nFigure3.\nConvolutionalneuralnetworksoutput\ndownscaledlabelplanes\n(comparedtotheinputimage)duetopoolinglayers.To\nalleviatethisproblem,onecanfeedseveralshiftedversionoftheinputimage(hererepresentedbypixels\n1\n:::\n25\n)intheXandYaxis.\nInthisexamplethenetworkisassumedtohaveasingle\n2\n\n2\npoolinglayer.Downscaledpredictedlabelplanes(hereinred)arethen\nmergedtogetbackthefullresolutionlabelplaneinanefmanner.Notethatpixelsrepresentedby\n0\nareadequatepadding.\nTable3.\nPixelandaveragedperclassaccuracyandcomputing\ntimeofothermethodsandourproposedapproachesontheStan-\nfordBackgroundDataset.Forrecurrentnetworks,\n\nn\nindicates\nthenumberofcompositions.\nM\nETHOD\nA\nP\nIXEL\n/C\nLASS\nA\nCCURACY\n(%)\nC\nOMPUTING\nT\nIME\n(\nS\n)\n(\nG\nOULDETAL\n.\n,\n2009\n)76.4/-10\nTO\n600\n(\nT\nIGHE\n&L\nAZEBNIK\n,\n2010\n)77.5/-10\nTO\n300\n(\nM\nUNOZETAL\n.\n,\n2010\n)\nz\n76.9/66.212\n(\nK\nUMAR\n&K\nOLLER\n,\n2010\n)79.4/-\n<\n600\n(\nS\nOCHERETAL\n.\n,\n2011\n)78.1/-?\n(\nL\nEMPITSKYETAL\n.\n,\n2011\n)81.9/72.4\n>\n60\n(\nF\nARABETETAL\n.\n,\n2013\n)\n?\n78.8/72.40.6\n(\nF\nARABETETAL\n.\n,\n2013\n)\ny\n81.4/76.060.5\nP\nLAIN\nCNN\n1\n79.4/69.515\nCNN\n2\n(\n\n1\n)67.9/58.00.2\nR\nCNN\n2\n(\n\n2\n)79.5/69.52.6\nCNN\n3\n(\n\n1\n)15.3/14.70.06\nR\nCNN\n3\n(\n\n2\n)76.2/67.21.1\nR\nCNN\n3\n1/2\nRESOLUTION\n(\n\n3\n)79.8/69.32.15\nR\nCNN\n3\n1/1\nRESOLUTION\n(\n\n3\n)80.2/69.910.7\n?\nMultiscaleCNN+superpixels\ny\nMultiscaleCNN+CRF\nz\nUnpublishedimprovedresultshavebeenrecentlyreportedbytheauthors\nWeconsideredtwodifferentaccuracymeasurestocompare\ntheperformanceoftheproposedapproachwithotherap-\nproaches.Theoneistheaccuracyperpixeloftestim-\nages.Thismeasureissimplytheratioofcorrect\npixelsofallimagesinthetestset.However,inscenelabel-\ning(especiallyindatasetswithlargenumberofclasses),\nclasseswhicharemuchmorefrequentthanothers(\ne.g.\nthe\nclassskyismuchmorefrequentthanmoon)havemore\nimpactonthismeasure.Recentpapersalsoconsiderthe\naveragedperclassaccuracyonthetestset(allclasseshave\nthesameweightinthemeasure).Notethatasmentioned\nabove,wedidnottrainwithbalancedclassfrequencies,\nwhichwouldhaveoptimizedthissecondmeasure.\nTable4.\nPixelandaveragedperclassaccuracyofothermethods\nandourproposedapproachesontheSIFTFlowDataset.Forre-\ncurrentnetworks,\n\nn\nindicatesthenumberofcompositions.\nM\nETHOD\nA\nP\nIXEL\n/C\nLASS\nA\nCCURACY\n(%)\n(\nL\nIUETAL\n.\n,\n2011\n)76.67/-\n(\nT\nIGHE\n&L\nAZEBNIK\n,\n2013\n)77.0/30.1\n(\nF\nARABETETAL\n.\n,\n2013\n)78.5/29.6\nP\nLAIN\nCNN\n1\n76.5/30.0\nCNN\n2\n(\n\n1\n)51.8/17.4\nR\nCNN\n2\n(\n\n2\n)76.2/29.2\nR\nCNN\n3\n(\n\n2\n)65.5/20.8\nR\nCNN\n3\n(\n\n3\n)77.7/29.8\nWeconsiderthreeCNNsarchitectures.AplainCNN\n1\n\nwasdesignedtotakelargeinputpatches.CNN\n2\nandCNN\n3\narchitecturesweredesignedsuchthattheirrecurrentver-\nsions(withrespectivelytwoorthreecompositions)would\nstillleadtoareasonableinputpatchsize.WedenoterCNN\ni\nfortherecurrentversionoftheregularconvolutionalnet-\nworkCNN\ni\n.ForrCNN\n3\n,weshowresultsconsidering\nbothhalfresolutionandfull-resolutioninference(seeSec-\ntion\n3.3\n),inwhichweareabletoachievebetterresults(at\nthecostofahighercomputingtime).Table\n3\ncompares\ntheperformanceofourarchitectureswithrelatedworkson\ntheStanfordBackgroundDatasetandTable\n4\ncomparesthe\nperformanceontheSIFTFlowDataset.Notethatthein-\nferencetimeintheseconddatasetdoesnotchange,since\nweexcludetheneedofanysegmentationmethod.Inthe\nfollowing,weprovideadditionaltechnicaldetailsforeach\narchitectureused.\n4.1.PlainNetwork\nCNN\n1\nwastrainedwith\n133\n\n133\ninputpatches.Thenet-\nworkwascomposedofa\n6\n\n6\nconvolutionwith\nnhu\n1\nout-\n'b'RecurrentConvolutionalNeuralNetworksforSceneLabeling\nputplanes,followedbya\n8\n\n8\npoolinglayer,a\ntanh(\n\n)\nnon-linearity,another\n3\n\n3\nconvolutionallayerwith\nnhu\n2\noutputplanes,a\n2\n\n2\npoolinglayer,a\ntanh(\n\n)\nnon-linearity,\nanda\n7\n\n7\nconvolutiontoproducelabelscores.The\nhiddenunitswerechosentobe\nnhu\n1\n=25\nand\nnhu\n2\n=50\nfortheStanforddataset,and\nnhu\n1\n=50\nand\nnhu\n2\n=50\nfortheSIFTFlowdataset.\n4.2.RecurrentArchitectures\nWeconsidertwodifferentrecurrentconvolutionalnetwork\narchitectures.\nThearchitecture,rCNN\n2\n,iscomposedoftwoconsec-\nutiveinstancesoftheconvolutionalnetworkCNN\n2\nwith\nsharedparameters(systeminthecenterofFigure\n2\n).CNN\n2\niscomposedofa\n8\n\n8\nconvolutionwith\n25\noutputplanes,\nfollowedbya\n2\n\n2\npoolinglayer,a\ntanh(\n\n)\nnon-linearity,\nanother\n8\n\n8\nconvolutionallayerwith\n50\noutputplanes,\na\n2\n\n2\npoolinglayer,a\ntanh(\n\n)\nnon-linearity,anda\n1\n\n1\nconvolutiontoproduce\nN\nlabelscores.Asdescribed\ninSection\n3.2\n,rCNN\n2\nistrainedbymaximizingthelikeli-\nhoodgivenin(\n7\n).AsshowninFigure\n2\n,theinputcontext\npatchsizedependsdirectlyonthenumberofnetworkin-\nstancesintherecurrentarchitecture.InthecaseofrCNN\n2\n,\ntheinputpatchsizeis\n25\n\n25\nwhenconsideringonein-\nstance(\nf\n)and\n121\n\n121\nwhenconsideringtwonetwork\ninstances(\nf\n\nf\n).\nThesecondrecurrentconvolutionalneuralnetworkrCNN\n3\niscomposedofamaximumofthreeinstancesoftheconvo-\nlutionalnetworkCNN\n3\nwithsharedparameters.Eachin-\nstanceofCNN\n3\niscomposedofa\n8\n\n8\nconvolutionwith\n25\noutputplanes,followedbya\n2\n\n2\npoolinglayer,a\ntanh(\n\n)\nnon-linearity,another\n8\n\n8\nconvolutionwith\n50\nplanesand\na\n1\n\n1\nconvolutionwhichoutputsthe\nN\nlabelplanes.\nFollowing\n(7)\n,weaimatmaximizing\nL\n(\nf\n)+\nL\n(\nf\n\nf\n)+\nL\n(\nf\n\nf\n\nf\n)\n:\n(9)\nThisappearedtooslowtotrainonasinglecomputerinthe\ncaseofrCNN\n3\n.Instead,weinitializedthesystemby\nstartingtrainingwithtwonetworkinstances(maximizing\nL\n(\nf\n\nf\n)\n).Wethenswitchedtothetrainingofthefullcost\nfunction(\n9\n).Theinputpatchsizeis\n23\n\n23\n,\n67\n\n67\nand\n155\n\n155\nwhenconsideringone,twoorthreeinstancesof\nthenetwork(\nf\n,\nf\n\nf\nand\nf\n\nf\n\nf\n),respectively.\nFigure\n4\nillustratesinferenceoftherecurrentnetworkwith\noneandtwoinstances.Itcanbeseenthatthenetwork\nlearnsitselfhowtocorrectitsownlabelprediction.\nInallcases,thelearningratein(\n6\n)wasequalto\n10\n\n4\n.All\nhyper-parametersweretunedwitha10%held-outvalida-\ntiondata.\n4.3.ComputeTimeandSceneInference\nInTable\n5\n,weanalyzethetradeoffbetweencomputing\ntimeandtestaccuracybyrunningseveralexperimentswith\ndifferentoutputresolutionsforrecurrentnetworkrCNN\n3\n(seeSection\n3.3\nandFigure\n3\n).Labelingabout\n1\n=\n4\nth\nofthe\npixelsseemstobeenoughtoleadtonearstate-of-the-art\nperformance,whilekeepingaveryfastinferencetime.\nTable5.\nComputingtimeandperformanceinpixelaccuracyfor\ntherecurrentconvolutionalnetworkrCNN\n3\nwithdifferentlabel\nresolutionontheStanforddataset.Ouralgorithmswererunona\n4-coreInteli7.\nO\nUTPUT\nR\nESOLUTION\nC\nOMPUTING\nT\nIME\nP\nER\nI\nMAGE\nP\nIXEL\nA\nCCURACY\n1\n=\n8\n0.20\nS\n78.4%\n1\n=\n4\n0.70\nS\n79.3%\n1\n=\n2\n2.15\nS\n79.8%\n1\n=\n1\n10.68\nS\n80.2%\n5.Conclusion\nThispaperpresentedanovel\nfeed-forward\napproachforfull\nscenelabelingbasedonsuperviseddeeplearningstrategies\nwhichmodelinarathersimplewaynon-localclassdepen-\ndenciesinascenefromrawpixels.Wedemonstratedthat\ntheproblemofscenelabelingcanbeeffectivelyachieved\nwithouttheneedofanyexpensivegraphicalmodelorseg-\nmentationtechniquetoensurelabeling.Thescenelabeling\nisinferredsimplybyforwardevaluationofafunctionap-\npliedtoaRGBimage.\nIntermsofaccuracy,oursystemachievesstate-of-the-\nartresultsonbothStanfordBackgroundandSIFTFlow\ndatasets,whilekeepingafastinferencetime.Futurework\nincludesinvestigationofunsupervisedorsemi-supervised\npre-trainingofthemodels,aswellasapplicationtolarger\ndatasetssuchastheBarcelonadataset.\nAcknowledgments\nTheauthorsthankthereviewersfortheirusefulfeedback\nandcomments.ThisworkwassupportedbytheSwiss\nNSFthroughtheSwissNationalCenterofCompetencein\nResearch(NCCR)onInteractiveMultimodalInformation\nManagement(www.im2.ch).\nReferences\nCollobert,R.,Kavukcuoglu,K.,andFarabet,C.Imple-\nmentingneuralnetworksef.In\nNeuralNet-\nworks:TricksoftheTrade\n.Springer,2012.\nElman,J.L.Findingstructureintime.In\nCognitiveSci-\nences\n,1990.\n'b'RecurrentConvolutionalNeuralNetworksforSceneLabeling\nFigure4.\nInferenceresultsofourarchitectures.Thetwoexamples(rows)arefromtheStanfordBackgroundDatasetandthetwo\nlastonesarefromtheSIFTFlowDataset.Firstcolumnistheinputimage.Thesecondcolumnrepresentstheoutputoftheplain\nCNN\n1\nnetwork,thethirdcolumnillustratesresultsofrCNN\n2\nwithoneinstanceandthelastcolumntheresultwiththecompositionof\ntwoinstances:mostmistakesofinstancearecorrectedonthesecondone.Bestviewedincolor.\nFarabet,C.,Couprie,C.,Najman,L.,andLeCun,Y.Learn-\ninghierarchicalfeaturesforscenelabeling.\nIEEETrans-\nactionsonPatternAnalysisandMachineIntelligence\n,\n2013.\nGould,S.,Fulton,R.,andKoller,D.Decomposingascene\nintogeometricandsemanticallyconsistentregions.In\nInternationalConferenceonComputerVision(ICCV)\n,\n2009.\nGrangier,D.,Bottou,L.,andCollobert,R.Deepconvo-\nlutionalnetworksforsceneparsing.In\nInternational\nConferenceonMachineLearning(ICML)DeepLearn-\ningWorkshop\n,2009.\nGraves,A.andSchmidhuber,J.Ofhandwritingrecog-\nnitionwithmultidimensionalrecurrentneuralnetworks.\nIn\nAdvancesinNeuralInformationProcessingSystems\n(NIPS)\n,2008.\n'b"RecurrentConvolutionalNeuralNetworksforSceneLabeling\nJarrett,K.,Kavukcuoglu,K.,Ranzato,MA.,andLeCun,\nY.Whatisthebestmulti-stagearchitectureforobject\nrecognition?In\nProceedingsInternationalConference\nonComputerVision(ICCV'09)\n,2009.\nJordan,M.I.Attractordynamicsandparallelisminacon-\nnectionistsequentialmachine.In\nProceedingsofthe\nEighthAnnualConferenceoftheCognitiveScienceSo-\nciety\n,1986.\nKrizhevsky,A.,Sutskever,I.,andHinton,G.Imagenet\nwithdeepconvolutionalneuralnetworks.\nIn\nAdvancesinNeuralInformationProcessingSystems\n(NIPS)\n,2012.\nKumar,M.P.andKoller,D.Efselectingregions\nforsceneunderstanding.In\nComputerVisionandPattern\nRecognition(CVPR)\n,2010.\nLeCun,Y.Generalizationandnetworkdesignstrategies.In\nConnectionisminPerspective\n.1989.\nLeCun,Y.,Boser,B.,Denker,J.S.,Henderson,D.,\nHoward,R.E.,Hubbard,W.,andJackel,L.D.Hand-\nwrittendigitrecognitionwithaback-propagationnet-\nwork.In\nAdvancesinNeuralInformationProcessing\nSystems(NIPS)\n,1990.\nLempitsky,V.,Vedaldi,A.,andZisserman,A.Apylon\nmodelforsemanticsegmentation.In\nAdvancesinNeural\nInformationProcessingSystems(NIPS)\n,2011.\nLiu,C.,Yuen,J.,andTorralba,A.Nonparametricscene\nparsingvialabeltransfer.\nIEEETrans.PatternAnal.\nMach.Intell.\n,2011.\nMunoz,D.,Bagnell,J.,andHebert,M.Stackedhierarchi-\ncallabeling.In\nProceedingsEuropeanConferenceon\nComputerVision(ECCV)\n,2010.\nRobinson,T.Anapplicationofrecurrentnetstophone\nprobabilityestimation.\nIEEETransactionsonNeural\nNetworks\n,5:298305,1994.\nSchulz,H.andBehnke,S.Learningobject-classsegmenta-\ntionwithconvolutionalneuralnetworks.In\nProceedings\noftheEuropeanSymposiumonNeuralNet-\nworks(ESANN)\n,2012.\nSocher,R.,Lin,C.,Ng,A.,andManning,C.Parsingnatu-\nralscenesandnaturallanguagewithrecursiveneuralnet-\nworks.In\nInternationalConferenceonMachineLearn-\ning(ICML)\n,2011.\nSocher,R.,Huval,B.,Bhat,B.,Manning,C.D.,andNg,\nA.Y.Convolutional-recursivedeeplearningfor3dob-\njectIn\nAdvancesinNeuralInformation\nProcessingSystems(NIPS)\n.2012.\nStoianov,I.,Nerbonne,J.,andBouma,H.Modelling\nthephonotacticstructureofnaturallanguagewordswith\nsimplerecurrentnetworks.In\nComputationalLinguistics\nintheNetherlands\n,1997.\nTighe,J.andLazebnik,S.Superparsing:scalablenon-\nparametricimageparsingwithsuperpixels.In\nEuropean\nconferenceonComputervision(ECCV)\n,2010.\nTighe,J.andLazebnik,S.Superparsing-scalablenon-\nparametricimageparsingwithsuperpixels.\nInternational\nJournalofComputerVision\n,2013.\nTuraga,S.C.,Murray,J.F.,Jain,V.,Roth,F.,Helmstaedter,\nM.,Briggman,K.,Denk,W.,andSeung,H.S.Convolu-\ntionalnetworkscanlearntogenerateafgraphsfor\nimagesegmentation.\nNeuralComputation\n,2010.\nVerbeek,J.andTriggs,B.Scenesegmentationwithcrfs\nlearnedfrompartiallylabeledimages.In\nAdvancesin\nNeuralInformationProcessingSystems(NIPS)\n,2008.\n"