b'Locality-constrainedLinearCodingforImageClassication\nJinjunWang\n,JianchaoYang\n,KaiYu\n,FengjunLv\n,ThomasHuang\n,andYihongGong\nAkiiraMediaSystem,PaloAlto,California\nBeckmanInstitute,UniversityofIllinoisatUrbana-Champaign\nNECLaboratoriesAmerica,Inc.,Cupertino,California\n{jjwang,ygong\n}@akiira.com,{jyang29,huang\n}@ifp.uiuc.edu,{kyu,flv\n}@sv.nec-labs.comAbstractThetraditionalSPMapproachbasedon\nbag-of-features(BoF)requiresnonlinearclassierstoachievegoodim-\n\nageclassicationperformance.Thispaperpresentsasim-\nplebuteffectivecodingschemecalledLocality-constrained\nLinearCoding(LLC)inplaceoftheVQcodingintradi-\n\ntionalSPM.LLCutilizesthelocalityconstraintstoproject\neachdescriptorintoits\nlocal-coordinatesystem\n,andthe\nprojectedcoordinatesareintegratedby\nmaxpooling\ntogen-\neratethenalrepresentation.Withlinearclassier,thepro-\nposedapproachperformsremarkablybetterthanthetra-\nditionalnonlinearSPM,achieving\nstate-of-the-artperfor-\nmanceonseveralbenchmarks.\nComparedwiththesparsecodingstrategy[22],theob-\njectivefunctionusedbyLLChasananalyticalsolution.\nInaddition,thepaperproposesafastapproximatedLLC\nmethodbyrstperformingaK-nearest-neighborsearch\n\nandthensolvingaconstrainedleastsquarettingproblem,\nbearingcomputationalcomplexityof\nO(M+K2).Hence\nevenwithverylargecodebooks,oursystemcanstillprocess\n\nmultipleframespersecond.Thisefciencysignicantly\naddstothepracticalvaluesofLLCforrealapplications.\n1.Introduction\nTherecent\nstate-of-the-artimageclassicationsystems\nconsistoftwomajorparts:\nbag-of-features\n(BoF)[19,4]\nandspatialpyramidmatching\n(SPM)[15].TheBoFmethod\nrepresentsanimageasahistogramofitslocalfeatures.It\nisespeciallyrobustagainstspatialtranslationsoffeatures,\n\nanddemonstratesdecentperformanceinwhole-imagecat-\negorizationtasks.However,theBoFmethoddisregards\ntheinformationaboutthespatiallayoutoffeatures,hence\n\nitisincapableofcapturingshapesorlocatinganobject.\nOfthemanyextensionsoftheBoFmethod,including\nthegenerativepartmodels[7,3,2],geometriccorrespon-\n\ndencesearch[1,14]anddiscriminativecodebooklearn-\ning[13,17,23],themostsuccessfulresultswerereported\nFeature vector [                            ]\nFeature Extraction\nDescriptor\nCoding\nPooling\nCode\nSPM\nConcatenating\nStep 1: Find K-Nearest Neighbors of \nxi, denoted \nas BiStep 2: \nReconstruct \nxiusing \nBic* = argmin || x\ni ciTBi ||2 cImage\nStep 3: ciis an Mx1 vector with K non-zero elements \nwhose values are the corresponding \nc*of step 2LLC Coding process\nKst. \ncj = 1jinput: xicodebook: B={bj} j=1,,Minput: xicode: ciFigure1.Left:owchartofthespatialpyramidstructureforpool-\ningfeaturesforimageclassication.Right:theproposedLLC\ncodingprocess.\nbyusingSPM[15].Motivatedby[10],theSPMmethod\npartitionstheimageintoincreasinglynerspatialsub-\nregionsandcomputeshistogramsoflocalfeaturesfrom\n\neachsub-region.Typically,\n2l2lsubregions,\nl=0,1,2areused.Otherpartitionssuchas\n31hasalsobeenat-\ntemptedtoincorporatedomainknowledgeforimageswith\n\nskyontopand/orgroundonbottom.Theresultedspa-\ntialpyramidisacomputationallyefcientextensionofthe\norderlessBoFrepresentation,andhasshownverypromis-\n\ningperformanceonmanyimageclassicationtasks.\nAtypicalowchartoftheSPMapproachbasedonBoF\nisillustratedontheleftofFigure1.First,featurepointsare\n\ndetectedordenselylocatedontheinputimage,anddescrip-\ntorssuchasSIFTorcolormomentareextractedfrom\neachfeaturepoint(highlightedinbluecircleinFigure1).\n\nThisobtainstheDescriptorlayer.Then,acodebookwith\nMentriesisappliedtoquantizeeachdescriptorandgen-\n1'b'eratetheCodelayer,whereeachdescriptorisconverted\nintoan\nRMcode(highlightedingreencircle).Ifhardvector\nquantization(VQ)isused,eachcodehasonlyonenon-zero\nelement,whileforsoft-VQ,asmallgroupofelementscan\nbenon-zero.NextintheSPMlayer,multiplecodesfrom\n\ninsideeachsub-regionarepooledtogetherbyaveragingand\nnormalizingintoahistogram.Finally,thehistogramsfrom\nallsub-regionsareconcatenatedtogethertogeneratethe-\n\nnalrepresentationoftheimageforclassication.\nAlthoughthetraditionalSPMapproachworkswellfor\nimageclassication,peopleempiricallyfoundthat,to\n\nachievegoodperformance,traditionalSPMhastouseclas-\nsierswithnonlinearMercerkernels,e.g.,Chi-squareker-\nnel.Accordingly,thenonlinearclassierhastoaffordaddi-\n\ntionalcomputationalcomplexity,bearing\nO(n3)intraining\nandO(n)fortestinginSVM,where\nnisthenumberof\nsupportvectors.ThisimpliesapoorscalabilityoftheSPM\napproachforrealapplications.\nToimprovethescalability,researchersaimatobtaining\nnonlinearfeaturerepresentationsthatworkbetterwithlin-\nearclassiers,e.g.[26,22].Inparticular,Yang\netal\n.[22]\nproposedtheScSPMmethodwheresparsecoding(SC)was\n\nusedinsteadofVQtoobtainnonlinearcodes.Themethod\nachievedstate-of-the-artperformancesonseveralbench-\nmarks.Yu\netal\n.[24]empiricallyobservedthatSCresults\ntendtobe\nlocalnonzerocoefcientsareoftenassignedto\nbasesnearbytotheencodeddata.Theysuggestedamodi-\ncationtoSC,calledLocalCoordinateCoding(LCC),which\n\nexplicitlyencouragesthecodingtobelocal,andtheoreti-\ncallypointedoutthatundercertainassumptionslocalityis\nmoreessentialthansparsity,forsuccessfulnonlinearfunc-\n\ntionlearningusingtheobtainedcodes.SimilartoSC,LCC\nrequirestosolveL1-normoptimizationproblem,whichis\nhowevercomputationallyexpensive.\nInthispaper,wepresentanovelandpracticalcoding\nschemecalledLocality-constrainedLinearCoding(LLC),\nwhichcanbeseemasafastimplementationofLCCthat\n\nutilizesthelocalityconstrainttoprojecteachdescriptorinto\nitslocal-coordinatesystem\n.Experimentalresultsshowthat,\nthenalrepresentation(Figure1)generatedbyusingLLC\n\ncodecanachieveanimpressiveimageclassicationaccu-\nracyevenwithalinearSVMclassier.Inaddition,theop-\ntimizationproblemusedbyLLChasananalyticalsolution,\nwherethecomputationalcomplexityisonly\nO(M+M)foreachdescriptor.Wefurtherproposeanapproximated\n\nLLCmethodbyperformingaK-nearest-neighbor(K-NN)\nsearchandthensolvingaconstrainedleastsquaretting\nproblem.Thisfurtherreducesthecomputationalcomplex-\n\nityto\nO(M+K),whereKisthenumberofnearestneigh-\nbors,andusually\nK<0.1M.Asobservedfromourex-\nperiment,usingacodebookwith2048entries,a\n300300imagerequiresonly0.24secondonaverageforprocessing\n(includingdenselocaldescriptorsextraction,LLCcoding\nandSPMpoolingtogetthenalrepresentation).Thisef-\nciencysignicantlyaddstothepracticalvaluesofLLCfor\n\nmanyrealapplications.\nTheremainderofthepaperisorganizedasfollows:Sec-\ntion2introducesthebasicideaoftheLLCcodingandlists\n\nitsattractivepropertiesforimageclassication;Section3\npresentsanapproximationmethodtoallowfastLLCcom-\nputation;InSection4,anincrementaltrainingalgorithms\n\nisproposedtoconstructthecodebookusedbyLLC;In\nSection5,imageclassicationresultsonthreewidelyused\ndatasetsarereported;andFinallyinSection6,conclusions\n\naremade,andsomefutureresearchissuesarediscussed.\n2.Locality-constrainedLinearCoding\nLetXbeasetof\nD-dimensionallocaldescriptorsex-\ntractedfromanimage,\ni.e.X=[x1,x2,...,\nxN]RDN.Givenacodebookwith\nMentries,B=[b1,b2,...,\nbM]RDM,differentcodingschemesconverteachdescriptor\nintoa\nM-dimensionalcodetogeneratethenalimage\nrepresentation.Thissectionreviewstwoexistingcoding\nschemesandintroducesourproposedLLCmethod.\n2.1.CodingdescriptorsinVQ\nTraditionalSPMusesVQcodingwhichsolvesthefol-\nlowingconstrainedleastsquarettingproblem:\nargmin\nCNi=1||xiBci||2(1)s.t.ci0=1,ci1=1,ci0,iwhereC=[c1,c2,...,\ncN]isthesetofcodesfor\nX.The\ncardinalityconstraint\nci0=1meansthattherewillbe\nonlyonenon-zeroelementineachcode\nci,corresponding\ntothequantizationidof\nxi.Thenon-negative,\n1constraintci1=1,ci0meansthatthecodingweightfor\nxis1.Inpractice,thesinglenon-zeroelementisfoundby\nsearchingthenearestneighbor.\n2.2.CodingdescriptorsinScSPM[22]\nToamelioratethequantizationlossofVQ,therestrictive\ncardinalityconstraint\nci0=1inEq.(1)canberelaxed\nbyusingasparsityregularizationterm.InScSPM[22],\nsuchasparsityregularizationtermisselectedtobethe\n1normof\nci,andcodingeachlocaldescriptor\nxithusbe-\ncomesastandardsparsecoding(SC)[16]problem\n1:argmin\nCNi=1xiBci2+ci1(2)1Thenon-negativeconstraintinEq.(1)isalsodroppedout,becausea\nnegative\ncicanbeabsorbedbyippingthecorrespondingbasis.\n'b'Thesparsityregularizationtermplaysseveralimportant\nroles:First,thecodebook\nBisusuallyover-complete,\ni.e.,M>D\n,andhence\n1regularizationisnecessarytoen-\nsurethattheunder-determinedsystemhasauniquesolu-\ntion;Second,thesparsitypriorallowsthelearnedrepresen-\n\ntationtocapturesalientpatternsoflocaldescriptors;Third,\nthesparsecodingcanachievemuchlessquantizationer-\nrorthanVQ.Accordingly,evenwithlinearSVMclassier,\n\nScSPMcanoutperformthenonlinearSPMapproachbya\nlargemarginonbenchmarkslikeCaltech-101[22].\n2.3.CodingdescriptorsinLLC\nInthispaper,wepresentanewcodingalgorithmcalled\nLocality-constrainedLinearCoding(LLC).Assuggested\nbyLCC[24],localityismoreessentialthansparsity,as\n\nlocalitymustleadtosparsitybutnotnecessaryviceversa.\nLLCincorporateslocalityconstraintinsteadofthesparsity\nconstraintinEq.(2),whichleadstoseveralfavorableprop-\n\nertiesasexplainedinSubsection2.4.Specically,theLLC\ncodeusesthefollowingcriteria:\nminCNi=1xiBci2+||dici||2(3)s.t.1ci=1,iwheredenotestheelement-wisemultiplication,and\ndiRMisthelocalityadaptorthatgivesdifferentfreedomfor\neachbasisvectorproportionaltoitssimilaritytotheinput\ndescriptorxi.Specically,\ndi=exp\ndist(xi,B).(4)wheredist\n(xi,B)=[dist(xi,b1),...,\ndist(xi,bM)]T,and\ndist(xi,bj)istheEuclideandistancebetween\nxiandbj.isusedforadjustingtheweightdecayspeedforthelocality\nadaptor.Usuallywefurthernormalize\nditobebetween\n(0,1]bysubtracting\nmaxdist(xi,B)fromdist\n(xi,B).Theconstraint\n1Tci=1followstheshift-invariantrequire-\nmentsoftheLLCcode.NotethattheLLCcodeinEqn.3is\nnotsparseinthesenseof\n0norm,butissparseinthesense\nthatthesolutiononlyhasfewsignicantvalues.Inpractice,\nwesimplythresholdthosesmallcoefcientstobezero.\n2.4.PropertiesofLLC\nToachievegoodclassicationperformance,thecoding\nschemeshouldgeneratesimilarcodesforsimilardescrip-\ntors.Followingthisrequirement,thelocalityregularization\n\nterm||dici||2inEq.(3)presentsseveralattractiveprop-\nerties:1.Betterreconstruction.\nInVQ,eachdescriptorisrep-\nresentedbya\nsinglebasisinthecodebook,asillus-\ntratedinFig.2.a.Duetothelargequantizationerrors,\ninput: \nxicodebook: B={b\nj} j=1,,Minput: xicodebook: B={b\nj} j=1,,Minput: \nxicodebook: B={b\nj} j=1,,MVQSCLLC\nFigure2.ComparisonbetweenVQ,SCandLLC.\nTheselected\nbassesforrepresentationarehighlightedinred\ntheVQcodeforsimilardescriptorsmightbeverydif-\nferent.Besides,theVQprocessignorestherelation-\nshipsbetweendifferentbases.Hencenon-linearker-\nnelprojectionisrequiredtomakeupsuchinformation\nloss.Ontheotherside,asshowninFig.2.cinLLC,\n\neachdescriptorismoreaccuratelyrepresentedbymul-\ntiplebases,andLLCcodecapturesthecorrelationsbe-\ntweensimilardescriptorsbysharingbases.\n2.Localsmoothsparsity.\nSimilartoLLC,SCalso\nachieveslessreconstructionerrorbyusingmultiple\nbases.Nevertheless,theregularizationtermof\n1norminSCisnotsmooth.AsshowninFig.2.b,dueto\ntheover-completenessofthecodebook,theSCprocess\nmightselectquitedifferentbasesforsimilarpatchesto\n\nfavorsparsity,thuslosingcorrelationsbetweencodes.\nOntheotherside,theexplicitlocalityadaptorinLLC\nensuresthatsimilarpatcheswillhavesimilarcodes.\n3.Analyticalsolution.\nSolvingSCusuallyrequires\ncomputationallydemandingoptimizationprocedures.\nForinstance,theFeatureSignalgorithmutilizedby\nYang\netal\n.[22]hasacomputationcomplexityof\nO(MK)intheoptimalcase[16],where\nKdenotesthenumberofnon-zeroelements.UnlikeSC,theso-\nlutionofLLCcanbederivedanalyticallyby:\nci=Ci+diag(d)\\1(5)ci=ci/1ci,(6)whereCi=(B1xi)(B1xi)denotesthedata\ncovariancematrix.AsseeninSection3,theLLCcan\nbeperformedveryfastinpractice.\n3.ApproximatedLLCforFastEncoding\nTheLLCsolutiononlyhasafewsignicantvalues,or\nequivalently,solvingEq.(3)actuallyperformsfeatureselec-\ntion:itselectsthe\nlocalbases\nforeachdescriptortoforma\nlocalcoordinatesystem\n.Thissuggeststhatwecandevelop\nanevenfasterapproximationofLLCtospeeduptheencod-\ningprocess.InsteadofsolvingEq.(3),wecansimplyuse\n\ntheK(K<D<M\n)nearestneighborsof\nxiasthelocal\nbasesBi,andsolveamuchsmallerlinearsystemtogetthe\n'b'codes:minCNi=1||xiciBi||2(7)st.1ci=1,i.Thisreducesthecomputationcomplexityfrom\nO(M2)toO(M+K2),where\nKM.Thenalimplementation\nofsuchapproximatedLLCprocessisillustratedinFigure.1\nright.Thoughthisapproximatedencodingappearstobe\nsimilartoLocalLinearEmbeding[18],thewholeproce-\n\ndureofLLCitselfdiffersfromLLEclearly,becauseLLC\nincorporatesanadditionalcodebooklearningstepwherethe\ninferenceisderivedfromEq.(3).Thecodebooklearning\n\nstepwillbefurtherexplainedinSection4.\nAsKisusuallyverysmall,solvingEq.(7)isveryfast.\nForsearchingK-nearestneighbors,weappliedasimplebut\n\nefcienthierarchicalK-NNsearchstrategy,whereeachde-\nscriptorisrstquantizedintooneof\nLsubspaces,andthen\nineachsubspacean\nRMDcodebookwasapplied.Theef-\nfectivesizeofthecodebookbecomes\nLM.Inthisway,a\nmuchlargercodebookcanbeusedtoimprovethemodeling\n\ncapacity,whilethecomputationinLLCremainsalmostthe\nsameasthatinusingasingle\nRMDcodebook.4.CodebookOptimization\nInalltheabovediscussions,wehaveassumedthatthe\ncodebookisgiven.Asimplewaytogeneratethecode-\nbookistouseclusteringbasedmethodsuchasK-Means\nalgorithm[15].Accordingtoourexperimentalresultsin\n\nSubsection5.4,thecodebookgeneratedbyK-Meanscan\nproducesatisfactoryaccuracy.Inthiswork,weusethe\nLLCcodingcriteriatotrainthecodebook,whichfurther\n\nimprovestheperformance.Inthissection,wepresentan\neffectiveon-linelearningmethodforthispurpose.\nRevisitingEq.(3),nowweseektofactorizeeachtraining\ndescriptorintotheproductofanLLCcodeandacodebook.\nHenceanoptimalcodebook\nBcanbeobtainedby\nargmin\nC,BNi=1||xiBci||2+||dici||2(8)st.1ci=1,i(9)||bj||21,jEq.(8)canbesolvedusingCoordinateDescentmethod\ntoiterativelyoptimizing\nC(B)basedonexisting\nB(C).However,inpractice,thenumberoftrainingdescriptors\nNisusuallylarge(\ne.g\n.,2,000,000+inourexperiment),such\nthatholdingalltheLLCcodestogetherineachiterationis\ntoomemoryconsuming.Henceweapplyanon-linemethod\n\nthatreadsasmallbatchofdescriptors\nxatatimeandincre-\nmentallyupdatethecodebook\nB.Toelaborate,werstuseacodebooktrainedbyK-\nMeansclusteringtoinitialize\nB.Thenweloopthroughall\nthetrainingdescriptorstoupdate\nBincrementally.Ineach\niteration,wetakeinasingleexamples\nxi(orasmallbatch\nofthem),andsolveEq.(3)toobtainthecorrespondingLLC\n\ncodesusingcurrent\nB.Then,asexplainedinSection3,we\nregardsthisstepasafeatureselector,\ni.e.weonlykeepthe\nsetofbasis\nBiwhosecorrespondingweightsarelargerthan\napredenedconstant,andret\nxiwithoutthelocalitycon-\nstraint.Theobtainedcodeisthenusedtoupdatethebasis\ninagradientdescentfashion.Finally,weprojectthoseba-\nsisoutsidetheunitcircleontotheunitcircle.Theabove\nprocessisillustratedinAlg.4.1.\nAlgorithm4.1\nIncrementalcodebookoptimization\ninput:BinitRDM,XRDN,,output:B1:BBinit.2:for\ni=1to\nNdo3:d1Mzerovector,\n{localityconstraintparameter\n}4:for\nj=1to\nMdo5:djexp1xibj2.6:endfor\n7:dnormalize\n(0,1](d){coding}8:ciargmaxc||xiBc||2+||dc||2s.t.1c=1.{removebias\n}9:idj|absci(j)>0.01},BiB(:,id\n),10:ciargmaxc||xiBic||2s.t.jc(j)=1.{updatebasis\n}11:Bi=2ci(xiBici),1/i,12:BiBiBi/|ci|2,13:B(:,id\n)proj\n(Bi).14:endfor\n5.ExperimentalResults\nInthissection,wereportresultsbasedonthreewidely\nuseddatasets:Caltech-101[7],Caltech-256[11],andPas-\n\ncalVOC2007[6].Weusedonlyasingledescriptor,the\nHistogramofOrientedGradient(HOG)[5],throughoutthe\nexperiment.Inoursetup,theHOGfeatureswereextracted\n\nfrompatchesdenselylocatedbyevery8pixelsontheim-\nage,underthreescales,\n1616,2525and3131re-spectively.ThedimensionofeachHOGdescriptoris\n128.DuringLLCprocessing,onlytheapproximatedLLCwas\nused,andthenumberofneighborswassetto\n5(Section3)\nwiththeshift-invariantconstraint.\nIntheSPMlayer,foreachspatialsub-region,thecodes\n'b'ofthedescriptors(e.g.,VQcodes,SCcodesorLLCcodes)\narepooledtogethertogetthecorrespondingpooledfeature.\n\nThesepooledfeaturesfromeachsub-regionareconcate-\nnatedandnormalizedasthenalimagefeaturerepresenta-\ntion.Specicallytwopoolingmethodshavebeenused\nsumpooling[15]::\ncout=cin1+,...,\n+cin2maxpooling[22]:\ncout=max(\ncin1,...,\ncin2)wheremaxfunctionsinarow-wisemanner,returninga\nvectorthesamesizeas\ncin1.Thesepooledfeaturescan\nthennormalizedby\nsumnormalization:\ncout=cin/jcin(j)2normalization:cout=cin/cin2Differentcombinationscanbeexplored,e.g.,sumpool-\ningfollowedbysumnormalizationwithVQcodespro-\n\nducesthehistogram.InourLLCframework,weusedmax\npoolingcombinedwith\n2normalizationasin[22].\nAlltheexperimentswereconductedonaDellPow-\nerEdge1950serverwith16Gmemoryand2.5GhzQuad\nCoreCPU.\n5.1.Caltech-101\nTheCaltech-101dataset[7]contains9144imagesin101\nclassesincludinganimals,vehicles,owers,etc,withsig-\nnicantvarianceinshape.Thenumberofimagespercat-\negoryvariesfrom31to800.Assuggestedbytheoriginal\ndataset[7]andalsobymanyotherresearchers[25,11],we\n\npartitionedthewholedatasetinto5,10,...,30trainingim-\nagesperclassandnomorethan50testingimagesperclass,\nandmeasuredtheperformanceusingaverageaccuracyover\n\n102classes(\ni.e.101classesandabackgroundclass).We\ntrainedacodebookwith2048bases,andused\n44,22and11sub-regionsforSPM.Intheexperiment,theim-\nageswereresizedtobenolargerthan\n300300pixelswith\npreservedaspectratio.\nInourevaluation,totally13classesachieve\n100%clas-sicationaccuracywith30trainingimageperclass.Fig-\nure5.1illustratesveoutofthese13classesthathavemore\nthan10testingimages.Wecomparedourresultwithsev-\neralexistingapproaches.DetailedresultsareshowninTa-\n\nble1,anditcanbeseenthatinmostcases,ourproposed\nLLCmethodleadstheperformance.Inaddition,theaver-\nageprocessingtimeforourmethodingeneratingthenal\n\nrepresentationfromarawimageinputisonly0.24second.\nTable1.ImageclassicationresultsonCaltech-101dataset\ntrainingimages\n51015202530\nZhang[25]\n46.655.859.162.0-66.20\nLazebnik[15]\n--56.40--64.60\nGrifn[11]\n44.254.559.063.365.867.60\nBoiman[2]\n--65.00--70.40\nJain[12]\n--61.00--69.10\nGemert[8]\n-----64.16\nYang[22]\n--67.00--73.20\nOurs51.1559.77\n65.4367.7470.1673.44\naccordion,acc:100%\ncar,acc:100%\nleopards,acc:100%\nrooster,acc:100%\nchair,acc:100%\n5.2.Caltech-256\nTheCaltech-256datasetholds30,607imagesin256cat-\negories.Itpresentsmuchhighervariabilityinobjectsize,\nlocation,pose,etcthaninCaltech-101.Eachclasscontains\n\natleast80images.SimilartoSubsection5.1,weresized\ntheimagestobelessthan\n300300pixelswithaspectratio\nkept.Wefollowedthecommonsetupduringexperiment,\n\ni.e.,wetriedouralgorithmon15,30,45,and60training\nimagesperclassrespectively.Wetrainedacodebookswith\n4096bases,andused\n44,22and11sub-regionsfor\nSPM.AscanbeseenfromTable2,theresultsareveryim-\npressive:underallthecases,ourLLCmethodoutperforms\nthebestoftheexistingtechniquesbymorethan6percent.\n\nBesides,theaveragetimeinprocessingeachimageis0.3\nsecond.Figure5.2lists20classesthatareeasiesttobe\nclassiedusing60trainingimagesperclass.\nTable2.ImageclassicationresultsusingCaltech-256dataset\ntrainingimages\n15304560\nGrifn[11]\n28.3034.10--\nGemert[8]\n-27.17--\nYang[22]\n27.7334.0237.4640.14\nOurs34.3641.1945.3147.68\n5.3.PascalVOC2007\nThePASCAL2007dataset[6]consistsof9,963images\nfrom20classes.Theseimagesrangebetweenindoorand\noutdoorscenes,close-upsandlandscapes,andstrangeview-\npoints.Thedatasetisanextremelychallengingonebecause\nalltheimagesaredailyphotosobtainedfromFlickerwhere\n\nthesize,viewingangle,illumination,etcappearancesofob-\njectsandtheirposesvarysignicantly,withfrequentocclu-\nsions(Figure5.3).\nTheclassicationperformanceisevaluatedusingtheAv-\neragePrecision(AP)measure,astandardmetricusedby\nPASCALchallenge.ItcomputestheareaunderthePre-\n\ncision/Recallcurve,andthehigherthescore,thebetterthe\nperformance.InTable3,welistourscoresforall20classes\n'b'bonsai,acc:89%chandelier,acc:87%globe,acc:100%eiffel-tower,acc:87%reworks,acc:88%\nfrench-horn,acc:91%golden-gate,acc:95%guitar-pick,acc:91%hawksbill,acc:91%ketch,acc:96%\nleopards,acc:100%mars,acc:88%motorbikes,acc:94%lawn-mower,acc:100%sunower,acc:95%\ntower-pisa,acc:100%trilobite,acc:100%zebra,acc:94%airplanes,acc:100%faces,acc:99%\nFigure3.ExampleimagesfromclasseswithhighestclassicationaccuracyfromtheCaltech-256dataset\nincomparisonwiththebestperformanceofthe2007chal-\nlenge[6],aswellasanotherrecentresultsin[20].To\nmakefaircomparison,twonoteworthycommentsneedto\nbemade:\n1.In[6],multipledescriptorswereusedwithnonlinear\nclassierinthewinnersystem.\n2.In[20],multipledecisionsystemswerecombinedto\ngetthenalresults.\n3.ForourLLCalgorithm,weonlyusedsingledescriptor\n(HoG)andsimplelinearSVMastheclassier.\nEventhough,asseenfromTable3,ourLLCmethodcan\nstillachievesimilarperformanceasthebestsystemin[6]\nand[20].Infact,ifonlydenseSIFT(similartodenseHOG\ninourframework)wasused,thebestresultonthevalidation\n\nsetonlyscored51.8%in[6],whileourmethodachieved\n55.1%,winningaremarkablemarginof3.3%.\n5.4.Discussion\nToprovidemorecomprehensiveanalysisoftheproposed\nLLCmethod,wefurtherevaluateditsperformancewithre-\n\nspecttocodebooktraining,numberofneighborsforap-\nproximation,variousconstraintsasmentionedinSubsec-\ntion2.4,anddifferenttypesofpoolingandnormalizing\nduringtheSPMstep.Thepapermainlyreportsthere-\nsultsusingtheCaltech-101dataset,andourexperiments\n\nhaveshownthattheconclusionscanbegeneralizedtoother\ndatasetaswell.\nFirst,wecomparedtheclassicationaccuraciesusing\ncodebookstrainedbyK-Meansalgorithmandbyourpro-\n\nposedAlg.4.1.Inapplyingouralgorithm,thetworelated\nparametersandinEq.(4)andEq.(8)werecarefullyse-\nlectedsuchthatthecardinality,\ni.e.thelengthof\nidinline8\nofAlg.4.1,couldmatchthenumberofneighborsuseddur-\ningclassication.Finally\n=500\nand=100\nwereused,\nandthecardinalityrangesbetween\n434duringtraining.\nWegeneratedtwocodebookswith\n1024and2048entriesforeachmethodrespectively.Thenthesameexperimen-\ntalsetupasthatreportedinSubsection5.1wereused,and\n\ntheresultsareplottedinFigure5.Asshown,underalldif-\nferentnumbersoftrainingsamplesperclass,thelearned\ncodebookbyAlg.4.1improvedtheclassicationaccuracy\n\nby0.31.4percentoverthecodebookbyK-Means.\n5101520253045505560657075number of training images per classaverage classification accuracy  kmean (1024)learned (1024)kmean (2048)learned (2048)Figure5.Performanceofcodebook(Caltech-101)\nSecond,westudiedtheeffectofdifferentnumberof\nneighborsKusedforapproximatedLLC.Figure6liststhe\nperformanceusing2,5,10,20and40neighborsrespec-\ntively.Ascanbeseen,generallysmallnumberofneigh-\n\nborsleadstobetterclassicationaccuracy.Thisisdesir-\nable,becausethesmallerthenumberofneighborsused,the\nfasterthecomputationgoes,andthelessthememorycon-\nsumed.However,whenKgoessmallerthan5,theperfor-\nmancestartstodrop.\nThird,wetestedtheperformanceunderdifferentcon-\nstraintsotherthantheshift-invariantconstraintinEq.(3)\nontheLLCcode.Otherconstraintstestedwere\nUnconstrained.Non-negativeconstraint.\nNon-negativeshift-invariantconstraint.\nEq.(7).Thenon-negativeconstraintissolvedusingtheal-\ngorithmin[9].Duringourexperiments,itisnoticedthat,\n'b'aerobicycle*birdboatbottle\nbus*carcat*chair*cow*\ndinningtabledoghorsembike*person\nplantsheepsofatraintv\nFigure4.ExampleimagesfromPascalVOC2007dataset.\nA*intheclassnamemeansthatourmethodoutperformsothers\nTable3.ImageclassicationresultsusingPascalVOC2007dataset\nobjectclass\naerobicyc\nbirdboatbottlebus\ncarcatchaircow\nObj.+Contex[20]\n80.261.049.869.621.066.880.751.151.435.9BestPASCAL07[6]\n77.563.656.171.933.160.678.058.853.542.6Ours74.865.250.770.928.768.878.561.754.348.6objectclass\ntabledoghorsembike\npersonplantsheepsofa\ntraintvObj.+Contex[20]\n62.038.669.061.484.628.753.561.981.759.5BestofPASCAL07[6]\n54.945.877.564.085.936.344.750.979.253.2Ours51.844.176.666.983.530.844.653.478.253.5average\n58.459.459.35101520253045505560657075number of training images per classaverage classification accuracy  Figure6.Performanceunderdifferentneighbors(Caltech-101)\nsinceKissmall,solvingEq.(7)withdifferentconstraints\ndoesntrequiresignicantlydifferentcomputation.Asil-\n\nlustratedinFigure.7,theshift-invariantconstraintleadsto\nthebestperformance.Thenon-negativeconstraint,either\nusedaloneortogetherwiththeshift-invariantconstraint,\n\ndecreasestheperformancedramatically.\nFinally,weevaluatedthetypesofpoolingandnormal-\nizationusedintheSPMlayer.Ascanbeobservedfrom\nFigure.8,thebestperformanceisachievedbythemax\n\npoolingand\nl2normalizationcombination.Thegood\nperformanceofmaxpoolingisinconsistencewith[22].\nForthe\nl2normalization,itmakestheinnerproductof\nanyvectorwithitselftobeone,whichisdesirableforlinear\nkernels.\n510152025304045505560657075number of training images per classaverage classification accuracy  unconstrainedbothFigure7.Performanceunderdifferentconstraint(Caltech-101)\n51015202530203040506070number of training images per classaverage classification accuracy  sum/summax/sumFigure8.PerformancewithdifferentSPMsetting(Caltech-101)\n6.ConclusionandFutureWork\nThispaperpresentsapromisingimagerepresentation\nmethodcalledLocality-constrainedLinearCoding(LLC).\nLLCiseasytocomputeandgivessuperiorimageclassi-\n'b'cationperformancethanmanyexistingapproaches.LLC\napplieslocalityconstrainttoselectsimilarbasisoflocalim-\n\nagedescriptorsfromacodebook,andlearnsalinearcom-\nbinationweightofthesebasistoreconstructeachdescrip-\ntor.Thepaperalsointroducesanapproximationmethod\n\ntofurtherspeed-uptheLLCcomputation,andanoptimiza-\ntionmethodtoincrementallylearntheLLCcodebookus-\ninglarge-scaletrainingdescriptors.Experimentalresults\n\nbasedonseveralwell-knowndatasetvalidatethegoodper-\nformanceofLLC.\nThefutureworkincludesthefollowingissues:First,\nadditionalcodebooktrainingmethods,suchassuper-\nvisedtraining,willbeinvestigated;Second,besidesexact-\n\nnearest-neighborsearchappliedinthepaper,approximated-\nnearest-neighborsearchalgorithmswillbeevaluatedtofur-\ntherimprovethecomputationalefciencyofapproximated\n\nLLC;Andthird,integrationoftheLLCtechniqueintoprac-\nticalimage/videosearch/retrieval/indexing/management\napplicationsison-going.\nReferences\n[1]A.Berg,T.Berg,andJ.Malik.Shapematchingandobject\nrecognitionusinglowdistortioncorrespondences.\nProc.of\nCVPR05,pages2633.\n[2]O.Boiman,E.Shechtman,andM.Irani.Indefense\nofnearest-neighborbasedimageclassication.\nProc.of\nCVPR08,2008.\n[3]A.Bosch,A.Zisserman,andX.Munoz.Sceneclassica-\ntionusingahybridgenerative/dicriminativeapproach.\nIEEETrans.onPatternAnalysisandMachineIntelligence(PAMI)\n,2008.[4]G.Csurka,C.Dance,L.Fan,J.Willamowski,andC.Bray.\nVisualcategorizationwithbagsofkeypoints.\nWorkshopon\nStatisticalLearninginComputerVision,ECCV\n,pages122,\n2004.[5]N.DalalandB.Triggs.Histogramsoforientedgradientsfor\nhumandetection.\nProc.ofCVPR05\n,pages886893,2005.\n[6]M.Everingham,L.Gool,C.Williams,J.Winn,andA.Zis-\nserman.ThePASCALVisualObjectClassesChallenge2007\n(VOC2007)Results.\n[7]L.Fei-Fei,R.Fergus,andP.Perona.Learninggenerative\nvisualmodelsfromfewtrainingexamples:anincremental\nbayesianapproachtestedon101objectcategories.\nInIEEE\nCVPRWorkshoponGenerative-ModelBasedVision\n,2004.\n[8]J.Gemert,J.Geusebroek,C.Veenman,andA.Smeul-\nders.Kernelcodebooksforscenecategorization.\nProc.of\nECCV08,pages696709,2008.\n[9]D.GoldfarbandA.Idnani.Anumericallystabledualmethod\nforsolvingstrictlyconvexquadraticprograms.\nMathemati-calProgramming\n,pages133,1983.\n[10]K.GraumanandT.Darrell.Pyramidmatchkernels:Dis-\ncriminativeclassicationwithsetsofimagefeatures.\nProc.\nofICCV05\n,2005.\n[11]G.Grifn,A.Holub,andP.Perona.Caltech-256objectcat-\negorydataset.(7694),2007.\n[12]P.Jain,B.Kullis,andK.Grauman.Fastimagesearchfor\nlearnedmetrics.\nProc.ofCVPR08\n,2008.\n[13]F.JurieandB.Triggs.Creatingefcientcodebooksforvi-\nsualrecognition.\nProc.ofICCV05\n,2005.\n[14]S.Lazebnik,C.Schmid,andJ.Ponce.Amaximumen-\ntropyframeworkforpart-basedtextureandobjectrecogni-\ntion.Proc.ofICCV05\n,2005.\n[15]S.Lazebnik,C.Schmid,andJ.Ponce.Beyondbagsof\nfeatures:Spatialpyramidmatchingforrecognizingnatural\n\nscenecategories.\nProc.ofCVPR06\n,2006.\n[16]H.Lee,A.Battle,R.Raina,andA.Ng.Efcientsparsecod-\ningalgorithms.\nAdvancesinNeuralInformationProcessing\nSystems,MITPress\n,pages801808,2007.\n[17]F.Moosmann,B.Triggs,andF.Jurie.Randomizedcluster-\ningforestsforbuildingfastanddiscriminativevisualvocab-\nularies.Proc.ofNIPS07\n,2007.\n[18]S.RoweisandL.Saul.Nonlineardimensionalityreduction\nbylocallylinearembedding.\nScience,pages23232326,\n2000.[19]J.SivicandA.Zisserman.Videogoogle:Atextretrieval\napproachtoobjectmatchinginvideos.\nProc.ofICCV03\n,pages14701477,2003.\n[20]J.Uijlings,A.Smeulders,andR.Scha.Whatisthespatial\nextentofanobject?\nProc.ofCVPR09\n,2009.\n[21]J.Wang,S.Zhu,andY.Gong.Resolutionen-\nhancementbasedonlearningthesparseassociation\nofimagepatches.\nPatternRecognitionLett.(2009)\ndoi:10.1016/j.patrec.2009.09.004\n.[22]J.Yang,K.Yu,Y.Gong,andT.Huang.Linearspatialpyra-\nmidmatchingusingsparsecodingforimageclassication.\nProc.ofCVPR09\n,2009.\n[23]L.Yang,R.Jin,R.Sukthankar,andF.Jurie.Unifyingdis-\ncriminativevisualcodebookgenerationwithclassiertrain-\ningforobjectcategoryrecognition.\nProc.ofCVPR08\n,2008.\n[24]K.Yu,T.Zhang,andY.Gong.Nonlinearlearningusinglocal\ncoordinatecoding.\nProc.ofNIPS09\n,2009.\n[25]H.Zhang,A.Berg,M.Maire,andJ.Malik.Svm-knn:Dis-\ncriminativenearestheighborclassicationforvisualcate-\n\ngoryrecognition.\nProc.ofCVPR06\n,2006.\n[26]X.Zhou,N.Cui,Z.Li,F.Liang,andT.Huang.Hierarchical\ngaussianizationforimageclassication.\nProc.ofICCV09\n,2009.'