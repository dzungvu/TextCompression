b"arXiv:1409.2329v5  [cs.NE]  19 Feb 2015UnderreviewasaconferencepaperatICLR2015\nR\nECURRENT\nN\nEURAL\nN\nETWORK\nR\nEGULARIZATION\nWojciechZaremba\nNewYorkUniversity\n\nwoj.zaremba@gmail.com\n\nIlyaSutskever,OriolVinyals\n\nGoogleBrain\n\nf\nilyasu,vinyals\ng\n@google.com\nA\nBSTRACT\nWepresentasimpleregularizationtechniqueforRecurrent\nNeuralNetworks\n(RNNs)withLongShort-TermMemory(LSTM)units.Dropout,t\nhemostsuc-\ncessfultechniqueforregularizingneuralnetworks,doesn\notworkwellwithRNNs\nandLSTMs.Inthispaper,weshowhowtocorrectlyapplydropo\nuttoLSTMs,\nandshowthatitsubstantiallyreducesoverttingonavarie\ntyoftasks.Thesetasks\nincludelanguagemodeling,speechrecognition,imagecapt\niongeneration,and\nmachinetranslation.\n\n1I\nNTRODUCTION\nTheRecurrentNeuralNetwork(RNN)isneuralsequencemodel\nthatachievesstateoftheartper-\nformanceonimportanttasksthatincludelanguagemodeling\nMikolov(2012),speechrecognition\nGravesetal.(2013),andmachinetranslationKalchbrenner\n&Blunsom(2013).Itisknownthat\nsuccessfulapplicationsofneuralnetworksrequiregoodre\ngularization.Unfortunately,dropout\nSrivastava(2013),themostpowerfulregularizationmetho\ndforfeedforwardneuralnetworks,does\nnotworkwellwithRNNs.Asaresult,practicalapplications\nofRNNsoftenusemodelsthatare\ntoosmallbecauselargeRNNstendtoovert.Existingregula\nrizationmethodsgiverelativelysmall\nimprovementsforRNNsGraves(2013).Inthiswork,weshowth\natdropout,whencorrectlyused,\ngreatlyreducesoverttinginLSTMs,andevaluateitonthre\nedifferentproblems.\nThecodeforthisworkcanbefoundin\nhttps://github.com/wojzaremba/lstm\n.\n2R\nELATEDWORK\nDropoutSrivastava(2013)isarecentlyintroducedregular\nizationmethodthathasbeenverysuc-\ncessfulwithfeed-forwardneuralnetworks.Whilemuchwork\nhasextendeddropoutinvariousways\nWang&Manning(2013);Wanetal.(2013),therehasbeenrelat\nivelylittleresearchinapplyingit\ntoRNNs.TheonlypaperonthistopicisbyBayeretal.(2013),\nwhofocusesonmarginalized\ndropoutWang&Manning(2013),anoiselessdeterministica\npproximationtostandarddropout.\nBayeretal.(2013)claimthatconventionaldropoutdoesnot\nworkwellwithRNNsbecausethere-\ncurrenceampliesnoise,whichinturnhurtslearning.Inth\niswork,weshowthatthisproblemcan\nbexedbyapplyingdropouttoacertainsubsetoftheRNNs'co\nnnections.Asaresult,RNNscan\nnowalsobenetfromdropout.\n\nIndependentlyofourwork,Phametal.(2013)developedthev\nerysameRNNregularizationmethod\nandappliedittohandwritingrecognition.Werediscovered\nthismethodanddemonstratedstrong\nempiricalresultsoverawiderangeofproblems.Otherworkt\nhatapplieddropouttoLSTMsis\nPachitariu&Sahani(2013).\n\nWorkdonewhiletheauthorwasinGoogleBrain.\n1\n"b'UnderreviewasaconferencepaperatICLR2015\nTherehavebeenanumberofarchitecturalvariantsoftheRNN\nthatperformbetteronproblemswith\nlongtermdependenciesHochreiter&Schmidhuber(1997);Gr\navesetal.(2009);Choetal.(2014);\nJaegeretal.(2007);Koutnketal.(2014);Sundermeyeret\nal.(2012).Inthiswork,weshowhow\ntocorrectlyapplydropouttoLSTMs,themostcommonly-used\nRNNvariant;thiswayofapplying\ndropoutislikelytoworkwellwithotherRNNarchitecturesa\nswell.\nInthispaper,weconsiderthefollowingtasks:languagemod\neling,speechrecognition,andma-\nchinetranslation.Languagemodelingisthersttaskwhere\nRNNshaveachievedsubstantialsuc-\ncessMikolovetal.(2010;2011);Pascanuetal.(2013).RNNs\nhavealsobeensuccessfullyused\nforspeechrecognitionRobinsonetal.(1996);Gravesetal.\n(2013)andhaverecentlybeenapplied\ntomachinetranslation,wheretheyareusedforlanguagemod\neling,re-ranking,orphrasemodel-\ningDevlinetal.(2014);Kalchbrenner&Blunsom(2013);Cho\netal.(2014);Chowetal.(1987);\nMikolovetal.(2013).\n3R\nEGULARIZING\nRNN\nSWITH\nLSTM\nCELLS\nInthissectionwedescribethedeepLSTM(Section3.1).Next\n,weshowhowtoregularizethem\n(Section3.2),andexplainwhyourregularizationschemewo\nrks.\nWeletsubscriptsdenotetimestepsandsuperscriptsdenote\nlayers.Allourstatesare\nn\n-dimensional.\nLet\nh\nl\n\nt\n2\nR\nn\nbeahiddenstateinlayer\nl\nintimestep\nt\n.Moreover,let\nT\nn;m\n:R\nn\n!\nR\nm\nbeanafne\ntransform(\nWx\n+\nb\nforsome\nW\nand\nb\n).Let\n\nbeelement-wisemultiplicationandlet\nh\n0\n\nt\nbeaninput\nwordvectorattimestep\nk\n.Weusetheactivations\nh\nL\n\nt\ntopredict\ny\nt\n,since\nL\nisthenumberoflayers\ninourdeepLSTM.\n3.1L\nONG\n-\nSHORTTERMMEMORYUNITS\nTheRNNdynamicscanbedescribedusingdeterministictrans\nitionsfromprevioustocurrenthidden\nstates.Thedeterministicstatetransitionisafunction\nRNN\n:h\nl\n\n1\nt\n;h\nl\n\nt\n\n1\n!\nh\nl\n\nt\nForclassicalRNNs,thisfunctionisgivenby\nh\nl\n\nt\n=\nf\n(\nT\nn;n\nh\nl\n\n1\nt\n+\nT\nn;n\nh\nl\n\nt\n\n1\n)\n,where\nf\n2f\nsigm\n;tanh\ng\nTheLSTMhascomplicateddynamicsthatallowittoeasilyme\nmorizeinformationforanextended\nnumberoftimesteps.Thelongtermmemoryisstoredinavec\ntorof\nmemorycells\nc\nl\n\nt\n2\nR\nn\n.Al-\nthoughmanyLSTMarchitecturesthatdifferintheirconnect\nivitystructureandactivationfunctions,\nallLSTMarchitectureshaveexplicitmemorycellsforstori\nnginformationforlongperiodsoftime.\nTheLSTMcandecidetooverwritethememorycell,retrieveit\n,orkeepitforthenexttimestep.The\nLSTMarchitectureusedinourexperimentsisgivenbythefol\nlowingequationsGravesetal.(2013):\nLSTM\n:h\nl\n\n1\nt\n;h\nl\n\nt\n\n1\n;c\nl\n\nt\n\n1\n!\nh\nl\n\nt\n;c\nl\n\nt\n0\n\nB\n\n@\ni\nf\no\ng\n1\n\nC\n\nA\n=\n0\n\nB\n\n@\nsigm\n\nsigm\n\nsigm\n\ntanh\n1\n\nC\n\nA\nT\n2\nn;\n4\nn\n\nh\nl\n\n1\nt\nh\nl\n\nt\n\n1\n\nc\nl\n\nt\n=\nf\n\nc\nl\n\nt\n\n1\n+\ni\n\ng\nh\nl\n\nt\n=\no\n\ntanh(\nc\nl\n\nt\n)\nIntheseequations,\nsigm\nand\ntanh\nareappliedelement-wise.Figure1illustratestheLSTMequ\na-\ntions.\n3.2R\nEGULARIZATIONWITH\nD\nROPOUT\nThemaincontributionofthispaperisarecipeforapplyingd\nropouttoLSTMsinawaythatsuccess-\nfullyreducesovertting.Themainideaistoapplythedropo\nutoperatoronlytothenon-recurrent\n2\n'b'UnderreviewasaconferencepaperatICLR2015\n\n\nc\nt\nCell\nf\n\n\n\nf\nForgetgate\n6\n\n\n\n\nh\nl\n\nt\n\n1\nA\nAK\nh\nl\n\n1\nt\n\n\ni\nInput\ngate\nAU\nh\nl\n\nt\n\n1\n\n\nh\nl\n\n1\nt\n\n\no\nOutput\ngate\nAU\nh\nl\n\nt\n\n1\n\n\nh\nl\n\n1\nt\n\n\ng\nInput\nmodulation\ngate\nf\n\n-\n-\nJ\nJ\nJ^\nf\n\n-\n-\n?\nh\nl\n\nt\nh\nl\n\nt\n\n1\nh\nl\n\n1\nt\n\n:\nX\nXz\nFigure1:AgraphicalrepresentationofLSTMmemorycellsus\nedinthispaper(thereareminor\ndifferencesincomparisontoGraves(2013)).\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\nx\nt\n\n2\nx\nt\n\n1\nx\nt\nx\nt\n+1\nx\nt\n+2\ny\nt\n\n2\ny\nt\n\n1\ny\nt\ny\nt\n+1\ny\nt\n+2\nFigure2:RegularizedmultilayerRNN.Thedashedarrowsind\nicateconnectionswheredropoutis\napplied,andthesolidlinesindicateconnectionswheredro\npoutisnotapplied.\nconnections(Figure2).Thefollowingequationdescribesi\ntmoreprecisely,where\nD\nisthedropout\noperatorthatsetsarandomsubsetofitsargumenttozero:\n0\n\nB\n\n@\ni\nf\no\ng\n1\n\nC\n\nA\n=\n0\n\nB\n\n@\nsigm\n\nsigm\n\nsigm\n\ntanh\n1\n\nC\n\nA\nT\n2\nn;\n4\nn\n\nD\n(\nh\nl\n\n1\nt\n)\nh\nl\n\nt\n\n1\n\nc\nl\n\nt\n=\nf\n\nc\nl\n\nt\n\n1\n+\ni\n\ng\nh\nl\n\nt\n=\no\n\ntanh(\nc\nl\n\nt\n)\nOurmethodworksasfollows.Thedropoutoperatorcorruptst\nheinformationcarriedbytheunits,\nforcingthemtoperformtheirintermediatecomputationsmo\nrerobustly.Atthesametime,wedonot\nwanttoerasealltheinformationfromtheunits.Itisespeci\nallyimportantthattheunitsremember\neventsthatoccurredmanytimestepsinthepast.Figure3sho\nwshowinformationcouldowfrom\naneventthatoccurredattimestep\nt\n\n2\ntothepredictionintimestep\nt\n+2\ninourimplementationof\ndropout.Wecanseethattheinformationiscorruptedbythed\nropoutoperatorexactly\nL\n+1\ntimes,\n3\n'b"UnderreviewasaconferencepaperatICLR2015\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\nx\nt\n\n2\nx\nt\n\n1\nx\nt\nx\nt\n+1\nx\nt\n+2\ny\nt\n\n2\ny\nt\n\n1\ny\nt\ny\nt\n+1\ny\nt\n+2\nFigure3:Thethicklineshowsatypicalpathofinformation\nowintheLSTM.Theinformationis\naffectedbydropout\nL\n+1\ntimes,where\nL\nisdepthofnetwork.\nthemeaningoflifeis\nthatonlyifanendwouldbeofthewholesupplier.widespread\nrulesarere-\ngardedasthecompaniesofrefusestodeliver.inbalanceoft\nhenation'sinformationandloan\ngrowthassociatedwiththecarrierthriftsareintheproces\nsofslowingtheseedandcommercialpaper.\nthemeaningoflifeis\nnearlyintherstseveralmonthsbeforethegovernmentwasa\nddressingsuchamoveas\npresidentandchiefexecutiveofthenationpastfromanatio\nnalcommitmenttocurbgrounds.meanwhilethe\ngovernmentinvestsovercapacitythatcriticismandintheo\nuterreversalofsmall-townamerica.\nFigure4:Someinterestingsamplesdrawnfromalargeregula\nrizedmodelconditionedonThe\nmeaningoflifeis.Wehaveremovedunk,N,$fromthes\netofpermissiblewords.\nandthisnumberisindependentofthenumberoftimestepstra\nversedbytheinformation.Standard\ndropoutperturbstherecurrentconnections,whichmakesit\ndifcultfortheLSTMtolearntostore\ninformationforlongperiodsoftime.Bynotusingdropouton\ntherecurrentconnections,theLSTM\ncanbenetfromdropoutregularizationwithoutsacricing\nitsvaluablememorizationability.\n4E\nXPERIMENTS\nWepresentresultsinthreedomains:languagemodeling(Sec\ntion4.1),speechrecognition(Section\n4.2),machinetranslation(Section4.3),andimagecaption\ngeneration(Section4.4).\n4.1L\nANGUAGEMODELING\nWeconductedword-levelpredictionexperimentsonthePenn\nTreeBank(PTB)datasetMarcusetal.\n(1993),whichconsistsof\n929\nktrainingwords,\n73\nkvalidationwords,and\n82\nktestwords.Ithas\n10\nk\nwordsinitsvocabulary.WedownloadeditfromTomasMikolov\n'swebpage\ny\n.Wetrainedregularized\nLSTMsoftwosizes;thesearedenotedthemediumLSTMandlarg\neLSTM.BothLSTMshave\ntwolayersandareunrolledfor\n35\nsteps.Weinitializethehiddenstatestozero.Wethenuseth\ne\nnalhiddenstatesofthecurrentminibatchastheinitialhi\nddenstateofthesubsequentminibatch\n(successiveminibatchessequentiallytraversethetraini\nngset).Thesizeofeachminibatchis20.\ny\nhttp://www.fit.vutbr.cz/\n\nimikolov/rnnlm/simple-examples.tgz\n4\n"b'UnderreviewasaconferencepaperatICLR2015\nModelValidationsetTestset\nAsinglemodel\nPascanuetal.(2013)107.5\n\nChengetal.100.0\n\nnon-regularizedLSTM120.7114.5\n\nMediumregularizedLSTM86.282.7\n\nLargeregularizedLSTM82.2\n78.4\nModelaveraging\nMikolov(2012)83.5\n\nChengetal.80.6\n\n2non-regularizedLSTMs100.496.1\n\n5non-regularizedLSTMs87.984.1\n\n10non-regularizedLSTMs83.580.0\n\n2mediumregularizedLSTMs80.677.0\n\n5mediumregularizedLSTMs76.773.3\n\n10mediumregularizedLSTMs75.272.0\n\n2largeregularizedLSTMs76.973.6\n\n10largeregularizedLSTMs72.869.5\n\n38largeregularizedLSTMs71.9\n68.7\nModelaveragingwithdynamicRNNsandn-grammodels\nMikolov&Zweig(2012)72.9\nTable1:Word-levelperplexityonthePennTreeBankdataset\n.\nThemediumLSTMhas\n650\nunitsperlayeranditsparametersareinitializeduniforml\nyin\n[\n0\n:05\n;0\n:05]\n.Asdescribedearlier,weapply\n50%\ndropoutonthenon-recurrentconnections.We\ntraintheLSTMfor\n39\nepochswithalearningrateof\n1\n,andafter\n6\nepochswedecreaseitbyafactor\nof\n1\n:2\naftereachepoch.Weclipthenormofthegradients(normaliz\nedbyminibatchsize)at\n5\n.\nTrainingthisnetworktakesabouthalfadayonanNVIDIAK20G\nPU.\nThelargeLSTMhas\n1500\nunitsperlayeranditsparametersareinitializeduniforml\nyin\n[\n0\n:04\n;0\n:04]\n.Weapply\n65%\ndropoutonthenon-recurrentconnections.Wetrainthemode\nlfor\n55\nepochswithalearningrateof\n1\n;after\n14\nepochswestarttoreducethelearningratebyafactor\nof\n1\n:15\naftereachepoch.Weclipthenormofthegradients(normaliz\nedbyminibatchsize)at\n10\nMikolovetal.(2010).Trainingthisnetworktakesanentire\ndayonanNVIDIAK20GPU.\nForcomparison,wetrainedanon-regularizednetwork.Weop\ntimizeditsparameterstogetthebest\nvalidationperformance.Thelackofregularizationeffect\nivelyconstrainssizeofthenetwork,forc-\ningustousesmallnetworkbecauselargernetworksovert.O\nurbestperformingnon-regularized\nLSTMhastwohiddenlayerswith\n200\nunitsperlayer,anditsweightsareinitializeduniformlyi\nn\n[\n0\n:1\n;0\n:1]\n.Wetrainitfor\n4\nepochswithalearningrateof\n1\nandthenwedecreasethelearningrate\nbyafactorof\n2\naftereachepoch,foratotalof\n13\ntrainingepochs.Thesizeofeachminibatchis\n20\n,\nandweunrollthenetworkfor\n20\nsteps.Trainingthisnetworktakes2-3hoursonanNVIDIAK20\nGPU.\n\nTable1comparespreviousresultswithourLSTMs,andFigure\n4showssamplesdrawnfromasingle\nlargeregularizedLSTM.\n4.2S\nPEECHRECOGNITION\nDeepNeuralNetworkshavebeenusedforacousticmodelingfo\nroverhalfacentury(see\nBourlard&Morgan(1993)foragoodreview).Acousticmodeli\nngisakeycomponentinmap-\npingacousticsignalstosequencesofwords,asitmodels\np\n(\ns\nt\njX\n)\nwhere\ns\nt\nisthephoneticstateat\ntime\nt\nand\nX\nistheacousticobservation.RecentworkhasshownthatLSTM\nscanachieveexcellent\nperformanceonacousticmodelingSaketal.(2014),yetrela\ntivelysmallLSTMs(intermsofthe\nnumberoftheirparameters)caneasilyovertthetrainings\net.Ausefulmetricformeasuringthe\nperformanceofacousticmodelsisframeaccuracy,whichism\neasuredateach\ns\nt\nforalltimesteps\nt\n.Generally,thismetriccorrelateswiththeactualmetrico\nfinterest,theWordErrorRate(WER).\n5\n'b"UnderreviewasaconferencepaperatICLR2015\nModelTrainingsetValidationset\nNon-regularizedLSTM71.668.9\n\nRegularizedLSTM69.4\n70.5\nTable2:Frame-levelaccuracyontheIcelandicSpeechDatas\net.Thetrainingsethas93kutterances.\nModelTestperplexityTestBLEUscore\nNon-regularizedLSTM5.825.9\n\nRegularizedLSTM5.029.03\nLIUMsystem33.30\nTable3:ResultsontheEnglishtoFrenchtranslationtask.\nSincecomputingtheWERinvolvesusingalanguagemodelandt\nuningthedecodingparametersfor\neverychangeintheacousticmodel,wedecidedtofocusonfra\nmeaccuracyintheseexperiments.\nTable2showsthatdropoutimprovestheframeaccuracyofthe\nLSTM.Notsurprisingly,thetraining\nframeaccuracydropsduetothenoiseaddedduringtraining,\nbutasisoftenthecasewithdropout,\nthisyieldsmodelsthatgeneralizebettertounseendata.No\ntethatthetestsetiseasierthanthetrain-\ningset,asitsaccuracyishigher.Wereporttheperformance\nofanLSTMonaninternalGoogle\nIcelandicSpeechdataset,whichisrelativelysmall(93kut\nterances),sooverttingisagreatconcern.\n4.3M\nACHINETRANSLATION\nWeformulateamachinetranslationproblemasalanguagemod\nellingtask,whereanLSTMistrained\ntoassignhighprobabilitytoacorrecttranslationofasour\ncesentence.Thus,theLSTMistrainedon\nconcatenationsofsourcesentencesandtheirtranslations\nSutskeveretal.(2014)(seealsoChoetal.\n(2014)).Wecomputeatranslationbyapproximatingthemost\nprobablesequenceofwordsusinga\nsimplebeamsearchwithabeamofsize12.WerananLSTMontheW\nMT'14EnglishtoFrench\ndataset,ontheselectedsubsetfromSchwenk(2014)which\nhas340MFrenchwordsand304M\nEnglishwords.OurLSTMhas4hiddenlayers,andbothitslaye\nrsandwordembeddingshave\n1000units.ItsEnglishvocabularyhas160,000wordsandits\nFrenchvocabularyhas80,000words.\nTheoptimaldropoutprobabilitywas0.2.Table3showsthepe\nrformanceofanLSTMtrained\nwithandwithoutdropout.WhileourLSTMdoesnotbeatthephr\nase-basedLIUMSMTsystem\nSchwenketal.(2011),ourresultsshowthatdropoutimprove\nsthetranslationperformanceofthe\nLSTM.\n4.4I\nMAGE\nC\nAPTION\nG\nENERATION\nWeappliedthedropoutvarianttotheimagecaptiongenerati\nonmodelofVinyalsetal.(2014).The\nimagecaptiongenerationissimilartothesequence-to-seq\nuencemodelofSutskeveretal.(2014),\nbutwheretheinputimageismappedontoavectorwithahighly\n-accuratepre-trainedconvolutional\nneuralnetwork(Szegedyetal.,2014),whichisconvertedin\ntoacaptionwithasingle-layerLSTM\n(seeVinyalsetal.(2014)forthedetailsonthearchitectur\ne).WetestourdropoutschemeonLSTM\nastheconvolutionalneuralnetworkisnottrainedontheima\ngecaptiondatasetbecauseitisnotlarge\n(MSCOCO(Linetal.,2014)).\n\nOurresultsaresummarizedinthefollowingTable4.Inbrief\n,dropouthelpsrelativetonotusing\ndropout,butusinganensembleeliminatesthegainsattaine\ndbydropout.Thus,inthissetting,the\nmaineffectofdropoutistoproduceasinglemodelthatisasg\noodasanensemble,whichisa\nreasonableimprovementgiventhesimplicityofthetechniq\nue.\n5C\nONCLUSION\nWepresentedasimplewayofapplyingdropouttoLSTMsthatre\nsultsinlargeperformancein-\ncreasesonseveralproblemsindifferentdomains.Ourworkm\nakesdropoutusefulforRNNs,and\nourresultssuggestthatourimplementationofdropoutcoul\ndimproveperformanceonawidevariety\nofapplications.\n6\n"b"UnderreviewasaconferencepaperatICLR2015\nModelTestperplexityTestBLEUscore\nNon-regularizedmodel8.4723.5\n\nRegularizedmodel7.9924.3\n10non-regularizedmodels7.524.4\nTable4:Resultsontheimagecaptiongenerationtask.\n6A\nCKNOWLEDGMENTS\nWewishtoacknowledgeTomasMikolovforusefulcommentsont\nherstversionofthepaper.\nR\nEFERENCES\nBayer,Justin,Osendorfer,Christian,Chen,Nutan,Urban,\nSebastian,andvanderSmagt,Patrick.Onfast\ndropoutanditsapplicabilitytorecurrentnetworks.\narXivpreprintarXiv:1311.0701\n,2013.\nBourlard,H.andMorgan,N.\nConnectionistSpeechRecognition:AHybridApproach\n.KluwerAcademic\nPublishers,1993.\nCheng,Wei-Chen,Kok,Stanley,Pham,HoaiVu,Chieu,HaiLeo\nng,andChai,KianMingA.Language\nmodelingwithsum-productnetworks.\nCho,Kyunghyun,vanMerrienboer,Bart,Gulcehre,Caglar,B\nougares,Fethi,Schwenk,Holger,andBengio,\nYoshua.Learningphraserepresentationsusingrnnencoder\n-decoderforstatisticalmachinetranslation.\narXiv\npreprintarXiv:1406.1078\n,2014.\nChow,Y,Dunham,M,Kimball,O,Krasner,M,Kubala,G,Makhou\nl,J,Price,P,Roucos,S,andSchwartz,R.\nByblos:Thebbncontinuousspeechrecognitionsystem.In\nAcoustics,Speech,andSignalProcessing,IEEE\nInternationalConferenceonICASSP'87.\n,volume12,pp.8992.IEEE,1987.\nDevlin,J.,Zbib,R.,Huang,Z.,Lamar,T.,Schwartz,R.,and\nMakhoul,J.Fastandrobustneuralnetworkjoint\nmodelsforstatisticalmachinetranslation.In\nACL\n,2014.\nGraves,Alex.Generatingsequenceswithrecurrentneuraln\networks.\narXivpreprintarXiv:1308.0850\n,2013.\nGraves,Alex,Liwicki,Marcus,Fernandez,Santiago,Bert\nolami,Roman,Bunke,Horst,andSchmidhuber,\nJurgen.Anovelconnectionistsystemforunconstrainedha\nndwritingrecognition.\nPatternAnalysisand\nMachineIntelligence,IEEETransactionson\n,31(5):855868,2009.\nGraves,Alex,Mohamed,Abdel-rahman,andHinton,Geoffrey\n.Speechrecognitionwithdeeprecurrentneural\nnetworks.In\nAcoustics,SpeechandSignalProcessing(ICASSP),2013IEE\nEInternationalConferenceon\n,\npp.66456649.IEEE,2013.\nHochreiter,SeppandSchmidhuber,Jurgen.Longshort-ter\nmmemory.\nNeuralcomputation\n,9(8):17351780,\n1997.\nJaeger,Herbert,Lukosevicius,Mantas,Popovici,Dan,a\nndSiewert,Udo.Optimizationandapplicationsof\nechostatenetworkswithleaky-integratorneurons.\nNeuralNetworks\n,20(3):335352,2007.\nKalchbrenner,N.andBlunsom,P.Recurrentcontinuoustran\nslationmodels.In\nEMNLP\n,2013.\nKoutnk,Jan,Greff,Klaus,Gomez,Faustino,andSchmidhu\nber,Jurgen.Aclockworkrnn.\narXivpreprint\narXiv:1402.3511\n,2014.\nLin,Tsung-Yi,Maire,Michael,Belongie,Serge,Hays,Jame\ns,Perona,Pietro,Ramanan,Deva,Dollar,Piotr,\nandZitnick,CLawrence.Microsoftcoco:Commonobjectsinc\nontext.\narXivpreprintarXiv:1405.0312\n,\n2014.\nMarcus,MitchellP,Marcinkiewicz,MaryAnn,andSantorini\n,Beatrice.Buildingalargeannotatedcorpusof\nenglish:Thepenntreebank.\nComputationallinguistics\n,19(2):313330,1993.\nMikolov,Tomas.\nStatisticallanguagemodelsbasedonneuralnetworks\n.PhDthesis,Ph.D.thesis,Brno\nUniversityofTechnology,2012.\nMikolov,TomasandZweig,Geoffrey.Contextdependentrecu\nrrentneuralnetworklanguagemodel.In\nSLT\n,\npp.234239,2012.\n7\n"b"UnderreviewasaconferencepaperatICLR2015\nMikolov,Tomas,Karaat,Martin,Burget,Lukas,Cernock\ny,Jan,andKhudanpur,Sanjeev.Recurrentneural\nnetworkbasedlanguagemodel.In\nINTERSPEECH\n,pp.10451048,2010.\nMikolov,Tomas,Deoras,Anoop,Povey,Daniel,Burget,Luka\ns,andCernocky,Jan.Strategiesfortraininglarge\nscaleneuralnetworklanguagemodels.In\nAutomaticSpeechRecognitionandUnderstanding(ASRU),20\n11\nIEEEWorkshopon\n,pp.196201.IEEE,2011.\nMikolov,Tomas,Le,QuocV,andSutskever,Ilya.Exploiting\nsimilaritiesamonglanguagesformachinetrans-\nlation.\narXivpreprintarXiv:1309.4168\n,2013.\nPachitariu,MariusandSahani,Maneesh.Regularizationan\ndnonlinearitiesforneurallanguagemodels:when\naretheyneeded?\narXivpreprintarXiv:1301.5650\n,2013.\nPascanu,Razvan,Gulcehre,Caglar,Cho,Kyunghyun,andBen\ngio,Yoshua.Howtoconstructdeeprecurrent\nneuralnetworks.\narXivpreprintarXiv:1312.6026\n,2013.\nPham,Vu,Kermorvant,Christopher,andLouradour,Jerom\ne.Dropoutimprovesrecurrentneuralnetworksfor\nhandwritingrecognition.\narXivpreprintarXiv:1312.4569\n,2013.\nRobinson,Tony,Hochberg,Mike,andRenals,Steve.Theuseo\nfrecurrentneuralnetworksincontinuousspeech\nrecognition.In\nAutomaticspeechandspeakerrecognition\n,pp.233258.Springer,1996.\nSak,H.,Vinyals,O.,Heigold,G.,Senior,A.,McDermott,E.\n,Monga,R.,andMao,M.Sequencediscriminative\ndistributedtrainingoflongshort-termmemoryrecurrentn\neuralnetworks.In\nInterspeech\n,2014.\nSchwenk,Holger.Universitylemans,2014.\nhttp://www-lium.univ-lemans.fr/\n\nschwenk/cslm_joint/paper\n.\nSchwenk,Holger,Lambert,Patrik,Barrault,Loc,Servan\n,Christophe,Ai,Haithem,Abdul-Rauf,Sadaf,and\nShah,Kashif.Lium'ssmtmachinetranslationsystemsforwm\nt2011.In\nProceedingsoftheSixthWorkshop\nonStatisticalMachineTranslation\n,pp.464469.AssociationforComputationalLinguistics,\n2011.\nSrivastava,Nitish.\nImprovingneuralnetworkswithdropout\n.PhDthesis,UniversityofToronto,2013.\nSundermeyer,Martin,Schluter,Ralf,andNey,Hermann.Ls\ntmneuralnetworksforlanguagemodeling.In\nINTERSPEECH\n,2012.\nSutskever,Ilya,Vinyals,Oriol,andLe,QuocVV.Sequencet\nosequencelearningwithneuralnetworks.In\nAdvancesinNeuralInformationProcessingSystems\n,pp.31043112,2014.\nSzegedy,Christian,Liu,Wei,Jia,Yangqing,Sermanet,Pie\nrre,Reed,Scott,Anguelov,Dragomir,Erhan,Du-\nmitru,Vanhoucke,Vincent,andRabinovich,Andrew.Goingd\neeperwithconvolutions.\narXivpreprint\narXiv:1409.4842\n,2014.\nVinyals,Oriol,Toshev,Alexander,Bengio,Samy,andErhan\n,Dumitru.Showandtell:Aneuralimagecaption\ngenerator.\narXivpreprintarXiv:1411.4555\n,2014.\nWan,Li,Zeiler,Matthew,Zhang,Sixin,Cun,YannL,andFerg\nus,Rob.Regularizationofneuralnetworks\nusingdropconnect.In\nProceedingsofthe30thInternationalConferenceonMachin\neLearning(ICML-13)\n,\npp.10581066,2013.\nWang,SidaandManning,Christopher.Fastdropouttraining\n.In\nProceedingsofthe30thInternationalConfer-\nenceonMachineLearning(ICML-13)\n,pp.118126,2013.\n8\n"