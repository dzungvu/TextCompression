b'ABSTRACT\nRecently Viola et al. have introduced a rapid object detection\nscheme based on a boosted cascade of simple feature classifiers. In\nthis paper we introduce and empirically analysis two extensions to\ntheir approach: Firstly, a novel set of rotated haar-like features is\n\nintroduced. These novel features significantly enrich the simple\nfeatures of [6] and can also be calculated efficiently. With these new\nrotated features our sample face detector shows off on average a\n\n10% lower false alarm rate at a given hit rate. Secondly, we present\na through analysis of different boosting algorithms (namely\nDiscrete, Real and Gentle Adaboost) and weak classifiers on the\n\ndetection performance and computational complexity. We will see\nthat Gentle Adaboost with small CART trees as base classifiers\noutperform Discrete Adaboost and stumps. The complete object\n\ndetection training and detection system as well as a trained face\ndetector are available in the Open Computer Vision Library at\nsourceforge.net [8].\n1  Introduction\nRecently Viola et al. have proposed a multi-stage classification\n\nprocedure that reduces the processing time substantially while\nachieving almost the same accuracy as compared to a much slower\nand more complex single stage classifier [6]. This paper extends\n\ntheir rapid object detection framework in two important ways:\nFirstly, their basic and over-complete set of haar-like features is\nextended by an efficient set of 45 rotated features, which add\nadditional domain-knowledge to the learning framework and which\nis otherwise hard to learn. These novel features can be computed\n\nrapidly at all scales in constant time. \nSecondly, we empirically show that Gentle Adaboost outperforms\n(with respect to detection accuracy) Discrete and Real Adaboost for\nobject detection tasks, while having a lower computational\ncomplexity, i.e., requiring a lower number of features for the same\n\nperformance. Also, the usage of small decision trees instead of\nstumps as weak classifiers further improves the detection\nperformance at a comparable detection speed.\nThe complete training and detection system as well as a trained face\ndetector are available in the Open Computer Vision Library at http:/\n\nsourceforge.net/projects/opencvlibrary/ [8].\n2  Features\nThe main purpose of using features instead of raw pixel values as the\n\ninput to a learning algorithm is to reduce/increase the in-class/out-\n\nof-class variability compared to the raw input data, and thus making\nclassification easier. Features usually encode knowledge about the\ndomain, which is difficult to learn from a raw and finite set of input\n\ndata. \nThe complexity of feature evaluation is also a very important aspect\nsince almost all object detection algorithms slide a fixed-size\nwindow at all scales over the input image. As we will see, our\nfeatures can be computed at any position and any scale in the same\n\nconstant time. At most 8 table lookups are needed per feature.\n2.1 Feature Pool\nOur feature pool was inspired by the over-complete haar-like\n\nfeatures used by Papageorgiou \net al\n. in [5,4] and their very fast\ncomputation scheme proposed by Viola \net al.\n in [6], and is a\ngeneralization of their work.\nLet us assume that the basic unit for testing for the presence of an\nobject is a window of  pixels. Also assume that we have a very\n\nfast way of computing the sum of pixels of any upright and 45\nrotated rectangle inside the window. A rectangle is specified by the\ntuple  with , , ,\n, , and its pixel sum is denoted by .\nTwo examples of such rectangles are given in Figure 1.\nOur raw feature set is then the set of all possible features of the form \n ,where the weights , the rectangles , and \nN are arbitrarily\nchosen.\nThis raw feature set is (almost) infinitely large. For practical reasons,\nit is reduced as follows:\n1.Only weighted combinations of pixel sums of two rectangles are\nconsidered (i.e., ).\n2.The weights have opposite signs, and are used to compensate for\nthe difference in area size between the two rectangles. Thus, for\nnon-overlapping rectangles we have\nWHrxywh\n\n=0xxwW\n+0yyhH\n+xy0wh0045RecSumr\nFigure 1:Example of an upright and 45 rotated\nrectangle.\nupright rectangle\n45 rotated rectangle\nWindow\nwhwhWHhwfeature\nIiRecSumr\niiI1N==iriN2=Empirical Analysis of Detection Cascades of Boosted Classifiers for Rapid Object \nDetection\nRainer Lienhart, Alexander Kuranov, Vadim Pisarevsky\nMicroprocessor Research Lab, Intel Labs\nIntel Corporation, Santa Clara, CA 95052, USA\nRainer.Lienhart@intel.com\nMRL Technical Report, May 2002, revised December 2002\n'b". Without restrictions we can set\n and get .\n3.The features mimic haar-like features and early features of the\nhuman visual pathway such as center-surround and directional\nresponses.\nThese restrictions lead us to the 14 feature prototypes shown in\nFigure 2:\nFour edge features,\nEight line features, and\nTwo center-surround features.\nThese prototypes are scaled independently in vertical and\nhorizontal direction in order to generate a rich, over-complete set of\n\nfeatures. Note that the line features can be calculated by two\nrectangles only. Hereto it is assumed that the first rectangle \nencompasses the black and white rectangle and the second rectangle\n represents the black area. For instance, line feature (2a) with\ntotal height of 2 and width of 6 at the top left corner (5,3) can be\nwritten as\n.Only features (1a), (1b), (2a), (2c) and (4a) of Figure 2 have been\nused by [4,5,6]. In our experiments the additional features\n\nsignificantly enhanced the expressional power of the learning\nsystem and consequently improved the performance of the object\ndetection system. This is especially true if the object under\n\ndetection exhibit diagonal structures such as it is the case for many\nbrand logos. \nNUMBER\n OF FEATURES\n. The number of features derived from each\nprototype is quite large and differs from prototype to prototype and\ncan be calculated as follows. Let  and  be the\n\nmaximum scaling factors in \nx and \ny direction. An upright feature of\nsize \nwxh then generates\nfeatures for an image of size \nWxH, while a  rotated feature\ngenerates\n with \nz=w+h.\nTable 1 lists the number of features for a window size of 24x24.\n2.2 Fast Feature Computation\nAll our features can be computed very fast in constant time for any\nsize by means of two auxiliary images. For upright rectangles the\n\nauxiliary image is the \nSummed Area Table . \n isdefined as the sum of the pixels of the upright rectangle ranging\nfrom the top left corner at (0,0) to the bottom right corner at (\nx,y\n)(see Figure 3a) [6]:\n.It can be calculated with one pass over all pixels from left to right\nand top to bottom by means of\nwith\nFrom this the pixel sum of any upright rectangle  can\nw0Arear\n0w1Arear\n1=w01=w1Arear\n0Arear\n1=r0r1feature\nI1RecSum\n536203RecSum\n73220+=Figure 2:Feature prototypes of simple haar-like and cen-\nter-surround features. Black areas have negative\nand white areas positive weights.\n1. Edge features\n3. Center-surround features\n2. Line features\n4. Special diagonal line feature used in [3,4,5]\n(a)\n(b)(c)\n(d)(a)(b)\n(c)\n(d)(e)\n(f)(g)\n(h)\n(a)\n(b)XWw\n=YHh\n=XYW\n1wX1+2----------\n+H1hY1+2---------\n-+45Feature \nType\nw/h\nX/Y\n#1a ; 1b2/1 ; 1/212/24 ; 24/1243,200\n1c ; 1d2/1 ; 1/28/88,464\n2a ; 2c3/1 ; 1/38/24 ; 24/827,600\n2b ; 2d4/1 ; 1/46/24 ; 24/620,736\n2e ; 2g3/1 ; 1/36/64,356\n2f ; 2h4/1 ; 1/44/43,600\n3a3/38/88,464\n3b3/33/31,521\nSum117,941\nTable 1: \nNumber of features inside of a 24x24 window for \neach prototype.\nXYW\n1zX1+2----------\n+H1zY1+2---------\n-+Figure 3:(a) Upright \nSummed Area Table\n (SAT\n) and\n(b) \nRotated Summed Area Table\n (RSAT\n);calculation scheme of the pixel sum of up-\nright (c) and rotated (d) rectangles.\nTSAT\n(x,y)\n(a)\n(b)\nSAT(x,y)\n(c)\n++---+-+SATxy\nSATxy\nSATxy\nIx'y'x'xy'y=SATxy\nSATxy\n1SATx\n1yIxy\nSATx\n1y1++=SAT1ySATx\n1SAT110===\nrxywh\n0\n="b'be determined by four table lookups (see also Figure 3(c):\nThis insight was first published in [6].\nFor 45 rotated rectangles the auxiliary image is the \nRotated\nSummed Area Table\n . It is defined as the sum of the\npixels of a  rotated rectangle with the bottom most corner at\n(x,y\n) and extending upwards till the boundaries of the image (see\nFigure 3b):\n.It can be calculated also in one pass from left to right and top to\nbottom over all pixels by\nwith\nas shown in Figure 4. From this the pixel sum of any rotated\nrectangle  can be determined by four table lookups\n\n(see Figure 5):\n.2.3 Fast Lighting Correction\nThe special properties of the haar-like features also enable fast\ncontrast stretching of the form\n, . can easily be determined by means of \nSAT\n(x,y). Computing ,\nhowever, involves the sum of squared pixels. It can easily be\n\nderived by calculating a second set of \nSAT\n and \nRSAT\n auxiliary\nimages for . Then, calculating  for any window requires\nonly 4 additional table lookups. In our experiments \nc was set to 2.\n3  (Stage) Classifier\nWe use boosting as our basic classifier. Boosting is a powerful\nlearning concept. It combines the performance of many "weak"\nclassifiers to produce a powerful \'committee\' [1]. A weak classifier\n\nis only required to be better than chance, and thus can be very\n\nsimple and computationally inexpensive. Many of them smartly\ncombined, however, result in a strong classifier, which often\noutperforms most \'monolithic\' strong classifiers such as SVMs and\n\nNeural Networks.\nDifferent variants of boosting are known such as Discrete Adaboost\n(see Figure 6), Real AdaBoost, and Gentle AdaBoost (see Figure\n7)[1]. All of them are identical with respect to computational\ncomplexity from a classification perspective, but differ in their\n\nlearning algorithm. All three are investigated in our experimental\nresults.\nLearning is based on \nN training examples  with\nand .  is a \nK-component vector. Each\ncomponent encodes a feature relevant for the learning task at hand.\n\nThe desired two-class output is encoded as 1 and +1. In the case of\nobject detection, the input component  is one haar-like feature.\nAn output of +1 and -1 indicates whether the input pattern does\n\ncontain a complete instance of the object class of interest. \n4  Cascade of Classifiers\nA cascade of classifiers is a degenerated decision tree where at each\n\nstage a classifier is trained to detect almost all objects of interest\n\n(frontal faces in our example) while rejecting a certain fraction of\nthe non-object patterns [6] (see Figure 8). For instance, in our case\neach stage was trained to eliminated 50% of the non-face patterns\n\nwhile falsely eliminating only 0.1% of the frontal face patterns; 20\nstages were trained. Assuming that our test set is representative for\nthe learning task, we can expect a false alarm rate about\n and a hit rate about .\nRecSumr\nSATx\n1y1SATxw\n1yh1+++=SATx\n1yh1+SATxw\n1y1+RSATxy\n45RSATxy\nIx\'y\'y\'yy\'yxx\n\'=RSATxy\nRSATx\n1y1+RSATx\n1+y1RSATxy\n2Ixy\nIxy\n1++=RSAT\n1yRSATx\n1RSATx\n20===\nRSAT\n11RSAT\n120==rxywh\n45\n=RecSumr\nRSATxh\nw+ywh\n1++RSATxy\n1+=RSATxh\nyh1+RSATxwyw\n1+++RSAT(x+1,y-1)\n+RSAT(x-1,y-1)\n-RSAT(x,y-2)\nFigure 4:Calculation scheme for Rotated Summed Area\nTables (\nRSAT\n).+I(x,y)+I(x,y-1)\nIxy\nIxy\n\nc-------------------\n-=cR+I2xy\n(x,y)\n+RSAT(x-h+w,y+w+h-1)\nwwhh+RSAT(x,y-1)\n-RSAT(x+w,y+w-1)\nFigure 5:Calculation scheme for rotated areas.\n-RSAT(x-h,y+h-1)\nx1y1\nxNyNxkyi11xixi0.5\n209.6\ne070.999\n200.98\n'b'Each stage was trained using one out of the three Boosting variants.\nBoosting can learn a strong classifier based on a (large) set of weak\nclassifiers by re-weighting the training samples. Weak classifiers\nare only required to be slightly better than chance. Our set of weak\n\nclassifiers are all classifiers which use one feature from our feature\npool in combination with a simple binary thresholding decision. At\neach round of boosting, the feature-based classifier is added that\n\nbest classifies the weighted training samples. With increasing stage\nnumber the number of weak classifiers, which are needed to achieve\nthe desired false alarm rate at the given hit rate, increases (for more\n\ndetail see [6]).\n5  Experimental Results\nAll experiments were performanced on the complete CMU Frontal\nFace Test Set of 130 grayscale pictures with 510 frontal faces [7]. A\nhit was declared if and only if \nthe Euclidian distance between the center of a detected and\nactual face was less than 30% of the width of the actual face as\nwell as\nthe width (i.e., size) of the detected face was within 50% of the\nactual face width. \nEvery detected face, which was not a hit, was counted as a false\nalarm. Hit rates are reported in percent, while the false alarms are\nspecified by their absolute numbers in order to make the results\n\ncomparable with related work on the CMU Frontal Face Test set. \nExcept otherwise noted 5000 positive frontal face patterns and 3000\nnegative patterns filtered by stage 0 to \nn-1 were used to train stage\nn of the cascade classifer. The 5000 positive frontal face patterns\nwere derived from 1000 original face patterns by random rotation\n\nabout 10 degree, random scaling about 10%, random mirroring\nand random shifting up to 1 pixel. Each stage was trained to reject\nabout half of the negative patterns, while correctly accepting 99.9%\n\nof the face patterns. A fully trained cascade consisted of 20 stages.\nDuring detection, a sliding window was moved pixel by pixel over\nthe picture at each scale. Starting with the original scale, the\nfeatures were enlarged by 10% and 20%, respectively (i.e.,\nrepresenting a rescale factor of 1.1 and 1.2, respectively) until\n\nexceeding the size of the picture in at least one dimension. \nOften multiple faces are detect at near by location and scale at an\nactual face location. Therefore, multiple nearby detection results\nwere merged. Receiver Operating Curves (ROCs) were constructed\nby varing the required number of detected faces per actual face\n\nbefore merging into a single detection result.\nDuring experimentation only one parameter was changed at a time.\nThe best mode of a parameter found in an experiment was used for\nthe subsequent experiments.\nDiscrete AdaBoost (Freund & Schapire [\n1])1.Given \nN examples  with \n2.Start with weights \nwi = 1/\nN, i = 1, ..., \nN.3.Repeat for \nm = 1, ..., \nM(a) Fit the classifier  using weights \nwi on the training data .\n(b) Compute , .\n(c) Set , \ni = 1, ..., \nN, and renormalize weights so that .\n4.Output the classifier \nFigure 6:Discrete AdaBoost training algorithm [\n1].x1y1\nxNyNxkyi11fmx11x1y1\nxNyNerrmEw1yfmx=cm1errmerrmlog\n=wiwicm1yifmxiexp\nwii1=signc\nmfmxm1=MGentle AdaBoost\n1.Given \nN examples  with \n2.Start with weights \nwi = 1/\nN, i = 1, ..., \nN.3.Repeat for \nm = 1, ..., \nM(a) Fit the regression function  by weighted least-squares of  to with weights \n\n(c) Set , \ni = 1, ..., \nN, and renormalize weights so that .\n4.Output the classifier \nFigure 7:Gentle AdaBoost training algorithm [1]\nx1y1\nxNyNxkyi11fmxyixiwiwiwiyifmxiexpwii1=signf\nmxm1=MFigure 8:Cascade of classifiers with \nN stages. At each\nstage a classifier is trained to achieve a hit rate of\n\nh and a false alarm rate of \nf.stage123  ......N  \nhitrateh\nN=hhhhh\ninput pattern classified as a non-object\n1-f\n1-f\n1-f1-f\nfalsealarmsf\nN='b'5.1 Feature Scaling\nAny multi-scale image search requires either rescaling of the\npicture or the features. One of the advantage of the Haar-like\nfeatures is that they can easily be rescaled. Independent of the scale\n\neach feature requires only a fixed number of look-ups in the sum\nand squared sum auxilary images. These look-ups are performed\nrelative to the top left corner and must be at integral positions.\n\nObviously, by fractional rescaling the new correct positions become\nfractional. A plain vanilla solution is to round all relative look-up\npositions to the nearest integer position. However, performance\n\nmay degrade significantly, since the ratio between the two areas of\na feature may have changed significantly compared to the area ratio\nat training due to rounding. One solution is to correct the weights of\n\nthe different rectangle sums so that the original area ratio between\nthem for a given haar-like feature is the same as it was at the original\nsize. The impact of this weight adapation on the performance is\n\namazing as can be seen in Figure 9.*-Rounding show the ROCs\nfor simple rounding, while *-AreaRatio shows the impact if also\nthe weight of the different rectangles is adjusted to reflect the\n\nweights in the feature at the original scale.\n5.2 Comparision Between Different Boosting Algorithms\nWe compared three different boosting algorithms:\nDiscrete Adaboost,\nReal Adaboost, and\nGentle Adaboost.\nThree 20-stage cascade classifiers were trained with the respective\nboosting algorithm using the basic feature set (i.e., features 1a, 1b,\n2a, 2c, and 4a of Figure 2) and stumps as the weak classifiers. As\ncan be seen from Figure 10, Gentle Adaboost outperformed the\n\nother two boosting algorithm, despite the fact that it needed on\naverage fewer features (see Table 2, second column). For instance,\nat a an absolute false alarm rate of 10 on the CMU test set, RAB\n\ndeteted only 75.4% and DAB only 79.5% of all frontal faces, while\nGAB achieved 82.7% at a rescale factor of 1.1. Also, the smaller\nrescaling factor of 1.1 was very beneficial if a very low false alarm\n\nrate at high detection performance had to be achieved. At 10 false\nalarms on the CMU test set, GAB improved from 68.8% detection\nrate with rescaling factor of 1.2 to 82.7% at a rescaling factor of 1.1. \nTable 2 shows in the second column (nsplit =1) the average number\nof features needed to be evaluted for background patterns by the\ndifferent classifiers. As can be seen GAB is not only the best, but\nalso the fastest classifier. Therefore, we only investigate a rescale\nscaling factor 1.1 and GAB in the subsequent experiments.\n5.3 Input Pattern Size\nMany different input pattern sizes have been reported in related\nNSPLIT\n1234DAB45.0944.4331.8644.86\nGAB30.9936.0328.5835.40\nRAB26.2833.1626.7335.71\nTable 2: \nAverage number of features evaluated per back-\nground pattern at a pattern size of 20x20.\n0.010\n0.100\n1.000\n0.300.400.500.600.700.800.901.00\nhit rate\nfalse alarm rate\nBASIC14 -Rounding\nBASIC14 - AreaRatio\nBASIC19 - AreaRatio\nBASIC19 - Rounding\nFigure 9:Performance comparision between different fea-\nture scaling approaches. *-Rounding rounds the\nfractional position to the nearest integer position,\n\nwhile *-AreaRatio also restores the ratio be-\n\ntween the different rectangles to its original value\n\nused during training.\nFigure 10:Performance comparison between identically trained cascades with three different boosting algorithms. Only the\nbasic feature set and stumps as weak classifiers (nsplit=1) were used.\n'b'work on face detection ranging from 16x16 up to 32x32. However,\nnone of them have systematically investigated the effect of the input\npattern size on detection performance. As our experiments show for\nfaces an input pattern size of 20x20 achieves the highest hit rate at\n\nan absolute false alarms between 5 and 100 on the CMU Frontal\nFace Test Set (see Figure 11). Only for less than 5 false alarms, an\ninput pattern size of 24x24 worked better. A similar observation has\n\nbeen made by [2]. \n5.4 Tree vs. Stumps\nStumps as weak classifer do not allow learning dependencies\nbetween features. In general, N split nodes are needed to model\ndependency between N-1 variables. Therefore, we allow our weak\n\nclassifier to be a CART tree with NSPLIT split nodes. Then,\nNSPLIT=1 represents the stump case.\nAs can be seen from Figure 12 and Figure 13 stumps are\noutperformed by weak tree classifiers with 2, 3 or 4 split nodes. For\n18x18 four split nodes performed best, while for 20x20 two nodes\n\nwere slighlty better. The difference between weak tree classifiers\nwith 2, 3 or 4 split nodes is smaller than their superiority with\nrespect to stumps.The order of the computational complexity of the\n\nresulting detection classifier was unaffected by the choise of the\nvalue of NSPLIT (see Table 1). The more powerful CARTs\nproportionally needed less weak classifiers to achieve the same\n\nperformance at each stage.\n5.5 Basic vs. Extended Haar-like Features\nTwo face detection systems were trained: One with the basic and\n\none with the extended haar-like feature set. On average the false\nalarm rate was about 10% lower for the extended haar-like feature\nset at comparable hit rates. Figure 14 shows the ROC for both\n\nclassifiers using 12 stages. At the same time the computational\ncomplexity was comparable. The average number of features\nevaluation per patch was about 31 (see [3] for more details).\nThese results suggest that although the larger haar-like feature set\nusually complicates learning, it was more than paid of by the added\n\ndomain knowledge. In principle, the center surround feature would\nhave been sufficient to approximate all other features, however, it is\nin general hard for any machine learning algorithm to learn joint\nbehavior in a reliable way.\n5.6 Training Set Size\nSo far, all trained cascades used 5000 positive and 3000 negative\nexamples per stage to limit the computational complexity during\ntraining. We also trained one 18x18 classifiers with all positive face\nexamples, 10795 in total and 5000 negative training examples. As\n\ncan be seen from Figure 15, there is little difference in the training\nresults. Large training sets only slightly improve performance\nindicating that the cascade trained with 5000/3000 examples\n\nalready came close to its representation power.\nConclusion\nOur experimental results suggest, that 20x20 is the optimal input\n\npattern size for frontal face detection. In addition, they show that\nFigure 11:Performance comparison between identically\ntrained cascades, but with different input pattern\n\nsizes. GAB was used together with the basic fea-\n\nture set and stumps as weak classifiers (nsplit=1).\nFigure 12:Performance comparison with respect to the or-\nder of the weak CART classifiers. GAB was used\ntogether with the basic feature set and a pattern\n\nsize of 18x18.\nFigure 13:Performance comparison with respect to the or-\nder of the weak CART classifiers. GAB was used\n\ntogether with the basic feature set and a pattern\nsize of 20x20.\n'b'Gentle Adaboost outperforms Discrete and Real Adaboost.\nLogitboot could not be used due to convergence problem on later\nstages in the cascade training. It is also beneficial not just to use the\n\nsimplest of all tree classifiers, i.e., stumps, as the basis for the weak\nclassifiers, but representationally more powerful classifiers such as\nsmall CART trees, which can model second and/or third order\n\ndependencies.\nWe also introduced an extended set of haar-like features. Although\nfrontal faces exhibit little diagonal structures, the 45 degree rotated\nfeatures increased the accuracy. In practice, the have observed that\nthe rotated features can boost detection performance if the object\n\nunder detection exhibit some diagonal structures such as many\nbrand logos.\nThe complete training and detection system as well as a trained face\ndetector are available in the Open Computer Vision Library at http:/\nsourceforge.net/projects/opencvlibrary/ [8].\n6  REFERENCES\n[1]Y. Freund and R. E. Schapire. Experiments with a new boost-\ning algorithm. In Machine Learning: Proceedings of the Thir-\nteenth International Conference, Morgan Kauman, San\nFrancisco, pp. 148-156, 1996.\n[2]Stan Z. Li, Long Zhu, ZhenQiu Zhang, Andrew Blake,\nHongJiang Zhang, and Harry Shum. Statistical Learning of\n\nMulti-View Face Detection. In Proceedings of  The 7th Euro-\npean Conference on Computer Vision. Copenhagen, Denmark.\nMay, 2002.\n[3]Rainer Lienhart and Jochen Maydt. An Extended Set of Haar-\nlike Features for Rapid Object Detection. IEEE ICIP 2002,\nVol. 1, pp. 900-903, Sep. 2002. \n[4]A. Mohan, C. Papageorgiou, T. Poggio. Example-based object\ndetection in images by components. IEEE Transactions on Pat-\ntern Analysis and Machine Intelligence, Vol. 23, No. 4, pp. 349\n\n-361, April 2001.\n[5]C. Papageorgiou, M. Oren, and T. Poggio. A general frame-\nwork for Object Detection. In \nInternational Conference on\nComputer Vision\n, 1998.\n[6]Paul Viola and Michael J. Jones. Rapid Object Detection using\na Boosted Cascade of Simple Features. IEEE CVPR, 2001.\n[7]H. Rowley, S. Baluja, and T. Kanade. Neural network-based\nface detection. In IEEE Patt. Anal. Mach. Intell., Vol. 20, pp.\n22-38, 1998.\n[8]Open Computer Vision Library. http:/sourceforge.net/projects/\nopencvlibrary/\nFigure 14:Basic versus extended feature set: On average\nthe false alarm rate of the face detector exploiting\n\nthe extended feature set was about 10% better at\n\nthe same hit rate (taken from [3]).\n0.9\n0.91\n0.92\n0.93\n0.94\n0.95\n0.96\n0.001\n0.0015\n0.002\n0.0025\n0.003\n0.0035\n0.004\n0.0045\n0.005\nhit rate\nfalse alarms\nPerformance comparison between basic and extented feature set\nUsing Basic Features\nUsing Extended Features\nFigure 15:Performance comparison with respect to the\ntraining set size. One 18x18 classifier was\n\ntrained with 10795 face and 5000 non-face ex-\namples using GAB and the basic feature set.\n'