b'A Statistical Approach to 3D Object DetectionApplied to Faces and Cars\nHenry SchneidermanCMU-RI-TR-00-06May 10, 2000Robotics InstituteCarnegie Mellon University\nPittsburgh, PA 15213\nThesis Committee:Takeo Kanade, Carnegie Mellon, Chair\nAlex Pentland, MIT Media Lab\nTai Sing Lee, Carnegie Mellon\nDean Pomerleau, AssistWare Technology\n'b'AbstractIn this thesis, we describe a statistical method for 3D object detection.   In this method, wedecompose the 3D geometry of each object into a small number of viewpoints.  For each view-\n\npoint, we construct a decision rule that determines if the object is present at that specic orienta-\ntion.  Each decision rule uses the statistics of both object appearance and non-object visual\nappearance.  We represent each set of statistics using a product of histograms.  Each histogram\n\nrepresents the joint statistics of a subset of wavelet coefcients and their position on the object.\n\nOur approach is to use many such histograms representing a wide variety of visual attributes.\n\nUsing this method, we have developed the rst algorithm that can reliably detect faces that vary\n\nfrom frontal view to full prole view and the rst algorithm that can reliably detect cars over a\n\nwide range of viewpoints.\n'b'Acknowledgments\nI would like to thank my advisor, Takeo Kanade, for his guidance in this research.  His ideas,\ninsights, suggestions, questions, and enthusiasm were great help in stimulating my thought and in\nbringing this research forward.  I would like to thank the other members of my dissertation com-\n\nmittee Tai Sing Lee, Dean Pomerleau, and Sandy Pentland for their helpful feedback and sugges-\n\ntions.  I would like to thank John Krumm for originally guiding me into this area of research as\n\npart my summer internship in his laboratory in Sandia National Laboratories in 1995.  I would\n\nlike to thank Henry Rowley for his feedback on my research and for providing much useful data\n\nand programs.  I would also like to thank the many people with whom I had valuable discussions,\n\nprovided helpful feedback on my research, and provided technical assistance including Chris Lee,\n\nAlan Lipton, Rahul Sukthankar, Tim Doebler, Larry Ray, Farhana Kagalwala, David LaRose,\n\nMichael Nechyba, Frank Dellaert, Mark Ollis, Simon Baker, Teck Khim Ng, Chris Diehl, Dennis\n\nStrewlow, Martin Martin, Dongmei Zhang, YingLi Tian, Bob Collins, Jeff Cohn, Terence Sim,\n\nDave Duggins, Tom Drayer, Yanxi Liu, Tsuhan Chen, Tommy Poggio, Huang Fu Jie, Steve Seitz,\n\nSundar Vedula, Jeff Schneider, Geoff Gordon, and Marina Meila. And nally, most of all, I would\n\nlike to thank Laura and the rest of my family for their unwavering support and encouragement\n\nduring my graduate studies.'b' 1   Introduction1\n1.1  Challenges in Object Detection. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .2\n\n1.2  Choosing a Representation for Object Detection . . . . . . . . . . . . . . . . . . . . . . . . . . . . .4\n\n1.3  Representational Choices for Object Detection. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .5\n\n1.4  Detection of Human Faces and Cars. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .7\n\n1.5  Overview of Approach . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .7\n 2  View-Based Detectors10\n\n 3  Functional Form of Detector:   Statistical Representation Using Histograms13\n3.1  Representation of Object and Non-Object Statistics. . . . . . . . . . . . . . . . . . . . . . . . . .14\n\n3.2  Representation of Statistics Using Histograms. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .15\n\n3.3  Multiple Histograms to Represent Multiple Attributes of Visual Appearance. . . . . .17\n\n3.4  Parts-based Decomposition of Appearance. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .19\n3.4.1   Frequency/Scale Decomposition. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .19\n3.4.2   Orientation Decomposition. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .21\n\n3.4.3   Generalized Decomposition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .21\n\n3.4.4   Spatial Decomposition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .21\n\n3.4.5   Geometric Decomposition. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .22\n3.5  Implementation of Visual Attributes. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .23\n3.5.1   Wavelet Transform . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .24\n3.5.2   Quantization of Wavelet Coefficients. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .27\n\n3.5.3   Specification of Visual Attributes. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .29\n3.6  Overall Decision Rule and Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .32\n3.7  Appendix: Wavelet Transform . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .34\n 4  Functional Form of Detector:Re-derivation from an Ideal Form36\n4.1  Ideal Functional Form of Detector . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .36\n'b'4.2  Generalizations to Ideal Functional Form. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .37\n4.3  Wavelet Transform of Image. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .42\n\n4.4  Simplifications to Functional Form. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .42\n4.4.1   Statistical Independence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .42\n4.4.2   Quantization of Wavelet Coefficients. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .45\n\n4.4.3   Reduced Resolution in Pattern Position . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .46\n4.5    Conclusion. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . \n. .46\n 5  Training Detectors47\n5.1  Images of the Object . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .47\n5.1.1   Collection of Original Images. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .47\n5.1.2   Size Normalization and Spatial Alignment. . . . . . . . . . . . . . . . . . . . . . . . . . . . .48\n\n5.1.3   Intensity Normalization. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .49\n\n5.1.4   Synthetic Variation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .50\n5.2  Non-Object Images . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .51\n5.3  Training Method (I) - Probabilistic Approximation . . . . . . . . . . . . . . . . . . . . . . . . . .52\n\n5.4  Training Method (II) - Minimization of Classification Error using AdaBoost. . . . . .52\n 6  Implementation of the Detectors57\n6.1  Exhaustive Search. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .57\n\n6.2  Searching for Objects at One Size. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .57\n\n6.3  Re-using Multi-resolution Information. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .60\n\n6.4  Heuristic Speed-ups. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .61\n6.4.1   Coarse to Fine Search Strategy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .61\n6.4.2   Adjacent Heuristic. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .61\n\n6.4.3   Color Heuristics. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .61\n6.5  Performance Time. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .62\n 7  Face Detection Performance63\n7.1  Results in Face Detection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .63\n'b'7.2  Positional Decomposition of Face Detection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .68\n7.3  Analysis of Pair-wise Statistical Dependency. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .68\n 8  Car Detection Performance72\n8.1  Results in Car Detection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .72\n\n8.2  Positional Decomposition of Car Detection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .74\n\n8.3  Analysis of Pair-wise Statistical Dependency. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .75\n 9  Review of Other Statistical Detection Methods77\n9.1  Comparison of Our Approach to Previous Detection / Recognition Methods . . . . . .77\n9.1.1   Local Appearance Versus Global Appearance . . . . . . . . . . . . . . . . . . . . . . . . . .77\n9.1.2   Sampling of Local Appearance. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .77\n\n9.1.3   Representation of Geometric Relationships . . . . . . . . . . . . . . . . . . . . . . . . . . . .78\n\n9.1.4   Representation of Local Appearance using Wavelets. . . . . . . . . . . . . . . . . . . . .78\n\n9.1.5   Representation of Statistics/Discriminant Functions . . . . . . . . . . . . . . . . . . . . .79\n\n9.1.6   Estimation of Statistics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .79\n9.2  Summary of Previous Methods for Face Detection . . . . . . . . . . . . . . . . . . . . . . . . . .80\n9.2.1   Sung and Poggio [18] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .80\n9.2.2   Rowley, Baluja, and Kanade [14] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .81\n\n9.2.3   Osuna, Freund, and Girosi [19]. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .82\n\n9.2.4   Moghaddam and Pentland [12]. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .82\n\n9.2.5   Colmenarez and Huang [20] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .84\n\n9.2.6   Burl and Perona [26]. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .86\n\n9.2.7   Roth, Yang, Ahuja [38]. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .89\n9.3  Summary of Previous Methods for Car Detection . . . . . . . . . . . . . . . . . . . . . . . . . . .90\n9.3.1   Rajagopalan, Burlina, Chellappa [44]. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .90\n9.3.2   Papageorgiou, Poggio [83] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .90\n 10  Conclusion91\nReferences 95\n'b'1Chapter 1. Introduction\nObject detection is a big part of our lives.  We are constantly looking for and detecting objects:\npeople, streets, buildings, hallways, tables, chairs, desks, sofas, beds, automobiles. Yet it remains\n\na mystery how we perceive objects so accurately and with so little apparent effort.   Comprehen-\n\nsive explanations have deed physiologists and psychologists for more than a century.\nIn this thesis, our goal is not to understand how humans perceive, but to create computer meth-\nods for automatic object detection.  Automated object detection could have many uses.  The avail-\n\nability of large digital image collections has grown dramatically in recent years.  Corbis estimates\n\nit currently has more than 67 million images in its current collection[1].  The Associated Press\ncollects and archives an estimated 1,000 photographs a day[2].  The number of images on the\n\nWorld Wide Web is at least in the hundreds of millions.  However, the usability of these collec-\n\ntions is limited by a lack of effective retrieval methods.  Currently, to nd a specic image in such\n\na collection, we have to search using text-based captions and low-level image features such as\n\ncolor and texture.   Automatic object detection and recognition could be used to extract more\nFigure 1.  Examples of computer detection of human faces\nFigure 2.  Examples of computer detection of automobiles'b'2information from these images and help automatically label and categorize them.   By making\nthese databases easier to search, they will become accessible to wider groups of users, such as\n\ntelevision broadcasters, law enforcement agencies, medical practitioners, graphic and multimedia\n\ndesigners, book and magazine publishers, journalists, historians, artists, and hobbyists.  Auto-\nmatic object detection could also be useful in photography. As camera technology changes from\n\nlm to digital capture, cameras will become part optics and part computer (giving true meaning to\n\nthe term computer vision).   Such a camera could automatically focus, color balance, and zoom\n\non a specied object of interest, say, a human face.  Also, specic object detectors, such as a face\n\ndetectors and car detectors, have specialized uses.  Face detectors are a necessary component in\n\nany system for automatic face identication.  Car detectors could be used for automatically moni-\n\ntoring trafc.\n1.1.Challenges in Object Detection\nAutomatic object detection is a difcult undertaking.  In 30 years of research in computer\nvision, little progress has been made.  The main challenge is the amount of variation in visual\n\nappearance. For example, cars vary in size, shape, coloring, and in small details such as the head-\n\nlights, grill, and tires.  An objects orientation and distance from the camera affects its appearance.\n\nA more general difculty is that visual information is ambiguous.  Geometric ambiguity exists\n\nsince the three dimensions of the world are projected on to two in the image.  Also, a pixels inten-\n\nsity depends on many dispersed factors in the environment.  It depends on the light sources: their\n\nlocations, their color, their intensity.  It depends on the surrounding objects.  Some objects may\n\ncast shadows on the object or reect additional light on to the object.  Pixel intensity also depends\n\non the reective properties of the viewed surfaces.  A smooth surface will reect light differently\n\nthan a rough one.Figure 3. Objects of the same class can vary signicantly in appearance\n'b'3Object detection is also difcult because images contain a large amount of data.  There may be\nhundreds or even thousands of input pixels, each of which may carry important information.  To\n\nuse this information to its fullest extent, we would have to build the detector as an enormous table\n\nwith an entry for every possible input indicating its classication, object or non-object, such as\n\nTable 1.  Such a representation would account for all the forms of variation we just mentioned.\nUnfortunately, such a table is infeasible. entries would be required for describing\nthe classication of a 20x20 region. Computer power and memory limit us to using a classica-\ntion rule that is hundreds of orders of magnitude smaller.  However, there is hope that we can get\n\nby with such a representation.  The physical world imposes constraints on the appearance of\n\nobjects; that is, of all the possible images that could conceivably exist in the physical world, only\n\na small subset actually do.  Moreover, people and animals are living examples things that achieve\n\nsuccessful perception within their own computational limits.\nTable 1: Ideal but infeasible classier\n(1,1)(1,2). . .(20,20)classication\n00. . .0Non-object\n00. . .1Non-object\nFigure 4.  Variation due to the pose (the relationship between the orien-\ntation of the object and the position of the camera)Figure 5.  Variation due to lighting and shadowing\n25640010963'b'41.2.Choosing a Representation for Object Detection\nThe main issue in object detection is choosing a good representation within our computingconstraints.  An object detector must accommodate all the variation we mentioned and still distin-\n\nguish the object from any other visual scene in the world.\nThe most attractive way to choose a representation would be to use an automated method that\nwould choose it for us.    Such a goal has motivated many methods such as genetic algorithms and\n\nprune and grow methods for articial neural networks. However, with current computer power,\n\nthis is too great a challenge.  The search space of possible models is enormous for the high dimen-\n\nsional spaces created by images.Ultimately, we must make representational choices by hand.\n1  In making these choices, wehave to ask the question:\nWhat relationships help distinguish the object from the rest of the visual\nworld?  The best we can do is make educated guesses.   We can look at the work of other research-\ners in object recognition and detection and try to understand why some models have been more\n\nsuccessful than others.  We can look at psychological and biological studies on how animals and\n\npeople see. And, we can use our intuition about what makes an object distinct from everything\n\nelse.  These sources of knowledge helped us to design the object detector we use in this thesis.\nLet us rst look at what some of the representational choices and trade-offs are.\n1.This statement may sound surprising. Most papers on recognition do not discuss mak-ing such decisions.  This omission is probably because these methods start from a repre-\n\nsentational framework (e.g., a mixture of Gaussians, multilayer perceptron, quadratic\n\nlter, etc.) in which representational possibilities have already been pruned.\n. . .. . .. . .. . .. . .\n3545. . .28Object\n\n. . .. . .. . .. . .. . .\n\n255255. . .255Non-object\nTable 1: Ideal but infeasible classier\n(1,1)(1,2). . .(20,20)classication\n'b'51.3.Representational Choices for Object Detection\nBelow we outline some of the major differences among existing methods for detection and rec-\nognition: 3D vs. 2D - An object can be represented directly through a 3D model or indirectly through acollection of 2D models.  For example, alignment methods [3] use explicit 3D models relating the\n\nposition of key features.  Modular viewpoint-based models [4][30][29] decompose appearance\n\ninto separate 2D models corresponding to different viewpoints.  There can be a small nite num-\n\nber of viewpoints [4] or the 2D information could be represented as a continuous function of the\n\n3D pose of the object [30][29].  Generally, it is easier to represent appearance in 2D models than\n\nin 3D models.  Conversely, shape can be directly represented using 3D models but only indirectly\n\nrepresented using 2D models. Wholistic appearance vs. appearance of parts - There are many algorithms that treat the\nobjects appearance as one monolithic quantity such as [30][29][5][6].  At the other extreme there\n\nare algorithms that represent the objects appearance as a collections of parts (edges, corners,\n\ncomplex features) [9][10][39].  The trade-off is that while monolithic models capture relation-\n\nships across the full extent of the object, describing the model in terms of smaller parts concen-\n\ntrates the representation power and describes the appearance of these smaller portions in greater\n\ndetail. Feature vs. Geometry -  If we use a parts-based description, there is trade-off between how\nmuch representational power we allocate to the description of the parts themselves and how much\n\nwe allocate to a description of their geometric relationships.  At one extreme, some representa-\n\ntions do not model spatial relationships [9][84].  At the other extreme, others use a minimalist fea-\n\nture representation and emphasize geometric relationships [32][41][45][46][47].  Sampling density - If we represent the object in terms of parts, there is the question of whatparts to represent.  Do we choose a few salient features.  Say the eyes, nose, and mouth on a face\n\nor some other relatively small subset of the visual information [26][4][40]?  Do we use an inter-\n\nest operator or some other automatic method to select a small set of features to use [81][39][82].\nOr do we uniformly sample the appearance of the object.  Obviously, both representation power\n'b'6cost increase by using a higher sampling density.\n  Non-object representation - Detection can be thought of as a two category classication\nproblem.  How do we represent the non-object category.  Do we not represent it all and thereby\n\nimplicitly assume a uniform distribution.  Or do we model it explicitly using real-imag-\n\nery.[14][18]  And if so what and how do we select images and weight them?\nProbabilistic representation - At one extreme we can represent the object as a xed determinis-\ntic template.  Such a representation will work well if the objects appearance does not vary and\n\nenvironment (lighting, etc.) is controlled.  As object recognition methods try to cope with increas-\n\ning degrees of variation in the object and environment, they use different modeling techniques.\n\nFor example, if the only form of variation is lighting intensity, normalized correlation may be the\n\nbest solution.  If the objects appearance is also subject to variation there are a plethora of methods\n\nand techniques.  Below we discuss some the issues in making these choices:\n- Dimensionality reduction - To use any statistical representation any method must signi-\ncantly reduce the dimensionality of the image in order to avoid overtting.  There are many ways\n\nto do this such as linear projection methods[5][29][30], and resolution reduction meth-\nods[14][18], or simply by hand selecting features.- Probability functions vs. discriminant functions - We can model the statistics of each class\nseparately, or we can model the classication problem directly through a discriminant function.\n- Flexibility of model - How many variable parameters are t to the training data?  Can the rep-\nresentation divide input space with a hyperplane, conic section[19][81][82], etc.  Can the proba-\n\nbility distributions represent multiple modes?\n- Parametric vs. non parametric - Non parametric models make no assumptions about the\nnature of the distribution but are more costly in terms of data reduction requirements and retrieval\n\nof probabilities.- Estimation \\ learning method-  Method for tting model to training data.  Can best t beachieved or is method subject to local minima.  Does closed-form solution exist or is method iter-\n'b'7ative?\n-Complexity of evaluation of probability - Given an input how much computation is required to\nretrieve the associated probability?\n1.4.Detection of Human Faces and Cars\nOur representational choices will depend on the object we are trying to detect.  It is unlikely\nthat one representation will work for all kinds of objects.  In this thesis, we develop methods for\n\ndetection of human faces and cars.  For automobile detection we detect 2-door and 4-door passen-\n\nger cars excluding all trucks, sport utility vehicles, vans, mini-vans, station wagons, racing cars,\n\nand pre-1950 models.  For both objects we detect upright objects allowing for variation between\n\nfull frontal view to full side view (no back views) and we have placed no restrictions on back-\n\nground scenery, lighting, or any other naturally occurring environmental conditions.\nWe choose faces and cars because we wanted the challenge of two objects that differ in many\nways.  Faces are natural objects that are irregular in their shape.  Cars are man-made and consist\n\nprimarily of at surfaces.  The surfaces on faces tend to be dull and non-reective while those on\n\ncars tend to be shiny and reective.  Yet, in spite of these difference, we also choose these objects\n\nbecause they are similar in a more abstract way.  They are both semi-rigid objects that have dis-\n\ntinctive features in a xed geometric arrangement.  On faces, the eyes, nose, and mouth form a\n\nunique conguration.  On cars, the grill, windshield, tires, and headlights are in a relatively xed\n\narrangement.  This similarity between cars and faces allows us to use the same underlying model\n\nfor both objects.  By being able to do so, we view this work as a rst step toward developing a\n\ngeneral method that applies to the whole class of semi-rigid objects.  Hopefully, with little modi-\n\ncation other than that of the training data, we will be able to apply this method to the detection of\npedestrians, airplanes, ships, boats, bicycles, tanks, re hydrants, stop lights, some animals ,and\n\nmany other objects with similar properties.\n1.5.Overview of Approach\nWe use a 2D view-based approach for representing the 3D geometry of each object.  In this\napproach, we use several decision rules for each object where each rule decides if the object is\n'b'8present at a particular orientation with respect to the camera.   In Chapter 2, we describe how we\npartition the 3D geometry of each object into decision rules for separate views and how we nor-\n\nmalize the size and alignment of the object within each of these views.\nAll of these decision rules (for different objects and for different viewpoints for the same\nobject) have the same underlying functional form.  The only difference among the rules is that\n\neach one is trained on a different set of images.  The key issue we address in this thesis is the\n\nchoice of the decision rules functional form.  This function form is the embodiment of all our\n\nrepresentational choices.We use a statistically based representations to model variation in appearance for the object and\nvariation in the appearance of the rest of the visual world.  We capture these statistics using a rep-\n\nresentation based on multiple histograms.  Each histogram captures the statistics of a different\n\nattribute of visual appearance.  These attributes represent appearance over differing spatial\n\nextents, frequency ranges, and orientations.  We use a large set of different attributes to take\n\nadvantage of as much information as possible in making a detection decision.  Depending on the\n\nimage, we may have to rely on different cues.  For example, in Figure 6 we show the variety of\n\ncues that may or may not exist in images of a human face, such as lack or presence of low/high\n\nfrequencies, glasses, facial hair, shadowing, surface markings, etc.  In Chapter 3, we describe\n\nthese representational choices and derive the functional form of our detector.\nWe also view our representation as our best attempt at approximating an ideal functional form\nwithin our computational constraints.  In Chapter 4 we re-derive our functional form by a series of\nFigure 6.  The cues available for detecting faces vary widely from image to image.\n'b'9approximations to this ideal form.  Through this analysis we gain a better understanding of several\nissues that are not obvious in the rst derivation.  This derivation reveals the statistical indepen-\n\ndence assumptions between the attributes.  It also reveals why it is useful to select non-object\n\nsamples by bootstrapping and why we should explicitly train the detector to reduce the classica-\n\ntion error over the training set.\nIn Chapter 5 we describe how we train each decision rule by collecting statistics from a speci-\ned set of training images.  First, we describe how we use bootstrapping to select non-object\n\nsamples.  We then describe how we use the AdaBoost algorithm to train the decision rules to\n\nexplicitly minimize classication error on the training set.\nIn Chapter 6 we describe the implementation of the detector.  We discuss issues related to\nspeed and efciency.  In particular, our approach is to search the image in a coarse-to-ne fashion\n\npruning out unpromising candidates along the way.\nWe describe our performance for face detection in Chapter 7 and our performance for car\ndetection in Chapter 8.In Chapter 9 we compare our approach to other probabilistic approaches to object detection,specically those for face detection and car detection.\nIn Chapter 10 we summarize our approach and discuss future topics of research.'b'10Chapter 2.View-Based Detectors\nOur overall goal is to be able to detect the object over a range of orientations, sizes, and posi-\ntions in an image.  We use a 2D view-based approach to accommodate variation in orientation and\n\nwe use exhaustive search in position and scale to accommodate variation in size and position.\nA view-based approach works as follows.  For each object, we build several detectors where\neach one is specialized to specic orientation of the object and can accommodate small amounts\nof variation around this orientation.  To be able to detect an object at any orientation we apply all\n\nthese detectors to the image and merge their results such that they are spatially consistent.  In  Fig-\n\nure 7 we show such a face detection result using this approach.   In this example, each detector\nleft proledetectorright proledetectorfrontal face\ndetectormerged\ndetectionsFigure 7.View-based detectors\n'b'11detects all the faces corresponding to its orientation.  The woman in front was initially detected by\nboth the frontal and left prole detectors, because the orientation of her face is somewhat in-\n\nbetween these orientations.  However, when the algorithm spatially integrates these results and\n\nchooses the more condent detection which in this case is the frontal detection.It may seem counter-intuitive to use a 2D based model such as this to represent a 3D object, but\nthere is an advantage to doing so.  The problem with a 3D model is that we do not have explicit\n\nknowledge of the 3D geometry of the object.  All our information is in the form of 2D images.  To\n\nmaintain a 3D representation we would have to rely on 3D recovery methods which are error-\n\nprone.  By maintaining 2D models we avoid introducing such errors into our representation.\nThe question of how many and which viewpoints to use is an open question.  One possible\nanswer is to select viewpoints from aspect graphs if the object has well-dened surfaces [62].\n\nHowever, our approach was to simply determine the number of viewpoints through experimenta-\n\ntion.  For face detection, we found that three separate detectors was sufcient: left-prole, frontal\n\nviews, and right-prole.  In practice, we built only two detectors, right-prole and frontal, since\n\nwe can detect left-proles by applying the right prole detector to a mirror-reversed image. We\n\nshow example training images for these in Figure 8. For automobile detection, we originally used\nthree detectors, left-side, front, and right-side, but found it was necessary to use more.  There are\nseveral explanations for this.  Automobile photographs tend to be taken from a wider variety of\n\nvantages, from road level to views from a higher vantage point.  In comparison, we usually photo-\n\ngraph faces at eye level, except in surveillance cameras.  Also, the shape of an automobile is recti-\n\nlinear.  Small changes in angle will produce bigger changes in appearance than they do for a\n\nsphere.  Overall, we used 15 decision rules corresponding to the following orientations: one fron-\nFigure 8.  Example training images for frontal and right prole face views\n'b'12tal viewpoint and 14 side viewpoints.  Here again, we only had to train 8 detectors (7 right side\ndetectors and one frontal detector), since 7 viewpoints are mirror reections of each other.  In Fig-\n\nure 9 we show example training images for each of the viewpoint we trained on .   We do not\n\ndetect back views of cars.  Also for both object we do not represent in-plane rotations.  Both faces\n\nand cars tend to appear as upright objects.In addition to detecting the object over variation in orientation, we also have to detect it over\nvariation in size and position within the image.  Our approach to detecting the object under these\n\nvariations is to use exhaustive search.  We train each view-based detector to only nd the object\n\nwhen it is normalized in size and centered in a given rectangular image window. (We design each\n\nsuch detector to accommodate small variation about this size and alignment.)  To then detect the\n\nobject at position in an image, we have to re-apply each detector at all possible positions of the\n\nrectangular window.  Then to detect the object at any size, we have to repeat this process over\n\nmagnied and contracted versions of the original image.\nFigure 9.  Example training images for each car viewpoint\n'b'13Chapter 3.Functional Form of Detector:\nStatistical Representation Using Histograms\nIn this chapter we derive the functional form of our detector.  We use a statistical representation\nto model variation in visual appearance.  We model both the statistics of appearance of the object\n\nand the statistics of the rest of the world.  The difculty in modeling these distributions is that we\n\ndo not know their true characteristics.  We do not know if they are Gaussian, Poisson, multi-\n\nmodal, etc. These characteristics are unknown since it is not tractable to analyze the joint statistics\n\nof large numbers of pixels.  Therefore, we sought statistical models that avoid making strong\n\nassumptions about distributional structure while still retaining good properties for estimation and\n\nretrieval.  As we will explain, the best compromise we found was histograms.\nHistograms, however, have one fundamental limitation.  A histogram can only use a discrete\nnumber of values to describe appearance.  More importantly, because of limited computer mem-\n\nory and nite training data, a histogram can only use a relatively small number of discrete values.\n\nTo overcome this limitation we will describe how we use multiple histograms where each histo-\n\ngram represents the statistical behavior of a different group of quantized wavelet coefcients.\n\nWith this representation, each histogram represents a different attribute of appearance in terms of\n\nspatial extent, frequency range, orientation. Our approach is to use many such histograms to make\n\nup for the limited scope and resolution of each individual one.\nIn this approach, by modeling groups of wavelet coefcients, we capture the statistics of\nappearance over limited spatial extents.  However, we would also like to capture the overall geo-\n\nmetric conguration of the object.  Therefore, as we will explain, in each histogram, we represent\n\nthe joint statistics of appearance and position, where we measure position with respect to a local\ncoordinate frame afxed to the object.  This representation implicitly captures each parts relative\n\nposition with respect to all the others.'b'143.1.Representation of Object and Non-Object Statistics\nCoping with variation in appearance is a big problem in object detection.  We not only have to\ncope with variation in the object, but we also have to cope with variation in the rest of the world;\n\nthat is, we have to distinguish the object from anything else that might occur in the world.  For this\n\nreason, we model both the statistics of appearance of the object,P(image\n| object), and the statis-tics of appearance of the rest of the visual world,\nP(image\n| non-object).Intuitively, the non-object statistics help us account for the distinctiveness or uniqueness of fea-\ntures.  Some features on an object are more distinctive than others.  For example, a paisley pattern\n\nis more distinctive than a plain white pattern.   On a human face, the eyes are more distinctive than\n\nthe areas on the cheeks.  Cheeks have little texture and could look like many other things in the\n\nvisual world.   However, an eye-like pattern and a cheek-like pattern will have similar proba-\n\nbilities of occurrence on a face:\n(1)since each face has two cheeks and two eyes.  Yet, an eye seems to be a better indicator of a face\nand should carry more inuence than a cheek in making the classication decision.We use the statistics of the rest of the world,\nP(image\n|non-object), to capture this notion of\ndistinctiveness.  Since eye-like patterns are rarer than cheek-like patterns in the visual world\n\nat large, we would expect that:\n(2)Therefore, we can get a better measure of a given features classication power by dividing\n1 itsobject probability by its non-object probability:1.It may seem arbitrary that we divided the two probability distributions in equations (3)\nand (4).  Conceivably, we could have combined them in other ways, such as by subtrac-\n\ntion.  When we re-derive the decision rule in Chapter 4, we will see how this equation\n\nfollows from Bayes decision rule.\nPeye-like patternface\n()Pcheek-like patternface\n()Peye-like patternnon-face\n()Pcheek-like patternnon-face\n()<'b'15(3)This ratio will be large for distinctive features on the object -- patterns that occur frequently on the\nobject but occur infrequently in the rest of the visual world.\nOur overall model for the decision rule will then be the ratio of the two probabilities:\n(4)If this ratio is greater thanl, we will decide that the object is present.  If it is less thanl then wedecide that the object is not present.In the rest of the chapter we explain how we represent the two probability distributions,\nP(image\n|object) andP(image\n|non-object).\n3.2.Representation of Statistics Using Histograms\nThe difculty in modeling\nP(image\n| object) andP(image\n|non-object), is that we do not know\nthe true statistical characteristics of appearance either for the object or for the rest of the world.\n\nFor example, we do not know if the true distributions are Gaussian, Poisson, multimodal.  These\n\nproperties are unknown since it is not tractable to analyze the joint statistics of large numbers of\n\npixels.\nSince we do not know the true structure of these distributions, the safest approach is to choose\nmodels that are exible and can accommodate a wide range of structure.  The most exible repre-\n\nsentations are non-parametric methods, such as nearest neighbor, Parzen windows and other ker-\n\nnel density models.  However, there are two problems with non-parametric models.  First, the high\n\ndimensional nature of images presents a problem.  An enormous set of training examples and\n\ndrastic dimensionality reduction are necessary to avoid overtting.  The second problem is the\n\nhigh computational cost of probability retrieval from these models.  For a given input, probability\n\nretrieval involves a computation over the entire training set; that is, we have to compare the input\n\nto all the training data.  Even if efcient strategies such as k-d trees are used, probability retrieval\nPfeature\nobject()Pfeature\nnon-object()------------------------------------------------------------Pimage\nobject()Pimage\nnon-object()--------------------------------------------------------l>'b'16calculations will be very time consuming.\nAn alternative would be to use exible parametric distributions such as a multilayer perceptron\n(articial neural network) or mixture model.  Each of these models has some exibility to model\n\nmulti-modal distributions and become a universal approximators as their sizes increase towards\n\ninnity (i.e., a multilayer perceptron with innite hidden units, or a mixture model with innite\nnumber of modes).  However, there are no closed form solutions for tting these models to a set of\n\ntraining examples.  Instead, we must estimate their parameters by iterative procedures: gradient\n\ndescent (backpropagation) and E-M, respectively.  These parameter estimates can become trapped\n\nin a local minimum and be suboptimal. Also, it is questionable whether such models are appropri-\nate for detection.  For example, a multilayer perceptron forms decision boundaries that are combi-\n\nnations of hyperplanes to a rst approximation. It is not clear whether such decision boundaries\n\nwill be good for separating two or more classes in a high dimensional space [51][52].\nInstead of these approaches, we use histograms as a basis for probabilistic representation (see[9][10][54] for other histogram-based methods in computer vision).  Histograms are almost as\nexible as non-parametric methods.  Histograms, however, have the advantage that probabilities\n\ncan be retrieved by a table look-up rather than a computation over the original training data.  Esti-\n\nmation of a histogram is also trivial.  We count how often each attribute value occurs in the train-\n\ning data.  This involves just one pass through the training images.  The resulting estimates are\n\nstatistically optimal: maximum likelihood, no bias, consistent, and  efcient (they satisfy the\n\nCramer-Rao lower bound).\nWe can model the accuracy of a histogram by considering each bin to be a binary variable:\neither the input falls in the bin outside of the bin.  Let us say the true probability that the input falls\n\nin the bin isp.  We can then model the probability of counting\nk occurrences inn trials by a bino-mial distribution:\n(5)Given values for\nk andn, we can compute the maximum likelihood estimate for\np as:Prk occurences in n trials()n!k!nk()!-----------------------\npk1p()nk='b'17(6)We can also compute the variance in this estimate:\n(7)As we can see the variance in our statistical estimates will be quite low if\nn is large.   This formula\nwill tell us how accurate our probability estimates are for given amount of training data,\nn.3.3.Multiple Histograms to Represent Multiple Attributes of Visual Appearance\nThe main drawback of a histogram is we can only use a relatively small number of discrete val-\nues to describe appearance.   For example, if we were to directly represent the appearance as the\nset of pixel values over a region, say just a small 8x8 region, there would be 256\n8 ~ 1019bins inthe histogram.  It is impossible to represent a histogram this large in computer memory.  In gen-\n\neral histogram size will be limited by a combination of memory and limited amounts of training\ndata.For the sake of argument, let us say that histogram size is limited to 100,000.  With this size we\nare effectively limited to a 16.6 bit representation of appearance since 2\n16.6 ~ 100,000.  Clearly,\n16.6 bits of information is not adequate for the purposes of object detection.  We would like a\n\nricher description of appearance.  To do so, we will extract\nmultiple attributes\nfrom the image andcompute their histograms separately.  Each attribute will represent something different about the\n\nimage, for example, low frequency information over large spatial extents, high frequency infor-\n\nmation over small extents, etc.  We will dene these later in this chapter.\nFirst, let us see how this strategy can improve our representation of local appearance beyond\n16.6 bits.  For the sake of illustration, let us consider the representation of an 8x8 region,\nX.  Ifeach pixel is 8 bits, the total region requires 512 bits for exact representation.\nLet us assume a maximum of 100,000 total histogram bins.  As mentioned above, if we use one\nvisual attribute, we are effectively representing only 16.6 bits of the image:\npmlkn---=Epp\nml()2[]p1p()n---------------------='b'18(8)where we use the variable\npatternr to denote the range of discrete values:\nr1, . . . r100,000.The representation of probability then becomes:(9)However, if we use two attributes each represented by 50,000 discrete values, then we are effec-\ntively representing the region with a total of 31.2 bits:\n(10)Our representation of probability then becomes:(11)where we have assumed statistical independence between the\npattern1 and\npattern2. We will\nexplore this assumption in more detail in Chapter 4.\nOf course, we could use more attributes, each with fewer discrete values:\nAppearanceAttribute #1Attribute #2Attribute #M\n. . .Figure 10.Decomposition of appearance into multiple attributes\npatternrQuantizerX()=pattern1r1r2. . .r100000\n,,,,\n()~16.6bits\nPX()Prpatternr()patternpQuantizepX()=patternpp1p2. . .p50000\n,,,,\n()~15.6bits\npatternqQuantizeqX()=patternqq1q2. . .q50000\n,,,,\n()~15.6bits\nPX()Pppatternp()Pqpatternq()'b'19(12)Our representation of probability then becomes:(13)In the extreme, we could even represent the region using 50,000 binary-valued attributes. Such a\nrepresentation could be thought of as a large decision tree.\nAs we can see, the number of bits used to represent appearance can be increased as we decom-pose our representation using more attributes.  However, the visual representation cannot be effec-\n\ntively increased beyond 512 bits, since thats how many bits are in the original representation of\n\nthe region.\nWhile this strategy improves the delity of the visual representation, the larger goal is to\nimprove the accuracy of the probability, given by equation (9). We would expect that equa-\ntions  (11) and (13) can improve the accuracy over equation (9).  However, many questions\nremain.  How do we decompose appearance into different attributes?  How many attributes do we\n\nneed? And how many discrete values do we use for each attribute?  There are no conclusive\n\nanswers to these questions.  In the next sections we provide some provisional answers.  We begin\n\nby discussing how we decompose appearance into different attributes.\n3.4.Parts-based Decomposition of Appearance\nIn this section, we describe the different ways we decompose appearance to form different\nvisual attributes.  In particular, we decompose the image in frequency, orientation, space, and\n\ngeometry.\n3.4.1. Frequency/Scale Decomposition\nIn general, we would like to base our representation on visual attributes that are suited to the\npatterns,1Quantizes,1X()=patterns,1s11,s12,. . .s1m,,,,\n()patterns,nQuantizes,nX()=patterns,nsn1,. . .snm,,,(). . .patterns,2Quantizes,2X()=patterns,2s21,s22,. . .s2m,,,,\n()PX()Ps,kpatterns,k()k1=nPX()'b'20scale of the features on the object.  If we base a representation solely on attributes that are too\nsmall -- say, one pixel at the extreme -- they will not be powerful enough to describe anything dis-\n\ntinctive about the object.  (Although there is a method for frontal face detection that works sur-\n\nprisingly well using such a pixel-based representation [38]).  On the other hand, we do not want a\n\nrepresentation that only describes the object over large spatial extents.  First, large attributes are\n\nnot a desirable allocation of modeling resources.  They spread the limited representational power\n\nover the entire object. This is true of any representation that reduces dimensionality in some way.\n\nFor example, if we look at the most signicant eigenimages of a human face, we will notice a lack\n\nof detail for small characteristics, such as the eyes and the nose. Second, a large template is\n\nknown to be sensitive to small differences in scale, position, and orientation [55].  Finally, the\n\nmatching of large regions may be strongly inuenced by irrelevant pixels.  On many objects,\n\nsuch as a car, there will be large indistinctive areas such as the hood and windshield that are punc-\n\ntuated by relatively smaller areas of distinctive detailing such as the headlights and grill.  In\n\nmatching a large region, the majority of the pixels will come from untextured parts and dominate\n\nselection of the match (using any norm that weighs each pixel equally such as L1 or L2).   This\n\nmay be one of the reasons, some researchers have tried to improve the eigenimage approach\n\n[5][30] by using templates of smaller size to describe specic features [35][34].Since important cues will exist at many sizes, the best solution is to describe the object over a\nrange of scales.  In the very least, we will need attributes to describe large areas, mid-sized areas,\n\nand small areas. Such a representation may seem redundant.  Small areas exist as part of large\n\nones.  However, we can dene these attributes to be completely independent, if we also divide up\n\nthe frequency content between them.  Since low frequencies only exist over large areas and high\n\nfrequencies can exist over small areas, the most natural decomposition is to use some attributes to\n\nrepresent low frequencies over large spatial extents, attributes  to represent mid-range frequencies\n\nusing mid-sized attributes, and attributes to represent high frequencies over small spatial extents\n\nas illustrated by Figure 11.  The attributes describing small areas can do so at high resolution.  On a human face, they will\nbe important for representing features such as the eyes, nose, and mouth.  Similarly on a car, small\n\nattributes are needed for the grill, headlights, tires, and various lines and boundaries.\n'b'21Attributes computed over larger areas will be able to capture many other types of features. On\na face, the eye sockets are usually darker than the forehead.  On a car, various surfaces such as the\n\nhood, windshield, and fenders may differ in average intensity.  By having these attributes special-\n\nized for low frequency content, we reduce their sensitivity to misalignment.\n3.4.2. Orientation DecompositionIn addition to having attributes that are specialized in frequency content, we also specialize\nsome attributes in orientation content.  For example, an attribute that is specialized to horizontal\n\nfeatures can devote greater representation power to horizontal features than if it also had to\n\ndescribe vertical features.\n3.4.3. Generalized DecompositionFinally, important cues may not be strictly localized at one orientation or in one frequency\nband.  Some cues may exist over a wide range of frequencies.  For example, edges involve all fre-\n\nquencies.  Similarly, not all cues exist at one orientation.  Corners involve both horizontal and ver-\n\ntical edges.  Therefore, we dene additional attributes that span wider ranges of frequencies and/\n\nor orientations.  The trade-off is that these attributes will be limited to smaller spatial extents.\n3.4.4. Spatial DecompositionSince each of the attributes will have a limited spatial extent we will have to spatially decom-\nfrequency\nspatial extentAttributes with large extent\nAttributes with mid-sized extent\nAttributes with small extent\nFigure 11.  Decomposition of appearance in terms of spatial extent and frequency range\n'b'22pose the object  We can decompose it in many ways.  We could sample a few regions that seem\ndistinctive to us or that we determine experimentally as shown in Figure 12a. Alternatively, we\n\ncould sample at regular intervals over the full extent of the object.  For instance, we could choose\n\nsampling intervals to achieve critical sampling as shown in Figure 12b.\nInstead, our approach is to oversample each attribute allowing these samples to partly overlap\nas shown in Figure 12c.   We do this for several reasons.  First, while salient features, such as the\n\neyes, nose, and mouth on a face, are probably the most important for detection, other important\n\nfeatures may exist, too.  By sampling everywhere, we include anything that might contribute to\n\nthe detection decision. We also improve our detection decision by using more measurements.\n\nThis idea is similar to using more data to improve the accuracy of a statistical estimate [56].\nSampling with overlap may appear to lead to greater computational cost for evaluating the\nattributes.  However, it does not.  If we were to sample the object more sparsely, we would still\n\nend up densely sampling attribute values in each the input image.   The reason for this is that we\n\nhave to search the input image at all possible object locations, including overlapping ones.  For\n\neach of these locations we collect a different set of attribute values.  Since we densely sample\n\nthese object locations we will end up with a dense sampling of attribute values, as well.\n3.4.5. Geometric DecompositionBy decomposing and sampling the object spatially, we lose the ability to represent all the rela-\ntionships between the various parts.  We believe that the spatial relationships of these various\n\nparts is a very important cue for detection.  On the human face, the eyes, nose, and mouth appear\na. Selective sampling\nb. Critical sampling\n c. Sampling with overlap\nFigure 12. Examples of sampling strategies\n'b'23in a xed geometric conguration.  This arrangement of these features is a distinctive characteris-\ntic of faces.  If this geometric information becomes scrambled, the face becomes unintelligible as\n\nshown in Figure 13.\nTo model geometry, we represent the positions of each sample with respect to a coordinate\nframe afxed to the object as shown in Figure 14.   This representation captures each samples rel-\n\native position with respect to all the others and implicitly captures many geometric properties\n\n[57].  For each attribute, we then represent the joint probability of appearance and position for\n\nboth the object and the rest of the world:\nP(pattern(x,y), x, y| object) andP(pattern(x,y), x, y| non-object) wherepattern(x, y) is the discrete value of the specied attribute sampled at position\nx, y.However, there is no need to represent position at the original resolution of the image.  Instead,\nwe represent position at a coarser resolution to save on modeling cost and to implicitly accommo-\n\ndate small variations in geometric arrangements of the parts of an object. The functions\ni(x) andj(y) indicate this reduction in spatial resolution, where r will be either 4, 8, 16, depending on thespecic attribute:\n(14)3.5.Implementation of Visual Attributes\nIn this section we describe the actual attributes we compute based on the image decomposition\nFigure 13. Face with geometric relationships scrambled\nix()xr--=jy()yr--='b'24described in the last section.  We choose each attribute to represent a subset of quantized wavelet\ncoefcients.   Overall, we use 17 different attributes with variation in spatial extent, frequency\n\nrange,  orientation, and quantization.3.5.1. Wavelet Transform\nTo form different attributes, we would like to decompose the image in the ways described in\nthe last section.  In particular, we would like a representation jointly localized in space, frequency\n\nand orientation.  To do so, we perform a wavelet transform on the image.  The wavelet transform\n\norganizes the image into subbands that are localized in orientation and frequency.  Within each\n\nsubband, each coefcient is spatially localized as well.\nThe wavelet transform is not the only method that can produce a representation that is jointly\nlocalized in space, frequency, and orientation.  Both the short-term Fourier transform (also known\n\nas Gabor wavelets) and pyramid algorithms can create representations that are jointly localized\n\nin space, frequency and orientation.  Wavelets, however, have no redundancy.  The number of\n\ntransform coefcients is equal to the original number of pixels in the image.  These other multi-\n\nresolution decompositions produce transforms that are larger than the original image.\nWe use a three level transform with a biorthogonal 5/3 wavelet lterbank\n1 [58], producing 10subbands, as shown in Figures 15 and 16\n.Level 1 coefcients describe the lowest octave of fre-\n1.Low-pass coefcients: -0.1768, 0.3535, 1.0607, 0.3535, -0.1768. High-pass coefcients: 0.3535, -0.7071,\n0.3535.xyijFigure 14.  Coordinate frames afxed to object.\nCoarse resolution in attribute position in terms of\ni andj'b'25quencies.  Each subsequent level represents a higher octave of frequencies.  In terms of spatial\nextent, a coefcient in level 1 describes four times the area of a coefcient in level 2, which\n\ndescribes four times the area of a coefcient in level 3.  In Figure 17 we illustrate these relation-\n\nships.In terms of orientation, the LH bands are the result of low-pass ltering in the horizontal direc-\ntion and high pass ltering in the vertical direction giving horizontal features.  Similarly, HL rep-\n\nresents vertical features.\nLevel 3\nLHLevel 3\nHHLevel 3\nHLLevel 2\nLHLevel 2\nHLLevel 2\nHHL1HLL1LHL1HHL1LLFigure 15. Wavelet representation of an image\nFigure 16.  Images and their wavelet transforms.\nNote: the wavelet coefcients are each quantized to ve values.\n'b'26An important property of the 5/3 lter is that its high-pass lter has one vanishing moment.  An\nnth order vanishing moment means:\n(15)for high pass lter coefcients\nd(0) throughd(m-1).  A zeroth vanishing moment indicates that the\ncoefcients add to zero.  The lter is blind to constants and the DC level of the signal across the\n\nlters extent.  The output of the lter only represents changes in intensity.  Only level 1 LL repre-\n\nsents DC information since it is the result of low-pass ltering in both directions.  Many high-pass\n\nlters used for compression are designed to have a rst and 2nd vanishing moments also.  This\n\ncan be accomplished by using longer lters.  These high-pass lters are then blind to linear and\n\nquadratic changes in the image.  It remains an open question, whether there are any advantage to\n\ndoing this for detection.For a more complete description of the transform and the properties of a biorthogonal 5/3 lter\nbank see the Appendix of this chapter, Section 3.7, and [58][59].\nfrequency\nspatial extent(in one direction)level 1 coefcients\nlevel 2 coefcients\nlevel 3 coefcients\nFigure 17.  Properties of wavelet coefcients\njndj()j0=m10='b'273.5.2. Quantization of Wavelet Coefcients\nSince we plan to represent the statistics of each attribute using a histogram, we must quantize\nthe wavelet coefcients so they each take on a nite range of values.  In particular, we chose to\n\nlimit each attribute to 10,000 discrete values.  We made this choice out of practical considerations.\nTo avoid excessive storage and memory we would like to keep overall histogram size under 10\n6bins.  Position in x,y will take on order 10\n2 values, therefore, we limit attributes to 10\n4 values.\nThere are conicting goals in choosing a quantization resolution for the transform coefcients.\nWe need enough quantization levels to preserve the recognizability of the object.  If the image\n\nreconstructed from the quantized transform is unrecognizable to a human, it is unlikely to have\n\nenough information for computer recognition.  Yet we want to use as few quantization levels as\n\npossible.  By doing so, we can represent larger spatial extents with a fewer number of discrete val-\n\nues.First, let us explain how we represent the various subbands.  To begin with, we discard all HH\nbands.  This reduces the amount of information by almost a third while degrading appearance lit-\n\ntle. In Figure 18 we show images with and without HH subbands.\nWe represent the level 1 LL subband differently from the remaining subbands.  It is the result\nof successive low-pass lterings in both directions.  It represents a very low resolution copy of the\n\noriginal image.  All the other subbands are produced by a high pass ltering in one direction.\nSince the coefcients in the high pass lter sum to zero, its output represents differential measure-\n\nments.  To make the level 1 LL band consistent, we represent each coefcient in terms of its dif-\nFigure 18.  Original images on top, HH bands removed on bottom.\n'b'28ferences with its neighbors.  In particular, we generate two representations for each coefcient.  In\nthe vertical representation each coefcient is replaced by the difference between it and its right\n\nmost neighbor.  In the horizontal representation, each coefcient is replaced by the difference\n\nbetween it and its top neighbor.  This captures horizontal features.  These two representations\n\ncould be thought of analogous to a HL and LH band respectively, and we effectively treat them as\n\nsuch.We would like to quantize all coefcients to 5 levels.  Fewer than 5 levels were not sufcient\nfor preserving appearance, especially for cars.  In Figure 19 we show images reconstructed from\n\nthese quantized subbands for both 3 and 5 quantization levels:\nThe problem with 5 quantization levels is that it restricts us to attributes based on a very small\nnumber of coefcients.  6 coefcients per attribute would exceed our limit of 10,000 discrete val-\nues since 56 = 15,625.  To overcome this limitation, our strategy is to apply only 3 quantization\nlevels each time we sample a coefcient.  However, since each coefcient will be sampled multi-\n\nple times, (because of sampling with overlap and samples from multiple attributes) we can apply a\n\ndifferent set of quantization thresholds each time we sample.  This way, each coefcient can be\n\nevaluated with respect to 5 (or more) levels but as part of different samples.  More specically, to\nFigure 19.  Quantization of HL and LH bands:  Original images on top, 5 levels per coef-\ncient in middle, 3 levels per coefcient on bottom.  Note: level 1 LL band is not quantized.\n'b'29achieve 5 quantization levels we will need 4 quantization boundaries, {-q\n2, -q1, q1, q2}.  We break\nthese quantization boundaries into two sets: {-q\n1, q1} and (-q2, q2}.  We will then specify each\nattribute by a window of sampling sites with specied quantization thresholds.  For example, an\nattribute dened over a 2x2 neighborhood could be dened with the following quantization at\n\neach site:By doing so, a coefcient quantized with respect to {-q\n1, q1} for one sample will be quantizedwith respect to (-q2, q2} for another sample shifted by one.   With this strategy, we choose\nattributes computed over windows of 8 coefcients, where each attribute is described by 3\n8 =6,561 discrete values.\n3.5.3. Specication of Visual Attributes\nWe dene four broad types of visual attributes:\nintra-subband\n,inter-frequency\n, andinter-orien-\ntation. The rst type of attribute takes all 8 coefcients from one subband.  We call these\nintra-\nsubband attributes.  These attributes are the most localized in frequency and orientation.\nInter-\nfrequency\n attributes use coefcients from multiple frequency bands but the same orientation band.\nInter-orientation\n attributes use coefcients from both orientations, LH and HL, but from within\nthe same frequency band.  Finally, an\ninter-orientation\n /inter-frequency\n attribute has no restric-\ntions on which the bands from which it draws is coefcients [61].  In Figure 21 we illustrate these\n\ndifferences in the support of each type of attribute.\nOur assumption is that all of these relationships are important.  Some visual cues will onlyinvolve coefcients with a subband, others involve coefcients over a range of frequencies, some\n\ninvolve both horizontal and vertical components, and some as relations across both frequency and\n\norientation.  Our provisional modeling choice was to give these relationships approximately equal\n\nrepresentational power.  With some slight exceptions, we choose to represent each transform coef-\n-q1,q1-q1,q1-q2,q2-q2,q2Figure 20. Quantization levels assigned to a 2x2 neighborhood\n'b'30cient in one intra-subband attribute, one inter-orientation attribute, at least one inter-frequency\nattribute, and one inter-orientation / inter-frequency attribute.\nOverall, we used total of 17 attributes for each face detector and 13 attributes for each car\ndetector. We also experimented with 33 attributes for two of the car detectors, where the 20 addi-\n\ntional attributes represented long, straight and narrow spatial extents.  These additional attributes\n\nimproved performance, but the memory requirements were too large to do further development.\n\nBelow we describe the 17 attributes we used for face detection including the 13 attributes we used\n\nfor car detection. (We sample each of these attributes at small step sizes across the extent of the\n\nimage, i.e, wavelet transform)\n 6 intra-subband attributes.  These are taken from the following subbands as listed below in\nTable 2.  Each is computed from a set of 8 coefcients, a 3x3 block with one coefcient removed.\n\nIn Figure 22 we show the shape of this window and for each coefcient we indicate the quantiza-\n\ntion boundaries it uses (similar to diagram in Figure 20).Table 2: Intra-subband Attributes\nSubbandLevel 1 LHfaces, cars\nLevel 1 HLfaces, cars\n\nLevel 2 LHfaces, cars\n\nLevel 2 HLfaces, cars\n\nLevel 3 LHfaces, cars\nIntra-subbandInter-frequency\nInter-orientationInter-frequency\nInter-orientation\nFigure 21. Regions of support for each type of attribute\n'b'31 4 inter-orientation attributes. These are taken from the following subbands as listed below in\nTable 3.  Each of these is computed from spatially corresponding 2x2 blocks in the two subbands\n\nas shown in Figure 23\nLevel 3 HLfaces, cars\nTable 3: Inter-orientation subbands\nSubbandsLevel 1 LL (horizontal)\nLevel 1 LL (vertical)\nfaces\nLevel 1 LH, Level 1 HLfaces, cars\nLevel 2 LH, Level 2 HLfaces, cars\n\nLevel 3 LH, Level 3 HLfaces, cars\nTable 2: Intra-subband Attributes\nSubband-q2,q2-q1,q1-q2,q2-q1,q1-q2,q2-q1,q1-q2,q2-q1,q1Figure 22. Region of support and quantization for intra-subband attributes\n-q1,q1-q2,q2-q2,q2-q1,q1-q1,q1-q2,q2-q2,q2-q1,q1Figure 23. Quantization for inter-orientation subbands\nLH orLL horizontalHL orLL vertical\n'b'32 6 inter-frequency attributes as given in Table 4\n 1 inter-orientation / inter-frequency attribute for faces only involving 1 LL-horizontal, level 1\nLL-vertical, level 1 LH , level 1HL, level 2 LH, level 2 HL.\nWith this representation, the attributes that use level 1 coefcients will cover the largest spatial\nextents and the lowest and smallest range of frequencies.  Conversely, the attributes that use level\n\n3 coefcients will cover the smallest spatial extents and the highest (and largest) range of frequen-\n\ncies.  The attributes that use level 2 coefcients represent attributes that are intermediate in spatial\n\nextent and frequency range.  As we will see in Chapter 6, this representation allows us to search\n\nfor objects using a coarse to ne heuristic strategy.  For example, we can evaluate the image using\n\nonly attributes that consist of level 1 coefcients.  Based on the output probabilities from these\n\nattributes we can decide how to evaluate the rest of the image.  We can terminate the search at\n\nthose regions that have low probability and continue searching for the object in those regions that\n\nhave higher probability.\n3.6.Overall Decision Rule and Summary\nBy combining all the representational decisions in this chapter we can write the overall func-\ntional form of our decision rule as:Table 4: Inter-frequency subbands\nsubbandsLevel 1 LL (horizontal),\nLevel 1 LH\nfaces\nLevel 1 LH, Level 2 LHfaces, cars\nLevel 2 LH, Level 3 LHfaces, cars\nLevel 1 LL (vertical),\nLevel 1 HL\nfaces\nLevel 1 HL, Level 2 HLfaces, cars\nLevel 2 HL, Level 3 HLfaces, cars\n'b'33(16)Let us explain how we combine the various ideas in this chapter to get this form:\n1.  Object and Non-Object probability - As described in Section 3.1, we write the probabilityfunction of the object in the numerator and the probability function of the non-object in the\ndenominator.\n2.  Histograms - We represent\nPk(patternk(x,y), i(x), j(y)| object) and\nPk(patternk(x,y), i(x), j(y)|non-object), as histograms.  Each of these histograms represents the joint statistics of appearance,\ngiven by\npatternk, and position on the object, given by\nx, y.  We compute multiple histograms over\n17 discrete-valued visual attributes,\npatternk.3.  Visual attributes - Each visual attribute,\npatternk, represents the values of a subset of quantized\nwavelet coefcients.  We use four types of visual attributes:\na.  Intra-subband - Coefcient subset is localized in frequency, orientation and space.\n\nb. Inter-frequency - Coefcient subset is localized in orientation and space only.\n\nc. Inter-orientation - Coefcient subset localized in frequency and space only.\n\nd. Inter-orientation / inter-frequency - Coefcient subset localized only in space.\n4.  Sampling with overlap - We sample each attribute at regularly spaced overlapping intervals\nwithin each candidate object region.\n5. Multiplication of probabilities - In Chapter 4 we will see why we multiply the probabilities cor-\nresponding to different observations.  In particular, we will see how doing so corresponds to an\n\nassumption of statistical independence of the observations.\n6. Threshold - We have not explained how to set the threshold,\nl.  In practice we will select itPimage\nobject()Pimage\nnon-object()--------------------------------------------------------Pkpatternkxy,()ix()jy(),,object()Pkpatternkxy,()ix()jy(),,non-object()--------------------------------------------------------------------------------------------------------\nxyregion,k17l>'b'34empirically.  We can, however, interpret it as the ratio of the prior probabilities, if the probability\nmodels are accurate (which they are not) as we will also see in Chapter 4.\n3.7.Appendix: Wavelet Transform\nThe wavelet transform decomposes an image into subbands that are localized in frequency\nand orientation.  A wavelet transform is created by passing the image through a series of lter\n\nbank stages.  In Figure 24 we show one stage in such a series.  In each stage we lter the image\n\nrst in the horizontal direction.  We use a one-dimensional high-pass and low-pass lter pair.  The\n\nhigh-pass lter (wavelet function) and the low-pass lter (scaling function) are nite impulse\n\nresponse lters.  In other words, the output at each point depends only on a nite portion of the\n\ninput. The ltered outputs are then downsampled by a factor of 2 in the horizontal direction.\n\nThese signals are then each ltered by an identical lter pair in the vertical direction.  We end up\n\nwith a decomposition of the image into 4 subbands denoted by LL, HL, LH, HH.  Each of these\nsubbands can be thought of as a smaller version of the image representing different image proper-\n\nties.  The LL (low-pass in both horizontal and vertical) is simply a low-frequency, low-resolution\n\nversion of the original. The LH (high-pass ltering in the vertical direction and low-pass ltering\n\nin the horizontal) subband represents vertical features, such as lines and edges.  Similarly, HL rep-\n\nresents horizontal features.  And, to some extent HH represents diagonal features.\nThese four subbands could be thought of as one frequency band or one level in a wavelet trans-\nform.  To further decompose the image in frequency, we iterate on the LL band; that is we decom-\n\npose the LL band just the same way we decomposed the original image by sending it through\n\nanother stage of ltering identical to the rst.  We can do this multiple times with each new LL\n\nband to generate multiple levels in the decomposition.  In our representation we use a 3 stage lter\n\nbank generating a a 3-level decomposition in frequency.\nIf the high pass - low pass lter pair is chosen properly, the original image can be reconstructed\nfrom its transform with no loss of information.  Such lter banks are called perfect reconstruction\n\nlter banks.  Several books describe their design [58][59].  There are several lter pairs we can\n\nchoose that will give perfect reconstruction.  We can choose orthogonal lters of various lengths:\n\nDaubechies 4, Daubechies 6, Daubechies 8 or we could choose linear phase of various lengths 3\n'b'35(low-pass)/5(high-pass), 7/9, FBI 7/9, 11/13, etc.  It is not possible to choose lters that are both\northogonal and linear phase except for the trivial case of Haar wavelets.  We choose symmetric 5/\n\n3 mainly because it served several practical needs.  By having linear phase there is no ambiguity\n\nabout the alignment of the coefcients in successive levels of the decomposition. We choose l-\n\nters of short length, 5/3 rather than 7/9 or 11/13 because we wanted to capture localized informa-\n\ntion.  Remember, each coefcient describes a localized region in space, frequency and orientation.\n\nAlso, of course, smaller lters have less computational cost.  Finally, there is also some evidence\n\nthat human vision is less sensitive to errors with symmetric lters (i.e. linear phase) [60], but there\n\nhas not been rigorous verication of this.\nImageHigh-passLow-pass\n22High-passLow-pass\n22High-passLow-pass\n22horizontal lteringvertical ltering\nHHHLLHLLFigure 24.  A lter bank stage'b'36Chapter 4.Functional Form of Detector:\nRe-derivation from an Ideal Form\nWe can also view the functional form of our detector as representing our best attempt at\napproximating to an ideal functional form within our computational constraints.  In this chapter,\n\nwe re-derive our functional form through a series of approximations to an ideal functional form.\n\nBy deriving our decision rule this way we get a clear picture of the functional forms representa-\n\ntional capacity and its deciencies.  In particular, we have a complete record of all the transforma-\n\ntions and simplications that limit its representational power.  Also, through this analysis, we gain\n\na better understanding of several issues that were not apparent in the rst derivation in Chapter 3.\n\nThis derivation reveals why it is useful to select non-object samples by bootstrapping and why we\n\nshould train the detector to explicitly reduce the classication error on the training set. Also, we\n\nsee why we divide object probability by non-object probability (in section 3.1) -- a consequence\n\nof Bayes decision rule.  Similarly, we see how multiplying the probabilities from different\n\nattributes (Section 3.6) corresponds to an assumption of statistical independence of the observa-\n\ntions.4.1.Ideal Functional Form of Detector\nIn Chapter 1 we introduced an ideal classier as a large table as shown in Table 5 below.   This\nclassier is ideal in several ways.  First, it based on a full representation of the data.  The classier\n\nuses the entire raw input, not a selected or ltered portion of it.  Second, it minimizes the proba-\n\nbility of error, assuming each entry in the table is labeled with its most probable classication.\n\nFinally, it is concise representation of the output. For each input, we simply represent its classi-\n\ncation and nothing else.  Of course, such a table is not feasible.  It is not possible to enumerateevery possible input in a table.  There are too many; we would need a table with 256\n400 entries for\nclassication of a 20x20 region where each pixel takes on 256 values.\nAlthough this decision rule is intractable, it provides a useful starting point for deriving a feasi-\nble decision rule.  In our case, the functional form of our decision rule, equation (17), can be'b'37derived by a series of two generalizations (Section 4.2), a wavelet transform (Section 4.3), and\nthree simplications (Section 4.4), as we will explain below.\n(17)4.2.Generalizations to Ideal Functional Form\nWe immediately notice several differences between equation (17) and the ideal model, Table 5.\nFirst, in the ideal classier, each entry in the table states a classication, object or not-object.  On\n\nthe other hand, our classier outputs a continuous value which it compares to a threshold.  Also,\n\nour classier separately represents object and non-object properties, whereas the table does not.\nThus, to transform the table into equation (17), we must rst make it more general in these ways.\n\nThese generalizations turn out to have important implications for the training of our detector, as\n\nwe will show.\nFirst, we generalize the output of this table from binary values (object, non-object) to posterior\nprobabilities, as shown in Table 6. The posterior probability,\nP(object| image\n), states the proba-bility that object is present for a given input.  The probability that the object is not in the image is\n\nsimply the complement of the posterior probability,\nP(non-object| image\n) = 1.0 -P(object|image\n). To make the best decision, we always choose the classication, object or non-object,\nwhich has the higher probability.  Equivalently, we decide that the object is present in the input\nTable 5: Ideal but infeasible classier\n(1,1)(1,2). . .(20,20)classication\n00. . .0Non-object\n00. . .1Non-object\n\n. . .. . .. . .. . .. . .\n\n3545. . .28Object\n\n. . .. . .. . .. . .. . .\n\n255255. . .255Non-object\nPkpatternkxy,()ix()jy(),,object()Pkpatternkxy,()ix()jy(),,non-object()--------------------------------------------------------------------------------------------------------\nxyregion,k17l>25620x20'b'38image if:(18)This decision rule is known by the names of Bayes decision rule and the maximum\na posteriori(MAP) decision rule.Now to generalize the decision rule further, we substitute Bayes theorem for\nP(object| image\n)in equation (18):(19)Through some algebra we can re-write the decision rule as a likelihood ratio test:\n(20)  The left side of this equation is called the likelihood ratio.  It divides object probability by non-\nobject probability.  If the ratio of these two probabilities is greater than the ratio of the prior prob-\n\nabilities, the right side of the equation, then we decide that the object is present. In equation (3) in\nChapter 3 we divide object probability by non-object probability as a direct consequence of this\nTable 6: Ideal classier using posterior probabilities\n(1,1)(1,2). . .(20,20)\nP(Object | Image\n)00. . .00.000001\n00. . .10.000003\n\n. . .. . .. . .. . .. . .\n\n3545. . .280.87521\n\n. . .. . .. . .. . .. . .\n\n255255. . .2550.00004\nPobjectimage()0.5>Pimage\nobject()Pobject()Pimage\nobject()Pobject()Pimage\nnon-object()Pnon-object()+----------------------------------------------------------------------------------------------------------------------------------------------------------------0.5>Pimage\nobject()Pimage\nnon-object()-----------------------------------------------------Pnon-object()Pobject()----------------------------------\n>l='b'39equation.There are two advantages to writing the Bayes decision rule as a likelihood ratio test as given\nby equation (20).  First, it is often easier to separately collect statistics for the two probability\n\nfunctions,P(image |\nobject) andP(image |\nnon-object), than it is to directly collect statistics forthe posterior probability function.  Also, by expressing the decision rule this way, we factor out\n\nthe contribution of the prior probabilities and combine them into a scalar threshold,\nl.  By chang-ing this threshold we can apply the algorithm to different settings in which the prior probabilities\n\nvary.\nTo use such a representation, we must build a table that stores two values for each input,\nP(image |\nobject) and\nP(image |\nnon-object), as shown below in Table 7.   There are several conse-\nquences of using a decision rule derived from this generalized form instead of one directly derived\nfrom the concise form in Table 5.  The generalized form will work just as well if our probability\n\nestimates are accurate representations of the true probabilities.  However, problems arise if there\n\nis some inaccuracy in the modeled probabilities.  These errors in the probability will be propa-\n\ngated and lead to errors in our classication decision.  The question is: will our probability models\n\nforP(image |\nobject) andP(image |\nnon-object) be accurate?  Unfortunately, the answer is no.\nFirst, there will be error in our estimates themselves because they are computed from nite train-\n\ning data.  In particular, we would need an extremely broad sample of imagery to get an accurate\n\nestimate ofP(image |\nnon-object).  An even greater source of error, though, is that our functional\nTable 7: Ideal classier using separate models for object and non-object probabilities\n(1,1)(1,2). . .(20,20)\nP(Image |\nObject),P(Image |\nNon-object)00. . .00.00000013, 0.013\n00. . .10.00000032, 0.014\n\n. . .. . .. . .. . .. . .\n\n3545. . .280.0092, 0.00045\n\n. . .. . .. . .. . .. . .\n\n255255. . .2550.00007, 0.03\n'b'40form is derived through incorrect assumptions about the nature of the true distribution.  As we\nwill see, in Section 4.4, there are several simplications we will make to equation (20) in order to\n\nachieve a computationally feasible decision.  It is unavoidable that some of these simplications\n\nwill be inaccurate or partially inaccurate.We can improve our detection performance if we bypass the intermediate estimates of the\nprobability functions and instead train our detector to directly minimize the classication error.\n\nThere will be errors in our estimate of the classication function, too, but it is better to estimate\n\nthis directly rather than combining two sources of error from intermediate estimates.  We will\n\nlater describe how we train the detector to do so using the AdaBoost algorithm in Chapter 5.\nThe fact that our end goal is classication, not probabilistic modeling, can also guide us in\nanswering an important question: how do we select training data for the\nnon-object class?  Should\nwe use any imagery that is not of the object or should we be more selective? Let us consider a\n\nhypothetical situation.  In Figure 25, we plot some hypothetical probability functions,\nP(x |object) andP(x |non-object), for a one-dimensional classication problem in terms of a feature,x.We also plot the classication function derived from these probabilities.   For input values of\nx inwhich one class is overwhelmingly favored over the other, a qualitatively accurate representation\n\nof their probabilities is sufcient to be accurate in classication.  It is not important to represent\n\nall the little bumps and ripples indicated by A, B, and C in Figure 25.  However, for values of x\n\nwhere there is not a clear difference between\nP(x |object) andP(x |non-object), such as thoseindicated by D, it is more important to represent the ratio of the probabilities accurately.  If we\n\nrepresent the probabilities accurately in this region, the classication boundary will be as precise\n\nas possible[15][27].  Therefore, in selecting samples for the non-object class, it is more important\n\nto select samples that are more likely to be confused with the object; that is, those close to the\n\ndecision boundary.  (This concept of selecting samples near the decision boundary is similar to the\n\nway support vector machines (SVMs) [15][24] work.) In Chapter 5 we will explain how we do\n\nthis using bootstrapping.  Of course, it is still necessary to have a broad sampling of the non-\n\nobject class, so we will be qualitatively correct in other parts of the input space.\n'b'41P(x |non-object)xP(x |object)xxClassicationnon-objectobjectFigure 25.  Probability functions and classication function for a hypothetical\nclassication problem (adapted from [27])ABCDD'b'424.3.Wavelet Transform of Image\nAs we described in Section 3.5.1, we perform a wavelet transform of the image.  It terms of\nrepresentational capacity, this transformation has no consequences.  The wavelet transformation is\n\nfully invertible, so no information is lost.  We can represent this transform mathematically as:\n(21)whereW represents the wavelet transform of the image.\n4.4.Simplications to Functional Form\nWe make three major simplications to equation (21) in order to get a computationally feasible\nform for the decision rule.4.4.1. Statistical IndependenceAs we have mentioned, computer power and memory limit us to only representing the joint\nprobability of small subsets of wavelet coefcients.  We do not model the statistical dependency\n\noutside these subsets. By doing so we transform equation (21) in the following way:\n(22)whereW = (w1, w2, . . ., wn) represents the wavelet transform of the image, and\nx andy indicatethe spatial positions of the group of coefcients with respect to a coordinate frame afxed to the\nobject.  (Note: In the representation above, each coefcient is shown as part of only one subset.  In\n\npractice, we will use each coefcient as part of multiple subsets; that is we will model each coef-\n\ncients joint statistics with several different groups of coefcients.  Each of these subsets will cor-\n\nrespond to a different visual attribute modeling statistical dependency over different frequency\n\nbands, orientations, or spatial extents (see Section 3.5).)  By making this simplication, we\nPimage\nobject()Pimage\nnon-object()-----------------------------------------------------PWWTimage()=object\n()PWWTimage()=non-object\n()--------------------------------------------------------------------------------=PWobject()PWnon-object()------------------------------------------Pw1. . .w8xy,,,,\nobject()Pw9. . .w16xy,,,,\nobject(). . .Pw1. . .w8xy,,,,\nnon-object()Pw9. . .w16xy,,,,\nnon-object(). . .----------------------------------------------------------------------------------------------------------------------------------------------------------------. . .Pwn7. . .wnxy,,,,\nobject(). . .Pwn7. . .wnxy,,,,\nnon-object()-----------------------------------------------------------------------------------------'b'43implicitly assume statistical independence between the sets of coefcients.  This assumption is the\nreason we multiplied the probabilities in the overall functional form of our decision rule, equation\n\n(16) in Section 3.6.This statistical independence simplication limits our representational power.  In general, we\ncannot represent relationships among coefcients that are not grouped together into a subset.  This\n\nincludes aspects of appearance that are similar across the full extent of the object, such as skin\n\ncolor or texture on a human face.  We cannot represent how the positions of the features systemat-\n\nically vary when viewing perspective changes.  We cannot represent long-range symmetries\n\nacross the object, such as the horizontal symmetry of the human face and cars, where features on\n\none side are strongly correlated with features on the other side, such as the eyes and ears on a\n\nhuman face.  We cannot represent the interaction between the lighting and the 3D geometry, i.e.,\n\ndepending on the position of the light source, some parts of the object will be brighter than others.\nThis relationship changes in a systematic fashion as light source position changes.\nWe can analyze this simplication from several other perspectives.  If we take the log of the\nleft side of equation (22), it becomes a sum of log probabilities:(23)In this form, we can interpret the classier as a linear discriminator where the log probabilities areweights and the features are binary-valued vectors indicating which patterns have been detected\n\n[64].  [17] documents the limitations of linear discriminators and, in particular, linear discrimina-\n\ntors cannot represent exclusive-or and parity type functions.  However, it is improbable that par-\n\nity-type functions play much of a role as cues for distinguishing objects.  They are the most\n\nunstable functions that are mathematically possible; the smallest possible pertubation in the input\nwill lead to a maximum change in the output.We can also view this equation as a simple or naive Bayes classier [63].  It can be shown\nPw1. . .w8xy,,,,\nobject(). . .Pwn7. . .wnxy,,,,\nobject()Pw1. . .w8xy,,,,\nnon-object(). . .Pwn7. . .wnxy,,,,\nnon-object()--------------------------------------------------------------------------------------------------------------------------------------------------------------------\nlog=\nPw1. . .w8xy,,,,\nobject()Pw1. . .w8xy,,,,\nnon-object()---------------------------------------------------------------------------log. . .\nPwn7. . .wnxy,,,,\nobject()Pwn7. . .wnxy,,,,\nnon-object()---------------------------------------------------------------------------------\nlog++'b'44that such a model cannot representk of\nn type problems [64].  Similar to parity functions, this type\nof relationship probably does not play an important role in distinguishing objects.Nevertheless, it has also been demonstrated that the naive Bayes classier performs well in a\nnumber of classication problems, even when there is signicant statistical dependency among\n\nthe attributes.  In particular, it has been shown that it performs equivalent to or better than other\n\nconcept learners [64].  There is not a full theoretical understanding of why this is true.  Although\n\n[64] shows that the classier is optimal for certain problems in which statistical independence\n\ndoes not hold such as conjunctions and disjunctions, they also show it can be optimal for other\n\ntypes of problems, too.Below we illustrate how classication can be accurate even when the assumption of statistical\nindependence is violated and the statistical representation is inaccurate.  Let us consider two ran-\n\ndom variables,\nx andy.  Lets say the\nx andy are statistically dependent for both object and non-object:(24)wheref(y) is a deterministic function.The correct decision rule is:(25)The decision rule under the (incorrect) assumption of statistical independence is:Pxfy\n()=yobject,()1=Pxfy\n()=ynon-object,()1=Pxy\nobject,()Pxy\nnon-object,()---------------------------------------------\nPxy\nobject,()Pyobject()Pxy\nnon-object,()Pynon-object()-------------------------------------------------------------------------------------\n=Pyobject()Pynon-object()---------------------------------------\nl>='b'45(26)However, if we choose\ng =l2 then the two decision rules become exactly the same even though\nwe have completely violated the assumption of statistical independence in the 2nd decision rule.\nIn the above example, the assumption of statistical independence worked partially because the\njoint behavior of\nx andy was the same for both the object and the non-object class.  In general,\nthis assumption will lead to errors ifP(x |y, object) andP(x |y, non-object) are very different func-\ntions.  This underscores the problem with naive Bayes classiers or any classier in general; that\n\nis, we will have errors when we do not model important relationships, such as that between\nx andy whenP(x |y, object) andP(x |y, non-object) are very different functions. No classier is\nimmune to this problem except the ideal model, Table 5, which is infeasible.  In a naive Bayes\n\nclassier, the relationships that are not modeled are very explicit.  In other models, they may be\n\nmore hidden (although if we can write such a classier as a sum or product, we can easily put it in\na form equivalent to a naive Bayes classier).  Thus, with any kind of classier, it is most impor-\n\ntant to represent the relationships that distinguish the two classes.  If we miss important relation-\n\nships, we may have classication errors.\n4.4.2. Quantization of Wavelet Coefcients\n  In quantizing the wavelet coefcients each subset of wavelet coefcients is replaced by a dis-\ncrete variable,\npattern, that takes on 6,561 possible values.  Below we factor equation (22) in\nterms of the 17 attributes we dened in Section 3.5.3 and write the attributes in terms of this dis-\n\ncrete valued variable:\nPxy\nobject,()Pxy\nnon-object,()---------------------------------------------\nPxobject()Pyobject()Pxnon-object()Pynon-object()-------------------------------------------------------------------------------=Pyobject()Pynon-object()---------------------------------------\n2g>='b'46(27)4.4.3. Reduced Resolution in Pattern Position\nWe also reduce the resolution of\nx,y.  For level 3 patterns, resolution is reduced by a total factor\nof 16 (4 in each direction).  For level 2, it also reduced by 16.  For level 1 it is reduced by a factor\n\nof 64 (8 in each direction).  We represent this reduction by the functions,\nik(x) andjk(y).(28)4.5.  Conclusion\nThe derivation of our functional form from an ideal form gives us another perspective on our\nmodeling choices.  We see that the main modeling deciencies are from not modeling the rela-\n\ntionships of all wavelet coefcients, by quantizing the values of the wavelet coefcients, and by\n\nrepresenting position of the wavelet coefcients at coarse resolution.  Therefore, assuming we\n\nhave adequate training data and an accurate estimate of the classication function, we can\n\nattribute any deciency in performance to these simplications.\nPkpatternkxy,()xy,,object()Pkpatternkxy,()xy,,non-object()------------------------------------------------------------------------------------------xyregion,k17l>Pkpatternkxy,()ikx()jky(),,object()Pkpatternkxy,()ikx()jky(),,non-object()-------------------------------------------------------------------------------------------------------------xyregion,k17l>'b'47Chapter 5.Training Detectors\nSo far we have only chosen the\nform of the decision rule; that is, we have specied the number\nof histograms, the size of each histogram and the variables over which we compute each histo-\n\ngram.  We have not specied the actual values within each histogram,\nPk(patternk(x,y), i(x), j(y)|object) andPk(patternk(x,y), i(x), j(y)| non-object) that are used in the decision rule.  We compute\nthese statistical values from various sets of images.   This process of gathering statistics is usually\nreferred to astraining\n.  Specically, we use a set of images of the object to generate samples for\ntrainingPk(patternk(x,y), i(x), j(y)| object) and we use images that do not contain the object to\ntrainPk(patternk(x,y), i(x), j(y)| non-object).  In this chapter, we begin with a discussion of our\ntraining images for faces and cars in Section 5.1.  In Section 5.2 we discuss the training images\nwe use for the non-object class.   Then in section 5.3, we describe a basic training algorithm in\nwhich we estimate each histogram separately then in Section 5.4 we describe an alternative train-\n\ning procedure which minimizes the classication error on the training set using the AdaBoost\nalgorithm.5.1.Images of the Object\nIn this section we describe the training examples we use for training\nPk(patternk(x,y), i(x), j(y)| object).  We describe where we collected our images, how we align and normalize each training\nimage, how we correct for lighting, and how we generate synthetic variations from each original\n\nimage.5.1.1. Collection of Original ImagesWe gathered face images from a number of sources including the following image collections:\nFERET1, NIST mug shot database2, Harvards face database\n3, and CMUs face collection\n4.  We\nalso used many images we found at a variety of sites on the World Wide Web.  Overall, we gath-\n1.Provided by Jonathon Phillips\n2.See http://www.nist.gov/srd/nistsd18.htm.\n\n3.Provided by Woodward Yang\n\n4.Provided by Henry Rowley and Shumeet Baluja\n'b'48ered about 2,000 images of frontal faces and 2,000 images of prole faces.\nWe collected car images using our own camera and from the World Wide Web, mostly from car\nsales sites and car enthusiast sites.  The latter sites were a good source of photographs of older\ncars.  Overall, we gathered between 250 and 500 images per viewpoint with a total of over 2,000\n\nimages.5.1.2. Size Normalization and Spatial AlignmentTo reduce the amount of variation among each set of training images, we aligned all the train-\ning images with respect to a prototype image.  We aligned the images using a set of pre-dened\n\nfeature points that we hand-labeled for each image.  Using these feature points, we applied the\ntranslation, scaling, and rotation [7] that brought each image into alignment with the prototype.Below we give the sizes and examples of the images we used for training each detector.  Notice\nthat each of these images contains some of the background.  We do so because the silhouette of\n\nthe object against the background is a strong cue for detection for many of these objects, particu-\n\nlarly for faces in prole.  In Section 5.1.4 we will discuss how we alter the background to generate\n\nmultiple samples from the same original image.Figure 26. Face frontal viewpoint.  Size = 56x48\nFigure 27. Face side viewpoint.  Size = 64x64\nFigure 32. Car viewpoint #5.  Size = 48x80\n'b'495.1.3. Intensity NormalizationFor faces, to also reduce the variation within each set of training images, we normalized the\nFigure 28. Car viewpoint #1.  Size = 56x72\nFigure 29. Car viewpoint #2.  Size = 48x80\nFigure 30. Car viewpoint #3.  Size = 56x72\nFigure 31. Car viewpoint #4.  Size = 40x88\nFigure 33. Car viewpoint #6.  Size = 40x104\nFigure 34. Car viewpoint #7.  Size = 40x96\nFigure 35. Car viewpoint #8.  Size = 32x96\n'b"50intensity of each image.  We normalized the left and right sides of each training image separately.\nFor each side we scale all the intensity values by a specied scalars,\ngleft andgright:(29)However, when we sample an attribute that extends across both sides of the object, we normalize\nthe entire sample using the value of\ng of the center pixel.  We choose these correction factors by\nhand for each training image.  However, for cars we did not normalize the training images.\nWe normalize the two sides of the face in the training images to compensate for situations in\nwhich the object is illuminated unevenly.  For example, a face could be brightly illuminated on\n\none side and in shadow on the other as shown below in Figure 36.\nWhen we detect the object, for both faces and cars, we evaluate each side of each candidate\nover a set of 5 discrete values for\ng.  We then choose the best response from each side and sum\nthem to give the total response for the candidate.\n5.1.4. Synthetic Variation\nWe will improve the accuracy of our object detectors by using more training data.  In particular\nthe accuracy of our histogram estimates will increase as we use more training examples (see equa-\n\ntion (7) in Section 3.2).To expand our training set, we generated synthetic variations of each original training image.\nDepending on the object, we generated between 1600-6400 synthetic variations through small\n\nvariations in position, orientation, size, aspect ratio, background scenery, lighting intensity, and\nI'xy,()g\nleftIxy\n,()=ifyyo<I'xy,()g\nrightIxy\n,()=ifyyoFigure 36.  Examples of uneven illumination\n"b'51frequency content.  Overall, this gave us at least 1,000,000 training examples for each object.  In\norder to substitute different background scenery, we segmented the objects from the background\n\nin the training images.  We segmented these images by hand and by automatic methods when pos-\n\nsible.  Also, in the synthetic variations, we modied frequency content by using low pass lters\n\nwith various responses.  Below we show several of the forms of synthetic variation we used on\n\neach original image:5.2.Non-Object Images\nWe collected these images from photography collections\n1 and from the World Wide Web, par-\nticularly the Washington University archive\n2.  We tried to get a balance of indoor and outdoor\nscenes, urban, industrial, and rural scenes.  Overall, we used over 2,000 such images.\nTo select non-object samples that resemble the object, we used bootstrapping [18][14].  Boot-\nstrapping is a two-stage process.  We rst trained a preliminary detector using image samples\n\ndrawn randomly from the non-object image collection.  We then ran this preliminary detection\n\nover the entire collection of non-object images.  We then selected samples from each of these\n\nimages where the detector gave a high response and we combined these samples with the original\n\nrandom samples.  For some detectors we repeated this process several times gathering more and\n1.Provided by John Krumm and Henry Rowley\n2.http://www.wuarchive.wustl.edu\nFigure 37.  Synthetic rotational variation\nFigure 38.  Synthetic variation in aspect ratio and zoom\nFigure 39. Synthetic variation in frequency content (low-pass ltering)\n'b'52more non-object samples.  We then used the nal combined set of samples for training the nal\ndetector.\n5.3.Training Method (I) - Probabilistic Approximation\nA set of 34 histograms makes up our decision rule:\nPk(patternk(x,y), i(x), j(y)| object) and\nPk(patternk(x,y), i(x), j(y)| non-object) for\nk = 1 . .  17.   To train our car detectors we estimated\neach histogram separately.   To estimate each histogram we simply counted how often each pat-\ntern occurs at each position in the appropriate set of training examples.  To estimate\nPk(pat-ternk(x,y), i(x), j(y)| object) we use images of the object, and to estimate\nPk(patternk(x,y), i(x), j(y)| non-object) we use images that do not contain the object.  Occasionally, a pattern and position\ncombination may not occur together in a training set.  The corresponding bin in the histogram will\n\nhave zero count denoting zero probability.  However, it may not be desirable to actually assign\n\nzero probability to this bin during testing, since its probably of occurrence usually will not be\nzero.  To correct for this situation, we use Laplace correction[8], where we add one to each bin in\n\nthe histogram.5.4.Training Method (II) - Minimization of Classication Error using AdaBoost\nTraining each histogram separately as described in Section 5.3, however, is not optimal for\nclassication.  To achieve better results, we should explicitly train our classier to minimize clas-\n\nsication error, as described in Chapter 4.   In this section, we describe how we estimate these his-\n\ntograms to minimize classication error using the AdaBoost algorithm.  In particular, we trained\n\nboth the frontal and prole detectors using this approach.AdaBoost [70][71][72][73]is a method for training a classier to have low classication error\non the training set.  Similarly, other methods [74] may work for this purpose as well. AdaBoost\n\ncan be applied to almost any classication algorithm.  It works by training multiple instances of\n\nthe classier.  Lets call each of these classiers\nht(x).   AdaBoost then takes a weighted sum of\nthese classiers as the nal classier:'b'53(30)AdaBoost trains each of the classiers,ht(x), by assigning different weighting to the training\nexamples.  For the rst classier,\nht(x), all training examples are given equal weight.  For all sub-\nsequent classiers,ht(x),t > 1, AdaBoost assigns more weight to training examples that have\nbeen incorrectly classied by the previous classier,\nht-1(x).  Conversely, it decreases the weight\nfor training examples that were correctly classied.\nIn using AdaBoost to train our detector we continue to collect each histogram by counting, but,\nwe increment each bin based on the weight assigned to the current training example.  We scale\n\nand round the training image weights to integers for this purpose.\nFollowing the development of [72], the algorithm works as follows.  Assume that we have a\nsequence ofm training examples {(\nxi, yi)}, wherexi is the input andyi = {-1, +1} is the desiredclass label.  Initialize the weights for each training example as\nD1(i) = 1/m.For\nt = 1. .T:1.  Train\nht(x) usingDt(i)2.  Computeat (we discuss how to compute it below)\n3.  Update:(31)whereZt is a normalization factor so that\nDt+1 will be a distribution:\nHx()signathtx()t1=T\n=Dt1+i()Dti()atyihtxi()()expZt----------------------------------------------------\n='b'54The nal classier is then given by equation (30) above.\nThis algorithm achieves the following bound on the training error:\n(32)To understand the power of this bound, let us consider a simple classier with output {-1, +1}\ninstead of a continuous value, as in our classier.  Then\n(33)whereui= yiht(xi).We can then choose\na as to minimize the right side of this equation:(34)Plugging this choice back into Z gives:\n(35)Since this is guaranteed to be less than one, we can see how the classication error can be\ndriven towards zero after a few iterations.\nDt1+i()i1=m1=PrHxi()yi()Ztt1=TZtDti()1ui+2-------------\neat1ui2-------------eat+i=at12---1rt+1rt-------------ln=rtDti()uii=Zt1rt2='b'55In our case, the classier,\nht(x), outputs a continuous value.  Therefore, we cannot determine\natanalytically to minimizeZt as we have done in equation (34).  Instead, we minimize\nZt numeri-cally in terms ofat, sinceZt is strictly quadratic (see [72] for proof).  In this form it is more dif-\ncult to put a bound on the error rate of the classier. However, we would expect the same behavior\nto hold.  For instance, [72] gives an upper bound on the error when the output of the classier is\n\ncontinuous but bounded.\nIn using AdaBoost to train our detector we continue to collect each histogram by counting, but,\nwe increment each bin based on the weight assigned to the current training example.  We scale\n\nand round the training image weights to integers for this purpose.  However, we train each histo-\n\ngram multiple times for multiple iterations of the algorithm.  For each training example we incre-\n\nment the appropriate bins by the weight assigned to the training example.\nIt should also be noted that forming the nal classier in equation (30) does not actuallyincrease the complexity of the classier.  Each classier is of the form\n(36)In this form, we only need to store one value for each discrete combination of\npo, io, and jo to rep-\nresent the quantity:(37)Since the composite classier is:(38)htzi()Pkt,patternkxy,()ix()jy(),,object()Pkt,patternkxy,()ix()jy(),,non-object()------------------------------------------------------------------------------------------------------------logxy,regionk1=17=Pkt,patternkxy,()po=ix()io=jy()jo=,,object()Pkt,patternkxy,()po=ix()io=jy()jo=,,non-object()------------------------------------------------------------------------------------------------------------------------------------------------logHx()signatPkt,patternkxy,()ix()jy(),,object()Pkt,patternkxy,()ix()jy(),,non-object()------------------------------------------------------------------------------------------------------------logxy,regionk1=17t1=T\n='b'56we also can store one value for each discrete combination of\npo, io, and jo given by:\n(39)An unresolved issue in using AdaBoost is when to stop the iteration.  Since weighting favors\nthe most difcult samples, those close to the decision boundary, we may become susceptible to\n\novertting.  Our approach was to monitor the performance of the algorithm, empirically using a\n\ncross-validation set and to stop the algorithm when performance stopped improving.\natPkt,patternkxy,()po=ix()io=jy()jo=,,object()Pkt,patternkxy,()po=ix()io=jy()jo=,,non-object()------------------------------------------------------------------------------------------------------------------------------------------------logt1=T'b'57Chapter 6.Implementation of the Detectors\nIn this chapter we describe how we implement our detectors.  Our main concern is speed of\nexecution.  We would like detection to be as fast as possible.  Our strategy is to re-use multi-reso-\n\nlution information wherever possible and to use a coarse-to-ne search strategy and various other\n\nheuristics to prune out unpromising object candidates.6.1.Exhaustive Search\n As explained in Chapter 2, each detector is specialized for a specic orientation, size, align-\nment, and intensity of the object.  However, an object can occur at any position, size, orientation,\n\nand intensity in the image.  Our approach is to use an exhaustive search along all these dimensions\n\nto nd objects in the image.  First, to be able to detect the object at any position in the image, we\n\nhave to re-apply all the detectors at regularly spaced intervals in the image.  At each of these sam-\n\npling sites we evaluate the candidate at ve different intensity corrections and select the one that\n\ngives the best response.  Then, to detect the object at any size, we have to repeat this process for\n\nmagnied and contracted versions of the original image.  We search at scales of magnication that\ndecrease by a multiplicative factor of.  As we explain below, we chose an integer\nroot of 2 so we could reuse information at each octave in this search through scale.  We then com-\nbine the results of running all these detectors.  If there are multiple detections at the same or adja-\ncent locations and/or scales, the algorithm chooses the strongest detection.Since it will be very time-consuming to evaluate the image in such an exhaustive fashion, we\nexperimented with several methods for decreasing computation time, as we will describe later in\n\nthis chapter.\n6.2.Searching for Objects at One Size\n In Figure 40 we outline the processing steps in searching for faces over one scaled version of\nthe original image, i.e., faces of one size.   First, we compute the wavelet transform of the entire\n\nimage.  We compute an overcomplete transform for each level of the wavelet transform.  We indi-\n241.189='b'58cate the phases of the overcomplete transform as (even, even), (even, odd), (odd, even), and (odd,\nodd). We then compute attribute values from the wavelet coefcients at every location in the\n\nimage.  When then classify the image on a region by region basis, considering every possible\n\noverlapping candidate region.  For each candidate region, we look up the probability of each\n\nattribute value.  Each of these probabilities is a joint function of the attribute value and the posi-\n\ntion of the attribute sample within the candidate region.  Rather than compute the output for each\n\nregion using equation (15) in Chapter 3, we compute its logarithm.  This transforms all the multi-\n\nplications into additions:LLHHHLLHLLHHHLLHLLHHHLLHLLHHHLLHLevel 1\neven, even\neven, odd\nodd, even\nodd, oddLLHHHLLHLLHHHLLHLLHHHLLHLLHHHLLHeven, even\neven, odd\nodd, even\nodd, oddLLHHHLLHLLHHHLLHLLHHHLLHLLHHHLLHeven, even\neven, odd\nodd, even\nodd, oddLevel 2\nLevel 3\nImageattribute samplesattribute samplesattribute samples\nRegion classication\nvalue\nDetections at this scale of resolutionFigure 40.  Sequence of operations for detectionRegion classication\nvalue\nRegion classication\nvalue\n. . .. . .\n. . .. . .'b'59(40)We then threshold on this value.  Finally for each real face in the image there will be a cluster of\ncandidate regions that all have a classication value over threshold.  Below in Figure 41 we show\n\nthe raw output from one image over 4 scales.  (The value shown at each pixel corresponds to the\n\noutput associated with a region centered at that position).  Notice that the algorithm gives a high\nresponse to a cluster of candidate regions across position and scale.  These clusters will usually\ncorrespond to one actual face in the image.  We use the following strategy to merge detections that\n\nare nearby in position and scale.  Once we have determined all the candidate regions that are\n\nabove the detection threshold, we nd the candidate region that gives the highest response.  This\n\ncandidate region is the rst to be labelled as the object.  We then remove from consideration all\n\ncandidate regions that are within a radius in position and scale from this detection.  For positional\nPkpatternkxy,()ix()jy(),,object()Pkpatternkxy,()ix()jy(),,non-object()--------------------------------------------------------------------------------------------------------\nlogxy,regionk1=17Figure 41.  Region based response as a function of scale of magnication and position.\n'b'60radius we use one half the width of the object.  For size we remove all candidates within this posi-\ntional radius that vary from half to twice the detections size.  We then continue to search among\n\nthe remaining candidate regions and nd the one with next largest response and remove all candi-\n\ndates within the positional and scale radius of this detection.  We continue this process until all\n\ncandidate regions have been classied as an object or discarded.\nThe main computational bottleneck in detection is retrieving the probability values from the\nhistograms.  As mentioned above, we only have to compute the attribute values once for the entire\n\nimage.  However, for each sample site within each candidate object location we have to look up a\n\nprobability indexed jointly by the attribute value and the position within the region.  This way\n\neach sampling site is evaluated multiple times for its probability since it will be used as part of\n\nmany candidate regions.  Moreover, the tables we index into to retrieve these probabilities are\n\nquite large, with as many as 1.6 million entries.  By working with data structures this large we are\n\npenalized in terms of memory performance.  In particular, data structures this large will not t into\n\nprimary or secondary memory cache on most standard computers of today.  When we make\n\naccesses into this data structure, the majority of them will be to main memory.  Since main mem-\n\nory access can be as long as 100 clock cycles[80], these operations will lead to signicant perfor-\n\nmance degradation.\n6.3.Re-using Multi-resolution Information\nIn searching for the object, we use multi-resolution in two ways.  First, we evaluate the input\nimages over a number of scales of magnication to search for the object at different sizes.  Then\n\nwe evaluate each of these at three scales of resolution corresponding to the three levels in the\n\nwavelet transform.  Rather than make all these evaluations separately, we can re-use many of\n\nthem.  Let us consider how this can be done.  Let us consider the rst four magnication scales at\nwhich we evaluate the image: 1.0, 1.189, 1.414 = 1.189\n2, 1.681=1.1893.  For each of these we\ncompute a three-level wavelet transform.  We then compute our attribute values from the wavelet\n\ncoefcients, and then look-up probabilities for the attribute values.  We can then re-use much of\n\nthis information when we evaluate the image at the 5th scale of magnication, 2.0.  Currently we\n\nonly re-use the level 3 LL subband as the input image.  Conceivable we could also re-use the\n\ntransform coefcients and the attribute values.  The main obstacle to doing so is the amount of\n'b'61memory that would be involved.\n6.4.Heuristic Speed-ups\nWe experimented with several heuristics to speed up processing time.  The most successful was\na coarse-to-ne strategy to prune out unpromising candidates.  We also experimented with color\n\nand adjacent heuristics.6.4.1. Coarse to Fine Search Strategy\nWe divide the attributes into three sets:  ones based on level 1 coefcients, ones based on level\n2 coefcients, and ones based on level 3 coefcients.  The rst set we evaluate over the image at 1/\n\n16 of the normal resolution.  We then threshold the partial output of the detector based on these\n\nattributes.  We only continue searching those candidates whose probability is above this threshold.\n\nWe repeat this process after we process the image using the level 2 attributes.   The danger in this\n\napproach is that we may discard candidates that we would be correctly detected as faces if we\n\nfully evaluated them.   For this reason we set the heuristic thresholds conservatively based on per-\n\nformance on a cross-evaluation set. Through this strategy we reduce our computational require-\n\nments by 1 to 2 orders of magnitude with little loss in accuracy.\n6.4.2. Adjacent HeuristicWe also noted that we search at successive scales, that if we our response was low at one scale\nit will be low at the adjacent scale.  We have experimented with thresholds on the value at the\n\nadjacent scale to prune out candidates at the current scale.6.4.3. Color HeuristicsWe also experimented with a color preprocessor to prune out unpromising candidates.  We\nused an 8x8x8 histograms to represent the color distribution of skin-color and  non-skin color.\n\nThis also improved performance speed by a factor of 2 or 4 when combined with the other heuris-\n\ntic.  It also helped discard a few candidates that would have otherwise been false detections.  The\n\nmain problem with color is that even with the color threshold set low, many actual faces get\n\nremoved on images which have poor color balancing.\n'b'626.5.Performance Time\nFor faces we can evaluate a 240x256 image in about 90 seconds on average using a Pentium II\nat 450MHz.For cars we can evaluate a 240x256 image in about 450 seconds on average using a Pentium II\nat 450MHz.'b'63Chapter 7.Face Detection Performance\nIn this chapter we describe our results in face detection in Section 7.1, provide analysis of how\nthe different parts of the face inuence detection in Section 7.2, and assess statistical dependency\n\nacross the extent of the face in Section 7.3.\n7.1.Results in Face Detection\nThe distinguishing characteristic of our face detector is that it works for both frontal and out-\nof-plane rotational views.  To date, several researchers [14][18][19][20][38] have had success\n\ndeveloping algorithms that work for frontal views of faces, but none, to our knowledge, have had\n\nsuccess with prole (side) views except [87] (below we will compare our performance with [87]).\nWe believe there are several reasons why prole view faces are more difcult to detect than\nfrontal views.  First, the salient features on the face (eyes, nose, and mouth) are not as prominent\n\nwhen viewed from the side as they are when they are viewed frontally.  Also, for frontal views\n\nthese features are interior to the object, whereas on a prole one of the strongest features is the sil-\nhouette with the background.  Since the background can be almost any visual pattern, a prole\n\ndetector must accommodate much more variation in the silhouettes appearance than a frontal\n\ndetector does for interior features.We compared the performance of our detectors with that reported by Rowley and Kanade [87]\non a test set of prole views selected from a set of proprietary images Kodak provided to Carnegie\n\nMellon University.  These images consists of typical amateur photographs with some of the typi-\n\ncal problems of such images, including poor lighting, contrast and focus.  This test set consisted\nof 17 images with 46 faces, of which 36 are in prole view (between 3/4 view and full prole\n'b'64view):\nWe also collected a larger test set for benchmarking face detection performance for out-of-\nplane rotation.  This test set consists of 208 images with 441 faces  that vary in pose from full\n\nfrontal to side view. Of these images approximately 347 are prole view (between 3/4 view and\n\nfull prole view). We gathered these images from a variety of sites on the World Wide Web,\n\nmainly news sites such as Yahoo and the New York Times.  Most of these images were taken by\n\nprofessional photographers and of better quality than the Kodak images in terms of composition,\n\ncontrast, and focus.  Otherwise, they are unconstrained in terms of content, background scenery,\n\nand lighting.  Below in Table 9 we show the performance at different values of the threshold\ngcontrolling the sensitivity of the detectors.  By changing\ng we linearly scale the detection thresh-olds of both the prole and frontal detectors. We also compare the performance of the detectors\n\ntrained with AdaBoost and without AdaBoost.  Below in Figure 42 we show some typical results\n\non this image set evaluated at\ng = 1.0 using detectors trained with AdaBoost.In terms of frontal face detection, the accuracy of our detector compares favorably with those\nTable 8: Face Detection results on Kodak data set\nRowley & Kanade [87]Schneiderman and Kanade (using AdaBoost)\nDetectionFalse\nDetectionsgDetection(all faces)\nDetection(proles only)False\nDetections58.7%13470.580.4%86.1%105\n41.3%6171.070.0%69.4%7\n\n32.6%1361.563.0%61.1%1\nTable 9: Face Detection Results on Schneiderman & Kanade Test Set\nWith AdaBoostWithout AdaBoost\ngDetections(all faces)\nDetection(proles)False\nDetectionsDetection(all faces)\nFalse\nDetections0.092.7%92.8%70082%137\n1.585.5%86.4%9174%27\n\n2.575.2%78.6%1260%3\n'b'65'b'66'b'67of other researchers.   In these experiments, we also noticed some differences in performance\nbetween the detector described in this thesis and an improved version of the detector we described\n\nin [76].  Both of these detectors use similar probabilistic structures but differ mainly in that the\n\ndetector in [76] uses visual attributes based on localized eigenvectors rather than wavelet coef-\n\ncients.  The wavelet based detector described in this thesis performs much better for prole view\n\nfaces.  However, the eigenvector based detector [76] seems to be perform slightly better on frontal\n\nfaces.  Below in Table 10 we compare our face detectors (wavelet-based and eigenvector-based\n\n[76]) with those results reported by others on the combined frontal face test set combining the test\n\nimages from Sung and Poggio [18] and Rowley, Baluja, and Kanade [14].\nTable 10: Frontal Face Detection on Sung & Poggio and Rowley [18] & Baluja & Kanade\nCombined Test Set [14]\nDetectionRateFalse\nDetectionsSchneiderman and Kanade* (wavelet)94.4%\n(95.8%)65Roth, Yang, Ahuja *[38](94.8%)78\nSchneiderman and Kanade (eigenvector)* [76]90.2%\n(91.8%)110Rowley, Baluja, and Kanade [14]86.0%31\nFigure 42. Face detection results\n'b'68* Indicates that 5 images of line drawn faces were excluded leaving 125 images with 483 labeled\nfaces.  However, there are at least 10 additional human faces that are not labeled in the ground\n\ntruth for this test set.  The numbers not in parentheses indicate results on just the 483 labeled\nfaces.  To be consistent with [38], we also indicate, in parentheses, the ratio between the total\n\nnumber of faces found by computer and 483 (the number labelled by hand).\n7.2.Positional Decomposition of Face Detection\nSince the detectors use all portions of the face, we wondered which regions tended to be most\ninuential in detecting faces.  For example, we wanted to know if the eyes, nose, and, mouth are\n\nreally the most inuential parts?  To get a sense of how much the various parts of a face contrib-\n\nuted to the overall detection score, we decomposed the contribution as a function of position on\n\nseveral faces; that is, for each face we plot an inuence image that indicates the amount of inu-\n\nence at each position on the face.  Brighter areas indicate strong positive inuence, gray regions\n\nindicate neutral inuence and darker areas indicate negative inuence.  As we would expect, areas\n\nthat are occluded usually contributed a negative inuence.  In terms of positive inuence, there\n\ndid not seem to be a region that was consistently more inuential than the others.  In particular,\n\nthe regions of positive inuence were not sharply localized but tended to be spread out. In Figure\n\n43 we show the corresponding inuence images for several frontal faces and in Figure 44 we\n\nshow the corresponding inuence images for several prole faces.\n7.3.Analysis of Pair-wise Statistical Dependency\nWe also evaluated pair-wise statistical dependency among wavelet coefcients for both frontal\nand right prole view.  We evaluated pair-wise dependency using the following measure\n\n[64][85][86]:'b'69(41)whereci andcj represent two wavelet coefcient that are discretized to take on\nm values,\nvk.Below we show some dependency images for individual coefcients.  These images show\nthe measure of pair-wise statistical dependency between a single coefcient and the rest of the\n\ncoefcients.  The statistical dependency is strongest between the coefcient and itself; thus the\n\ncoefcient is indicated by the brightest spot in the wavelet transform.  As we can see the extent of\nFigure 43.  Frontal faces and their corresponding inuence images\nDcicj,()Hci()Hcj()Hcicj,()+=Hci()Pcivk=()Pcivk=()()lnk1=m=Hcicj,()Pcivk=civl=,()Pcivk=civl=,()lnl1=mk1=m='b'70statistical dependency varies from coefcient to coefcient.  The strongest dependencies tend to\nbe inter-frequency over spatially registered positions.  Also we notice that statistical dependency\n\nis not always localized and can exist over disjoint regions.  In particular, we notice some statistical\n\ndependencies between the eye regions:\nFigure 44.  Prole images and their corresponding inuence images'b'71Figure 45.  Statistical dependency for frontal faces.  Each image gives a measure of the pair-\nwise statistical dependency between one coefcient and all the other coefcients in the\n\nwavelet transform.\nFigure 46.  Statistical dependency for prole faces.  Each image gives a measure of the pair-\n\nwise statistical dependency between one coefcient and all the other coefcients in the\n\nwavelet transform.\nFigure 47. Statistical dependency for non-object images.  Each image gives a measure\nof the pair-wise statistical dependency between one coefcient and all the other coef-\ncients in the wavelet transform\n'b'72Chapter 8.Car Detection Performance\nIn this chapter we describe our results in car detection in Section 8.1, provide analysis of how\nthe different parts of the car inuence detection in Section 8.2, and assess statistical dependency\n\nacross the extent of the car in Section 8.3.\n8.1.Results in Car Detection\nTo test the accuracy of the car detector, we collected a set of 104 images that contain 213 cars\nwhich span a wide variety of models, sizes, orientations, background scenery, lighting conditions\n\nand some partial occlusion.  We gathered these images using several cameras and from sites on\n\nthe World Wide Web.  Overall our performance was as follows:\nwhereg controls the sensitivity of the detectors; that is, by scaling\ng we linearly scale the detection\nthresholds of all eight car detectors.  Below we show some typical results on this image set evalu-\n\nated atg = 1.0:Table 11: Car Detection Results\ngDetectionsMisses\nFalse\nDetections1.05177 (83%)36 (17%)7\n1.0183 (86%)30 (14%)10\n\n0.9197 (92%)16 (8%)71\n'b'73'b'748.2.Positional Decomposition of Car Detection\nSince the detectors use all portions of the car, we wondered which regions tended to be most\ninuential. To get a sense of how much the various parts of a car contributed to the overall detec-\n\ntion score, we decomposed the contribution as a function of position on several car; that is, for\n\neach car we plot an inuence image that indicates the amount of inuence at each position on\nthe car.  Brighter areas indicate strong positive inuence, gray regions indicate neutral inuence\n\nand darker areas indicate negative inuence Below in Figure 48 we give some examples.\nFigure 48. Inuence images for various car inputs\n'b'758.3.Analysis of Pair-wise Statistical Dependency\nWe also evaluated pair-wise statistical dependency among wavelet coefcients for both frontal\nand right prole view.  We evaluated pair-wise dependency using the following measure\n\n[64][85][86]:(42)whereci andcj represent two wavelet coefcient that are discretized to take on\nm values,\nvk.Below we show some dependency images for individual coefcients.  These images show the\nmeasure of statistical dependency between a single coefcient and the rest of the coefcients.  The\n\nstatistical dependency is strongest between the coefcient and itself; thus the coefcient is indi-\n\ncated by the brightest spot in the wavelet transform.  As we can see the extent of statistical depen-\n\ndency varies from coefcient to coefcient.  The strongest dependencies tend to be inter-\n\nfrequency over spatially registered positions.  Also we notice that statistical dependency is not\n\nalways localized and can exist over disjoint regions. In Figures 49 - 51 we give some examples.\nDcicj,()Hci()Hcj()Hcicj,()+=Hci()Pcivk=()Pcivk=()()lnk1=m=Hcicj,()Pcivk=civl=,()Pcivk=civl=,()lnl1=mk1=m=Figure 49.  Statistical dependency for car (car viewpoint #8).  Each image gives a mea-\nsure of the pair-wise statistical dependency between one coefcient and all the other\ncoefcients in the transform\n'b'76Figure 50. Statistical dependency for car (car viewpoint #5).  Each image gives a mea-\nsure of the pair-wise statistical dependency between one coefcient and all the other\ncoefcients in the transform\nFigure 51.Statistical dependency for car (car viewpoint #1).  Each image gives a measure\nof the pair-wise statistical dependency between one coefcient and all the other coef-\ncients in the transform'b'77Chapter 9.Review of Other Statistical\nDetection MethodsIn this chapter in Section 9.1 we describe some of the major theoretical differences between\nour method of object detection and other methods of object detection and in Sections 9.2 and 9.3\nwe summarize several methods that have been applied to face and car detection.  In this discussion\n\nwe emphasize the particular modeling choices in each of these methods.9.1.Comparison of Our Approach to Previous Detection / Recognition Methods\nBelow we summarize the main difference between our approach and other previous\napproaches to object detection / recognition9.1.1. Local Appearance Versus Global Appearance\nMuch work in object recognition treats the appearance of the object in terms of full-sized rigid\ntemplates including the work of [5],[6], [30], [44], [29]. These methods represent the appearance\n\nof the entire object as one entity rather than decomposing the object into smaller parts. There are\nseveral disadvantages to this type of model.  First, the global methods that involve dimensionality\n\nreduction [5],[6], [30] will end up emphasizing the coarse attributes of object appearance rather\n\nthan the distinctive nature of the smaller parts such as the eyes, nose, and mouth on a face. Second\n\nthe matching of large template is known to be sensitive to small differences in scale, position, and\n\norientation.  Finally the matching of large regions can also be strongly inuenced by "irrelevant"\n\npixels.  On many objects, such as a car, there will be large indistinctive areas such as the hood and\n\nwindshield that are punctuated by relatively smaller areas of distinctive detailing such as the grill\n\nand headlights.  In matching a large region, the majority of the pixels will come from the untex-\n\ntured parts and dominate selection of the match (using any norm that weighs each pixel equally\n\nsuch as L1 or L2).9.1.2. Sampling of Local Appearance\nMost methods that use an local appearance representation [26], [4], [40], [78], [79] use a mini-'b'78mal set of hand-picked features, such as the eyes, nose, mouth on a face or a minimal set deter-\nmined by an automatic method [39], [81], [82], [83].   In contrast, methods such as [9][20] sample\n\nthe object everywhere.  We take this a step further by sampling with overlap and sampling multi-\n\nple attributes at each location.  Our strategy in doing so is use as much information as possible in\n\nmaking our detection decision.  As with any type of decision problem, by using more information\n\nwe improve our probability of making correct detections.  For example, in face detection we can\n\nrule out many candidate faces on the basis of inappropriate texture or features in areas we expect\n\na nearly uniform texture such as on the cheeks of a human face.\n9.1.3. Representation of Geometric Relationships\nAs we mentioned earlier, an important cue for detection is the geometric relationships of the\nvarious local appearance patches.  Some approaches do not model the geometry among local\n\nareas [84][9].  Other methods represent geometry rigidly [14], [19], [81], [82], [83], [38] and may\n\nbe brittle to small variation in the part positions.  Other methods use graph based matching tech-\n\nniques where nodes represent features and edges represent distances between feature pairs\n[78][79].   With this approach computational complexity grows polynomially with the number of\n\nfeatures.   Others depend on a coordinate system dened by two or three features [26][32] and\n\nmay be sensitive to any errors in these feature positions.  Our approach using multidimensional\n\nhistograms allows for exibility in feature position with respect to a stable coordinate system and\n\nlinear cost in the number of features and positional resolution.  We allow for exibility in feature\n\nposition by representing position,x,y, at a coarse resolution in the histogram, P(pattern,x,y) orwe can smooth the positional component of the histogram by convolving it with a Gaussian in\nx,yas we did in [76].  Also, by using a histogram we can process the image in a xed repetitive fash-\n\nion.  In contrast, methods that use a set of hand-picked features step through the image in irregular\n\nincrements and incur additional computational cost by doing so [80].9.1.4. Representation of Local Appearance using Wavelets\nHaar wavelets [81], [82], [83], Gaussian pyramid representation [84] and Gabor wavelets [78],\n[79] are all multi-resolution based representations that have been used in object recognition/detec-\n\ntion.  Haar wavelets are an orthogonal basis that achieves a piece-wise constant approximation to\n\nthe image.  We use 5/3 linear phase wavelet based representation which may have some advan-\n'b'79tages of the Haar wavelets in terms of smoothness and sparseness.  Also unlike the Gaussian pyr-\namid representation [84] and Gabor wavelets [78], [79] our wavelet representation has no\n\nredundancy; that is the transform is the same size as the original image.  Also, our representation\n\nis unique in decomposing appearance along the dimensions of space, frequency, and orientation.\n9.1.5. Representation of Statistics/Discriminant Functions\nResearch in object recognition has spanned a number of discriminant functions statistical mod-els.  [18] and [14] use articial neural networks (multilayer perceptrons) as part of their discrimi-\n\nnant functions.  This discriminant function is attractive in that it can achieve complicated\n\nboundaries in input space.  There are several disadvantages also.  The method for estimating such\n\na model from data does not have a closed form solution.  All solution methods involve iterative\n\nsearch and the most popular method (backpropagation) is simply gradient descent.  These meth-\n\nods can become trapped in local minima.  Mixture models [18][40] are also attractive because\n\nthey can achieve complicated boundaries in input space but are also susceptible to local minima.\n\nQuadratic discriminant functions [19]. [81], [82], [83] achieve partition of input space into conic\n\nsections.In previous work in computer vision, histograms have only be used for representing the statis-\ntics of appearance.  [54] uses a histogram to represent color statistics and [9] uses a histogram to\ncompute statistics of Gaussian derivatives.  One of the main contributions of this research is to use\n\nhistograms to jointly represent the statistics of visual attributes and position on the object as orig-\n\ninally published in [76].  Others have since used a similar representation [10].\n9.1.6. Estimation of StatisticsSince classication accuracy is more important than accuracy of probabilistic models that form\nthe classier, better performance can be achieved by training the detector to minimize classica-\n\ntion error.  To do so, we use the AdaBoost algorithm [70], [72] to explicitly reduce classication\n\nerror.  This algorithm works under a very similar principle to support vector machines [15] as\n\nused by [19] [81], [82], [83] by giving more weight to the training examples that are closet to the\n\ndecision boundary between the two classes.\n'b'809.2.Summary of Previous Methods for Face Detection\n9.2.1. Sung and Poggio [18]\nSung and Poggio developed one of the rst methods for frontal face detection.  They use a\n19x19 region as their standard size input region.  They normalize the intensities within this region\n\nby rst subtracting the best t intensity plane and then performing histogram normalization.Their model consists of two parts: a mixture of Gaussians model and a multilayer perceptron\nnetwork.  In the Gaussian part of the model, the face and non-face classes are each represented as\n\na mixture of 6 Gaussians.  In the mixture formulation, each Gaussian receives equal weighting.\n\nThe centers,mi, and covariances,\nSi, of these Gaussians are estimated using a modied k-meansalgorithm on labeled training samples.This representation could be viewed as forming class-conditional distributions are given by the\nmixtures(43)However, instead of using these for classication (as in a likelihood ratio test), they input each of\nthe Mahalanobis distances,into a multilayer perceptron which generates an output from 0 to 1.  They approximate the Mahal-\nanobis distance by computing the true Mahalanobis distance along the 75 principal dimensions of\nthe space and adding it to the Euclidean distance along the remaining dimensions (remember the\nentire input space is 19x19).The addition of the multilayer perceptron neural network makes the algorithm difcult to ana-\nPregion\nobject()12p()nS-------------------------0.5()xmi()TSi1xmi()()expi1=6Pregion\nobject()12p()nS--------------------------\n0.5()xmi()TSi1xmi()()expi1=6dxmi()TSi1xmi()='b'81lyze.  The multilayer perceptron does not increase the representational power over equation (43)\nsince the Mahalanobis distances are a sufcient statistic.  The main advantage would be that by\n\ntraining the neural network, they would be minimizing classication error rather than basing their\n\ndetector on probability estimating.9.2.2. Rowley, Baluja, and Kanade [14]\nSoon after Sung and Poggios algorithm, Rowley, Baluja, and Kanade developed a slightly\nmore accurate method for frontal face detection.  The method of Rowley, Baluja, and Kanade uses\n\na 20x20 input region that is presented to a multilayer perceptron neural network system for classi-\n\ncation.  They have performed experiments with both single neural networks and modular sys-\n\ntems consisting of several neural networks.\nIn all their algorithms, each 20x20 input region is pre-processed to correct for differences in\nlighting conditions using the same method as Sung and Poggio [18]:  a linear function of intensity\n\nis t to the region and subtracted out, then histogram normalization is performed.\nThe neural network topology they used in all their experiments consisted of one layer of hid-\nden units where each hidden has a receptive eld of either 5x5, 10x10, or 20x5. The used two\n\nvariations on this topology: one with 52 hidden units and the other with 78 hidden units.\nIn their experiments with modular systems, they separately trained two or three of the above\nnetworks and then applied various methods for merging their results, including training another\n\nmultilayer perceptron or an individual perceptron to act as an arbitrator.\nThey experimented with various heuristics to the output of the neural network, including merg-\ning nearby detections, discarding isolated detections, and not allowing overlapping faces.\nIts somewhat difcult to analyze the behavior of a multilayer perceptron.  We can infer that this\nparticular network topology emphasizes local features over global ones, since the hidden units\n\nhave only local support.  Statistical dependencies that span a larger region than any of the recep-\n\ntive elds are only modeled weakly through normalization and network arbitration.\n'b'829.2.3. Osuna, Freund, and Girosi [19]\nOsuna, Freund, and Girosi use a 19x19 input region,\nx, for detecting faces.  They apply the fol-\nlowing discriminant function to the input image,\nx (see [24] or [25] for denition of 2nd-degree\npolynomial kernel function):\n(44)WhereSV, indicates support vector.\nThis function can be rewritten as a quadratic discriminant function:\n(45)wherea0 is a scalar,\na1 is a vector, and\nA2 is a symmetric semi-positive denite matrix. These\nparameters can be directly computed fromci, yi, andc0 given above.\nThe quantities,ciand yi are determined through training.  Their method of training is explicitly\ndesigned to minimize generalization error.\nOne way we can compare this model to the general model of the posterior probability function\nis through the concept of Taylor approximation.  We can think of a quadratic discriminant func-\n\ntion loosely as being a Taylor series approximation to the posterior probability function truncated\n\nat its 2nd order term.  Likewise, we can think of a perceptron as being a Taylor series approxima-\n\ntion to the posterior probability function truncated at rst order.\n9.2.4. Moghaddam and Pentland [12]\nMoghaddam and Pentland have developed a face detection algorithm as part of a bigger system\nthat includes face recognition and face tracking.  For face detection, they appear to use a standard\n\nsized input region.  They normalize the intensity of the region by scaling the image to have zero\n\nmean and unit variance.\nfx()ciyiTx1+()2iSV\n=fx()a0a1TxxTA2x++='b'83Moghaddam and Pentland base their method of detection on the value of the class conditional\nprobability function,.  Their model of the class conditional density is decom-\nposed into two independent distributions:\nThe rst distribution,, is computed on a projection of the image region onto\na 10 dimensional subspace, given by the columns of\nF.  The subspace,F, consists of the principalcomponents of the face data set.  They model this distribution as a mixture of Gaussians:\nwhereAll the parameters in this distribution -- the weighting coefcients, the means, the covariances are\nestimated using the maximum likelihood principle and solved using the E-M algorithm, which\n\nnds a locally optimal solution.The 2nd distribution,, computes the distributions of the projection of\nthe image region onto the space orthogonal to the 10 principal components.  This distribution is\nmodeled as a Gaussian with zero mean and scaled identity matrix for the covariance:\nPregionobject\n()pregionX\n=object()P1FTXobject\n()P2IFT()Xobject\n()P1FTXobject\n()P1FTXobject\n()p\niNmiSi,()i1pii=P2IFT()Xobject\n()P2IFT()Xobject\n()N0Is2,()'b'84wherewhere thelis are the eigenvalues of the distribution.\n9.2.5. Colmenarez and Huang [20]\nColmenarez and Huang have developed a method for face detection.  They use a Markov eld\nmodel to capture the statistical behavior of the face and non-faces classes over a 11x11 input\n\nregion.  Their decision rule is based on thresholding the likelihood ratio:\n(46)They model both the face and non-face class-conditional probabilities using a rst order\nMarkov eld model over the 11x11 input region.  The relationship between this Markov eld\nmodel and the general form of the class-conditional probability function,is\nfairly straightforward.  Consider re-ordering the image\nregion\n into a 1 dimensional array ofn pix-els.  The general expression for class conditional probability of the image\nregion\n can then be re-written using the chain rule:s21n10---------------lii11=n=LPregionobject\n()Pregionobject\n()---------------------------------------------=Pregionobject\n()'b'85(47)whereregion(i)\n is the pixel intensity for the\nith pixel in the\nregion\n and casen = 121 for Col-menarez and Huangs representation.\nIn applying the Markov assumption, it is assumed that a pixel is only statistically dependent on\nthek pixels that immediately precede it in the ordering of the 1-dimensional array:\n(48)Colmenarez and Huang assume a 1st order Markov process,\nk = 1:(49)The class conditional density is then approximated by making this substitution for all terms on theright hand side of equation (47):Pregionn\n()regionn\n1(). . .region1()object,,,\n()=Pregionn\n()regionn\n1(). . .region1()object,,,\n()xPregionn\n1()regionn\n2(). . .region1()object,,,\n()xPregion\n1()object()..Pregionobject\n()=Pimagej\n()imagej\n1()imagej\n2(). . .image1()object,,,,\n()\nPimagej\n()imagej\n1()imagej\n2(). . .imagejk\n()object,,,,\n()Pregionj\n()regionj\n1()regionj\n2(). . .region1()object,,,,\n()\nPregionj\n()regionj\n1()object,()'b'86(50)Colmenarez and Huang consider all possible orderings of the pixels in mapping the 11x11\nregion into a 1 dimensional array.  They select the mapping that optimizes the Kullback diver-\n\ngence of the face class with respect to the non-face class.  To compute the Kullback divergence,\n\ntraining examples are used.  They do not state why they use the  Kullback divergence.\nIn building the overall model, they construct intermediate models for the 1st order conditional\nprobabilities:(51)for all pixels pairs.  They directly estimate the two terms on the right hand side of the equation.\nEach of these terms is a discrete valued function where each intensity value is quantized into 3\n\nlevels.\n9.2.6. Burl and Perona [26]\nBurl and Perona have developed a method for face detection.  Burl and Peronas method\nemphasizes the spatial distribution of features.  They detect 5 types of features on the face: the left\n\neye, right eye, nose/lip junction, left nostril, and right nostril.  They assume that the feature detec-\n\ntors for each feature are fallible.  They may not respond to the actual feature, and they may give\n\nfalse responses elsewhere.  Since they assume only one face is present in each image, they assume\n\nthat at most one feature response is correct for each type of detector and all other responses to that\ndetector are false alarms.\nTo locate each face, they consider all possible hypotheses.  In each hypothesis,\nHi, only one(or none) of the responses for each detector is selected.  Of all these hypotheses, they select the\nPregionn\n()regionn\n1(). . .region1()object,,,\n()\nPregionn\n1()regionn\n2()object,(). . .Pregion\n1()object()Pregionn\n()regionn\n1()object,()xPimagej\n()imagej\n1()object,()Pimagej\n()imagej\n1()object,()Pimagej\n1()object()---------------------------------------------------------------------------------------='b"87one that gives the maximum posterior probability:\n(52)whereW represents the locations for all feature responses for all detectors.SinceP(W) is the same for all hypotheses, they maximize a goodness function:\n(53)Their prior probability,\nPr(Hi) depends only on how many (and which) of the 5 features are in\nthe hypothesis.Pr(Hi) will differ between two hypotheses only if one hypothesis includes a fea-\nture type (e.g. eye) that is absent in the other hypothesis.  The difference depends primarily on a\nparameter,\ngj, which represents prior knowledge about the accuracy of each feature detector,\nj.To model\nP(W|H) for each hypothesis, the data is partitioned into 2 sets:\nW(H) - the featuresresponses hypothesized to correspond to the true features andW(H) - the remaining featureresponses (false alarms).  These two sets of feature responses are modeled by two statistically\n\nindependent distributions:\n(54)They further assume that the false alarms are statistically independent:\n(55)whereq(w) is the probability of an individual false alarm.\nTo simplify the goodness functions, it is divided by a constant given by\nPq(w):H*maxPH\niW()argmaxPWH\ni()PrH\ni()PW()-----------------------------------------arg==GHi()PWH\ni()PrH\ni()=PWH\n()PHWH()()QH'WH'()()=QH'WH'()()qWH\n'()()="b'88(56)By substituting (55) into (54) and substituting the result into (56), each goodness functionbecomes a function of only the feature responses in the hypothesis, W(H):\n(57)Berl and Perona attempt to compensate for differences in translation, scale and rotation of the\nfeature positions by dening a normalized coordinate system for each hypothesis.  The location of\n\nthe response of the rst feature, the left eye, becomes the origin of the new coordinate system and\n\nthe 2nd feature point, the right eye, is dened to be (1, 0).  The coordinates of all other features\n\ncan then be computed with respect to these two points.  To allow for the possibility of missing fea-\n\ntures, they also compute coordinate systems dened by other pairs of feature points.  Although\n\nthis method removes the differences in translation, scale, and rotation with respect to the detected\n\nfeatures, it is not stated how the feature detectors themselves are made invariant to scale and rota-\n\ntion.  In making this normalization, the goodness function becomes:(58)where S describes the coordinates of the features in the normalized coordinate frame.In this form, equation (58), we can draw some comparisons to our method of representation.\nWe can think of the goodness function as being proportional to the posterior probability of the\n\nobject conditioned on the normalized feature locations, S:(59)The class conditional distribution of the features given the object, P(S|object), can be re-written\ndecomposing position from appearance using the probability chain rule:GoHi()PWH\ni()PrH\ni()qw()-----------------------------------------=GoHi()PWH\ni()PrH\ni()qWH\n()()-----------------------------------------=GoHi()RS()PrH\ni()QS()------------------------------=GoHi()PSobject\n()Pobject\n()PS()--------------------------------------------------------\n'b'89(60)The rst term on the right of (60) embodies their description of shape and is modeled by a Gauss-ian:(61)wherem andS are estimated from the training data.The second probability on the right of (60) gives the probability of locating the actual features.\nThis is given by their notion of prior probability Pr(H) and is not computed from the training data:\n(62)  The term in the denominator of (58) represents the unconditional distribution of the features.\nThey represent the features as independently distributed with Gaussian distribution centered at the\ncenter of the image with variance\ns2 (no training data was used to estimate this distribution):\n(63)They report performances 80% and 94% recognition on various test sets they compiled.  And they\nreport 63% recognition on their training set.9.2.7. Roth, Yang, Ahuja [38]\nThis work describes a method for frontal face detection on 20x20 regions.  For every possible\npixel value at every possible location within this region they assign a weight, giving a total of\n\n20x20x256 weights.  They use an iterative training procedure using the Winnow update rule to\n\ndetermine these weights.  Once they have determined the weights they can classify any region by\n\nlooking up and summing the weights corresponding to each pixel value:\nPobjectS\n()Ps1w1. . .s5w5,,,,\nobject()==Ps1. . .s5,,w1. . .w5,,object,()Pw1. . .w5,,object()Ps1. . .s5,,w1. . .w5,,object,()RS()NmS,()=Pw1. . .w5,,object()PrH\ni()PS()QS()=N0s2,()'b'90(64)If this sum exceeds a threshold the region is classied as a face.\nThis method performs quite well on the Sung & Poggio and Rowley, Baluja, Kanade Test set.\nThey report 94.8% detection with 78 false detections.  It is somewhat surprising that this method\n\nworks so accurately since the features essentially have no spatial extent.  They are just individual\n\npixels.\n9.3.Summary of Previous Methods for Car Detection\n9.3.1. Rajagopalan, Burlina, Chellappa [44]This paper describes a method for car detection from aerial images.  They use a distance based\nclassication metric on 16x16 regions.  They cluster their training images into several classes of\n\ncars and several classes of non-cars.  For each 16x16 input region, they compute the distance to\n\neach class.  If the input is closest to a car cluster and under some threshold they classify it as a car.\n\nThe distance threshold they use could be thought of as a Mahalanobis-like distance metric, except\n\ninstead of normalizing distance by just 2nd order statistical moments, as in Mahalanobis distance,\nthey use some higher order moments also.  They have reported some success in detecting cars\n\nfrom this vantage.\n9.3.2. Papageorgiou, Poggio [83]\nIn this method the Haar wavelet transform is taken of each input region.  The wavelet coef-\ncients from two of the middle frequency bands (3,030) wavelet coefcients are used as input to a\n\nquadratic classier.  The coefcients in the quadratic classier are learned by using the Support\n\nVector Machine training method. They report some success in detecting straight-on frontal and\n\nstraight-on rear views.\nwIxy\n,()xy,,()xy,20x20'b'91Chapter 10.Conclusion\nIn this thesis we have advanced the state of the art in 3D object detection in the following ways.\nWe have developed the rst algorithm that can reliably detect faces that vary in viewpoint from\n\nfrontal to side view.  Previously only frontal face detection had been demonstrated reliably.  We\n\nhave also demonstrated the rst method for car detection that works robustly over a range of view\n\npoints.Several concepts contribute to the effectiveness of these methods:\n\n Joint statistics of appearance and position -   Much research in parts-based approaches toobject recognition overlook the importance of representing the geometric arrangement of the\n\nparts.  In our experiments, we have found  performance improves drastically when we model the\n\nstatistics of appearance and position jointly.\n Powerful representation of appearance - In our experiments we have observed that increased\nrepresentation power improves the accuracy of the object detector.  For example, we originally\n\ndeveloped a weaker representation based on a subset of localized eigenvectors [76].  Although\n\nthis representation worked well for frontal face detection, it was not fully satisfactory for prole\n\ndetection.  We also noticed that when we reconstructed prole images from this representation,\n\nmany of the small features that form of the silhouette of the face were lost.  We then redesigned\n\nour representation using the wavelet-based representation described in this thesis.  With this new\n\nrepresentation, our visual representation of these features was better leading to improved detec-\n\ntion performance for prole views of faces.\n Representation of the non-object - We also observed that performance depended on how we\nrepresented the non-object class.  We noticed that having some model was an improvement over\n\nhaving no model, even if our model was based on randomly sampled non-object images.  Perfor-\n\nmance improved further by using bootstrapping to select non-object samples and improved still\n\nfurther by using AdaBoost to weight them.'b'92 Visual cues based on local relationships - We have shown that by using a combination of\nvisual cues with selective localization in space, frequency and orientation we can achieve accurate\n\ndetection of faces and cars.\n Coarse-to-ne heuristics - By using coarse to ne heuristics, we have demonstrated that we\ncan use a large model with many visual cues in a computationally feasible way.\nThere are several research areas we see as a natural continuation of this work:\n\n Representation - Representation remains the most important issue in object detection.  Per-haps, some day researchers will develop specic statistic models for visual appearance in the\n\nsame way Gaussian and Poisson models were developed to model specic physical phenomena.\n\nOf course, to do so, we would need a way to cope with the high dimensional nature of images.\n\nOne approach would be to look at the pair-wise statistics among wavelet coefcients as we have\n\nsuggested in Chapters 7 and 8.  Another approach would be to use our knowledge of the physics\n\nof image formation.  Good models for the effects of illumination, reection, geometry, and mate-\n\nrial type on appearance exist.  These models have been used to synthesize images that look fairly\n\nrealistic.  It may be possible to use such models to characterize the statistics of appearance.  In\nparticular, it may be easier to characterize the statistical variation of the input to the image for-\n\nmation process -- the illumination, the geometry of the scene, and the surface characteristics --\n\nthan to characterize the image variation directly.  We could then analyze how these imaging mod-\n\nels transform these stochastic inputs and thereby indirectly arrive at a statistical characterization\n\nof appearance. Intensity/Lighting correction - In our work, performance improved when we were able to cor-\nrect for differences in illumination.  Most of the existing methods for intensity correction use sim-\n\nple methods such as histogram equalization or transforming the intensities to have zero mean and\n\nunit variance.  We believe that better performance can be achieved by explicitly accounting for the\n\nappearance of the object we are trying to enhance.  One approach would be to use a probabilistic\n\nmodel of the objects appearance to choose the correction.  Let us assume we have some lighting\n\ncorrection model:'b"93(65)where the parameter,\nq, controls the lighting correction.  Given that we have models for\nP(image\n |object) andP(image\n | non-object), we could choose the value of\nq that gives the highest response\nand therefore is corrected so it most looks like a member of its class:\n(66)We have conducted some preliminary experimentation with this approach but have not had much\nsuccess. Sample selection and weighting - In our experiments, performance improved when we used\nbootstrapping to select samples and boosting to weight samples.  Perhaps there is a principled way\n\nof combining these two methods to achieve better performance.\n Coding of appearance - We have used scalar quantization to discretize each wavelet coef-\ncient separately.  Methods of vector quantization whereby a whole group of coefcients is quan-\n\ntized together may improve performance.  We noticed such an improvement in an earlier method\n\nbased on eigenvector responses [76].\nThere are also several research problems that we see as a natural continuation of this work:\n\n Detection of other rigid objects - We would like to test the generality of this algorithm by\napplying it to other rigid objects such as boats, airplanes, animals, pedestrians, etc. Detection of more challenging objects - There are more challenging objects we would like to\ndetect such as buildings, trees, and text in video.  These objects have some structural regularity\n\nbut less so than faces or cars.  We believe it is possible to detect such objects accurately with cur-\n\nrent computing power, but new representations will have to be developed to do so.\nimage'Cimage\nq,()=qmax\nqargPCimage\nq,()object()PCimage\nq,()non-object()--------------------------------------------------------------------\n="b'94 Other classications - There are many other classication problems that are probably solv-\nable such as discriminating between indoor and outdoor scenes, urban and rural scenes, etc.  It\nshould also be possible to classify people based on activity (talking, smiling, walking) and their\n\ncharacteristics (age, gender, hair color, facial hair, glasses, etc.).  It may even be possible to\n\nrobustly identify people by computer.  Many research efforts are making progress in this area.\n'b'95References\n[1]. www.salon.com.  Bill Gates Other CEO.  2/7/2000.\n[2] www.ap.org\n\n[3]D. P. Huttenlocher, S. Ullman.   Recognizing Solid Objects by Alignment with an Image.\nIJCV.  5:2, pp. 195-212, 1990.\n[4] A. Pentland, B. Moghaddam, T. Starner. View-Based and Modular Eigenspaces for Face Rec-\nognition.  CVPR 1994.\n[5] M. Turk, A. Pentland.  Eigenfaces for Recognition.  Journal of Cognitive Neuroscience, 3:1,\npp. 71-86. 1991[6] P. N. Belhumeur, J. P. Hespanha, D. J. Kriegman.  Eigenfaces vs. Fisherfaces: Recognition\nUsing Class Specic Linear Projection.  PAMI. 19:7 pp. 711-720.  July, 1997.\n[7].  K. S. Arun, T. S. Huang, S. D. Blostein.  Least-Squares tting of two 3-D point sets.  IEEE\nTransactions on Pattern Recognition and Machine Intelligence (PAMI). vol. 9. pp. 698 - 700.\n\nSept., 1987.[8] T. Niblett. Constructing decision trees in noisy domains.  Proceedings of the second Euro-\npena working session on learning. pp 67-78.  Bled, Yugoslavia.\n[9]B. Schiele, J. L. Crowley.  Recognition without Correspondence using Multidimensional\nReceptive Field Histograms.  MIT Media Lab.  Tech Report 453.\n[10]B. Schiele, A. Pentland.  Probabilistic Object Recognition and Localization.  ICCV 99.\n\n[11] Martial Hebert, Jean Ponce, Terrance Boult, and Ari Gross. Report on the 1995 Workshop\non 3-D Object Recognition in Computer Vision.\nObject Recogntion in Computer Vision\n.Inter-national NSF-ARPA Workshop, Dec., 94.  Leccture Notes in Computer Science, 994.  pp. 1 - 18.\n\nSpringer, 1995.\n[12]. Baback Moghaddam and Alex Pentland.  Probabilistic Visual Learning for Object Detec-\ntion.  5th International Conference on Computer Vision (ICCV 95) (also M.I.T. Media Labora-\n\ntory Perceptual Computing Section, Technology Report No. 326).\n[13]. Jae S. Lim.Two-Dimensional Signal and Image Processing\n.  Prentice-Hall.  1990.[14].  Henry A. Rowley, Shumeet Baluja, and Takeo Kanade.  Neural Network-Based Face\nDetection. IEEE Transactions on Pattern Analysis and Machine Intelligence, Vol. 20, No. 1, Jan-\n\nuary, 1998, pp. 23-38.\n'b'96[15]  V. N. Vapnik.\nThe Nature of Statistical Learning Theory.\n   Springer-Verlag.  1995.\n[16]Abhijit S. Pandya and Robert B. Macy.\nPattern Recognition with Neural Networks in C++\n.CRC Press. Boca Raton, FL. 1996.[17]Marvin Minsky and Seymour Papert.\nPerceptrons: an Introduction to Computational Geome-\ntry (expanded edition) MIT Press. Cambridge, MA, 1988.\n[18] K-K Sung, T. Poggio.Example-Based Learning for View-Based Human Face Detection.\nPAMI, 20:1, pp. 39-51, 1998.\n[19] Edgar Osuna, Robert Freund, and Federico Girosi.  Training Support Vector Machines: an\nApplication to Face Detection.  CVPR 97.  pp. 130 - 136.\n[20] A. J. Comenarez and T. S. Huang. Face Detection with Information-Based Maximum Dis-\ncrimination.  CVPR 97.  pp 782 - 787.\n[21] P. J. Phillips, et. al. The FERET Evaluation Methodology for Face-Recognition Algo-\nrithms. CVPR 97. pp. 137 - 143.\n[22] Rutishauser, H.  The Jacobi Method for Real Symmetric Matrices.  Numer. Math., 9, pp. 1-\n10, 1966.[23] B. V. K. Vijaya Kumar.  Lectures on Pattern Recognition.  In Press.\n\n[24].  Corinna Cortes and Vladimir Vapnik.  Support-Vector Networks.\nMachine Learning\n, 20,pp 273-297.  1995.[25].  Fredercio Girosi and Tomaso Poggio.  Lecture notes for  MIT Course 9.520:\nLearning,\nApproximation, and Networks\n (class 9).  http: //www.ai.mit.edu/projects/cbcl/course9.520/\n[26]  M. C. Burl and P. Perona.  Recognition of Planar Object Classes.  CVPR 96.  pp 223 -\n230.[27]  B. Ripley.\nPattern Recognition and Neural Networks\n.  Cambridge University Press.  1996.\n[28]  C. Bishop.Neural Networks for Pattern Recognition\n.  Clarendon Press.  Oxford, 1995.[29]  D. Casasent and L. Neiberg.  Classier and Shift-invariant Automatic Target Recogntion\nNeural Networks.\nNeural Networks\n.  Vol. 8, No. 7/8.  pp. 1117 - 1129.  1995.\n[30]  H. Murase and S. K. Nayar.  Visual Learning and Recognition of 3-D Objects from Appear-\nance.\nInternational Journal of Computer Vision\n.  14, pp 5-24.  1995.[31]  William M. Wells III.  Statistical Approaches to Feature-Based Object Recognition.\nInter-national Journal of Computer Vision\n.  21(1/2), pp. 63-98, 1997.'b'97[32]  Y. Lamdan and H. J. Wolfson.  Geometric Hashing: A General and Efcient Model-Based\nRecognition Scheme.  ICCV, 1988.  pp. 238 - 249.\n[33]  Shimon Ullman and Ronen Basri.  Recognition of Linear Combinations of Models.\nPAMI.  13:10, pp. 992 - .  Oct. 1991.\n[34] John Krumm.  Eigenfeatures for Planar Pose Measurement of Partially Occluded Objects.\nCVPR, 1996.[35]  K. Ohta and K. Ikeuchi.  Recognition of Multi Specularity Objects using the Eigen-win-\ndow.  Tech. Report.  CMU-CS-96-105.\n[36] M. Koch, M. M. Moya, L. D. Hostetler, R. J. Fogler.  Cueing, Feature Discovery, and One-\nclass Learning for Synthetic Aperture Radar Automatic Target Recognition.\nNeural Networks\n.Vol. 8, No. 7/8.  pp. 1081 - 1101.  1995.\n[37]  B. V. K. Vijaya Kumar.  Tutorial Survey of Composite Filter Designs for Optical Correla-\ntors.\nApplied Optics.  31:23, pp. 4773 - 4801.  August, 1992.[38] D. Roth, M-H. Yang, N. Ahuja.  A SNoW-Based Face Detector. NPPS 99.\n\n[39]P. A. Viola.  Complex Feature Recognition: A Bayesian Approach for Learning to Recog-\nnize Objects.  AI Memo No. 1591.  November, 1996.\n[40]B. Moghaddam and A. Pentland.  Probabilistic Visual Learning for Object Representation.\nPAMI.  19:7.  pp. 696 - 710   July, 1997.\n[41] M. D. Wheeler and K. Ikeuchi.  Sensor Modeling, Probabilistic Hypothesis Generation, and\nRobust Localization for Object Recognition.  PAMI.  17:3.  pp. 252 - 265.  March, 1995.\n[42] F. Stein and G. Medioni.  Structural Indexing: Efcient 3-D Object Recognition. PAMI.\n14:2.  pp. 125 - 144.  Feb. 1992.\n[43] J. Huang, S. Gutta, H. Wechsler.  Detection of Human Faces Using Decision Trees.  2nd\nIntl. Conf. on Automated Face and Gesture Recognition.\n[44] A. N. Rajagopalan, P. Burlina, R. Chellappa.  Higher Order Statistical Learning for Vehicle\nDetection in Images.  ICCV 99.  pp. 1204 - .\n[45] J. L. Mundy, A. Zisserman (eds).\nGeometric Invariance in Computer Vision\n.  MIT Press.Cambridge, MA, 1992.[46] D. Forsyth, et. al.  Invariant Descriptors for 3D Object Recognition and Pose. PAMI 13:10.\npp. 971-991.[47] W. E. L. Grimson (with contributions from T. Lozano-Perez and D. P. Huttenlocher.\nObject'b'98Recognition by Computer: The Role of Geometric Constraints\n. MIT Press.  Cambridge, MA,1990.[48]  J. J. Koenderink,\nSolid Shape, MIT Press, 1990, Cambridge, MA[49] H. Moravec.  Obstacle Avoidance and Navigation in the Real World by a Seeing Eye Robot\nRover.  Ph. D. Stanford, 1980.\n[50] J. Shi and C. Tomasi.  Good Features to Track.  CVPR 94.  pp. 593-600.  June, 1994.\n\n[51] M. Gori,   F. Scarselli.  Are Mulilayer Perceptrons Adequate for Pattern Recognition and\nVerication.  PAMI 20:11.  pp. 1121-1132.  November, 1998.\n[52] S. Y. Kung.\nDigital Neural Networks\n.  Prentice-Hall, 1993.[53]  D. J. Field.   Wavelets, vision and the statistics of natural scenes.   Philosophical Transac-\ntions of the Royal Society: Mathematical, Physical and Engineering Sciences.  Vol. 357 Issue\n\n1760 - 1999.  pp. 2527-2542[54] M. J. Swain, D. H Ballard, Color Indexing. IJCV 7:1, pp. 11-32. November 1991,\n\n[55]Suetens, Fua, Hansen.  Computational Studies for Object Recognition, ASM\n[56] A. Papoulis.\nProbability, Random Variables, and Stochastic Processes\n. McGraw-Hill, 2nd\nEd, 1984.[57] D. Marr.\nVision\n. Freeman Publishers, 1982.[58]G. Strang, T. Nguyen.\nWavelets and Filter Banks\n.  Wellesley - Cambridge Press. Wellesley,\nMA.  1997.[59]M. Vetterli, J. Kovacevic.\nWavelets and Subband Coding\n.  Prentice-Hall, 1995.[60] I. Daubechies.Ten Lectures on Wavelets\n. CBMS-NSF Regional Conference Series in\nApplied Mathematics 61. 1992.[61] P. C. Cosman, R. M. Gray, M. Vetterli.  Vector Quantization of Image Subbands: A Survey.\nIEEE Transactions on Image Processing.  5:2  pp. 202-225.  February, 1996.\n[62] Z. Gigus,  J. Canny, R. Seidel. Efciently Computing and Representing Aspect Graphs of\nPolyhedral Objects.  PAMI 13:6. pp. 542-551.  June 1991.\n[63] T. Mitchell.\nMachine Learning\n.  McGraw Hill, 1997.\n[64] P. Domingos, M. Pazzani.  On the Optimality of the Simple Bayesian Classier under Zero-\nOne Loss.  Machine Learning.  29, pp 103-130.  1997.\n'b'99[65] T. Kailath.\nLinear Systems.  Prentice-Hall, 1980.[66]R. Sekuler, R. Blake.\nPerception\n(3rd Edition).  McGraw-Hill, 1994.\n[67]E. B. Goldstein.Sensation and Perception\n (4th Edition).  Brooks / Cole Publishing.  1996.[68] J. Frisby.\nSeeing: Illusion, Brain, and Mind\n. Oxford University Press, 1979.\n[69] D. H. Hubel.Eye, Brain, and Vision\n.  Scientic American Library Series, #22. 1988.[70]Y. Freund, R. E. Shapire.  A Decision-Theoretic Generalization of On-Line Learning and an\nApplication to Boosting.  Journal of Computer and System Sciences. 55:1, pp. 119-139. 1997.\n[71]L. Breiman.  Arcing Classiers.  The Annals of Statistics.  26:3, pp. 801-849.  1998.\n\n[72]R. E. Shapire, Y. Singer.  Improving Boosting Algorithms Using Condence-rated Predic-\ntions. Machine Learning  37:3,  pp. 297-336.  December, 1999.\n[73]E. Bauer, R. Kohavi.  An Empirical Comparison of Voting Classication Algorithms: Bag-\nging, Boosting, and Variants.  Machine Learning. 36:1/2, pp. 105-139. July, 1999.\n[74] T. Jaakkola, M. Meila, T. Jebara. Maximum entropy discrimination. MIT AITR-1668,\n1998.[75] C. Goerick, D. Noll, M. Werner, Articial Neural Networks in Real-Time Car Detection and\nTracking Applications.  Pattern Recognition Letters. 17:4. pp. 335-343.  April, 1996.\n[76] H. Schneiderman, T. Kanade.  Probabilistic Modeling of Local Appearance and Spatial\nRelationships for Object Recognition.  CVPR 98.\n[77]I. E. Gordon.Theories of Visual Perception\n.  John Wiley & Sons.  1989\n[78] M. Lades, J. C. Vorbruggen, J. Buhmann, J. Lange, C. v. d. Malsburg, R. P. Wurtz, W. Konen.\nDistortion Invariant Object Recognition in the Dynamic Link Architecture.  IEEE Transactions\n\non Computers. 42:3.  pp 300 - 311.  March, 1993.[79] L. Wiskott, J-M Fellous, N. Kruger, C. v. d. Malsburg.  Face Recognition by Elastic Bunch\nGraph Matching.  PAMI. 19:7.  pp. 775-779, 1997.\n[80]K. Dowd and C. Severance.\nHigh Performance Computing\n (2nd Ed).  OReilly, 1998.\n[81] M. Oren, C. Papageorgiou, P. Sinha, E. Osuna, T. Poggio.  Pedestrian Detection Using\nWavelet Templates.  CVPR 97. pp. 193 - 97.\n[82]C. P. Papageorgiou, M. Oren, T. Poggio.  A General Framework for Object Detection.\nICCV 98.'b'100[83] C. P. Papageorgiou, T. Poggio. A Trainable Object Detection System: Car Detection in\nStatic Images. MIT AI Memo No. 180.  October, 1999.\n[84] T. D. Rikert, M. J. Jones, P. Viola.  A Cluster-Based Model for Object Detection.  ICCV\n1999. pp. 1046 - 1053.[85] S. J. Wan, S. K. M. Wong.  A Measure for Concept Dissimilarity and Its Applications in\nMachine Learning.  Proceedings of the International Conference on Computing and Information.\n\npp. 267-273.[86]I. Kononenko.  Semi-naive Bayesian Classier. Proceedings of the Sixth European Working\nSession on Learning.  pp. 206-219.[87]H. Rowley.  Neural Network-Based Face Detection.  Ph.D thesis.  CMU-CS-99-117.  1999.\n'