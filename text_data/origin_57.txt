b"YOLO9000:\nBetter,Faster,Stronger\nJosephRedmon\n\n,AliFarhadi\n\nUniversityofWashington\n\n,AllenInstituteforAI\ny\nhttp://pjreddie.com/yolo9000/\nAbstract\nWeintroduceYOLO9000,astate-of-the-art,real-time\nobjectdetectionsystemthatcandetectover9000object\ncategories.Firstweproposevariousimprovementstothe\nYOLOdetectionmethod,bothnovelanddrawnfromprior\nwork.Theimprovedmodel,YOLOv2,isstate-of-the-arton\nstandarddetectiontaskslike\nP\nASCAL\nVOCandCOCO.Us-\ninganovel,multi-scaletrainingmethodthesameYOLOv2\nmodelcanrunatvaryingsizes,offeringaneasytradeoff\nbetweenspeedandaccuracy.At67FPS,YOLOv2gets\n76.8mAPonVOC2007.At40FPS,YOLOv2gets78.6\nmAP,outperformingstate-of-the-artmethodslikeFasterR-\nCNNwithResNetandSSDwhilestillrunning\nfaster.Finallyweproposeamethodtojointlytrainonob-\njectdetectionandUsingthismethodwetrain\nYOLO9000simultaneouslyontheCOCOdetectiondataset\nandtheImageNetdataset.Ourjointtraining\nallowsYOLO9000topredictdetectionsforobjectclasses\nthatdon'thavelabelleddetectiondata.Wevalidateour\napproachontheImageNetdetectiontask.YOLO9000gets\n19.7mAPontheImageNetdetectionvalidationsetdespite\nonlyhavingdetectiondatafor44ofthe200classes.Onthe\n156classesnotinCOCO,YOLO9000gets16.0mAP.But\nYOLOcandetectmorethanjust200classes;itpredictsde-\ntectionsformorethan9000differentobjectcategories.And\nitstillrunsinreal-time.\n1.Introduction\nGeneralpurposeobjectdetectionshouldbefast,accu-\nrate,andabletorecognizeawidevarietyofobjects.Since\ntheintroductionofneuralnetworks,detectionframeworks\nhavebecomeincreasinglyfastandaccurate.However,most\ndetectionmethodsarestillconstrainedtoasmallsetofob-\njects.\nCurrentobjectdetectiondatasetsarelimitedcompared\ntodatasetsforothertaskslikeandtagging.\nThemostcommondetectiondatasetscontainthousandsto\nhundredsofthousandsofimageswithdozenstohundreds\noftags[\n3\n][\n10\n][\n2\n].datasetshavemillions\nofimageswithtensorhundredsofthousandsofcategories\n[\n20\n][\n2\n].\nWewouldlikedetectiontoscaletolevelofobjectclas-\nHowever,labellingimagesfordetectionisfar\nmoreexpensivethanlabellingforortagging\n(tagsareoftenuser-suppliedforfree).Thusweareunlikely\nFigure1:\nYOLO9000.\nYOLO9000candetectawidevarietyof\nobjectclassesinreal-time.\n1\n"b"toseedetectiondatasetsonthesamescaleas\ndatasetsinthenearfuture.\nWeproposeanewmethodtoharnessthelargeamount\nofdatawealreadyhaveanduseittoexpand\nthescopeofcurrentdetectionsystems.Ourmethodusesa\nhierarchicalviewofobjectthatallowsusto\ncombinedistinctdatasetstogether.\nWealsoproposeajointtrainingalgorithmthatallows\nustotrainobjectdetectorsonbothdetectionand\ntiondata.Ourmethodleverageslabeleddetectionimagesto\nlearntopreciselylocalizeobjectswhileituses\nimagestoincreaseitsvocabularyandrobustness.\nUsingthismethodwetrainYOLO9000,areal-timeob-\njectdetectorthatcandetectover9000differentobjectcat-\negories.FirstweimproveuponthebaseYOLOdetection\nsystemtoproduceYOLOv2,astate-of-the-art,real-time\ndetector.Thenweuseourdatasetcombinationmethod\nandjointtrainingalgorithmtotrainamodelonmorethan\n9000classesfromImageNetaswellasdetectiondatafrom\nCOCO.\nAllofourcodeandpre-trainedmodelsareavailableon-\nlineat\nhttp://pjreddie.com/yolo9000/\n.\n2.Better\nYOLOsuffersfromavarietyofshortcomingsrelativeto\nstate-of-the-artdetectionsystems.ErroranalysisofYOLO\ncomparedtoFastR-CNNshowsthatYOLOmakesasig-\nnumberoflocalizationerrors.Furthermore,YOLO\nhasrelativelylowrecallcomparedtoregionproposal-based\nmethods.Thuswefocusmainlyonimprovingrecalland\nlocalizationwhilemaintainingaccuracy.\nComputervisiongenerallytrendstowardslarger,deeper\nnetworks[\n6\n][\n18\n][\n17\n].Betterperformanceoftenhingeson\ntraininglargernetworksorensemblingmultiplemodelsto-\ngether.However,withYOLOv2wewantamoreaccurate\ndetectorthatisstillfast.Insteadofscalingupournetwork,\nwesimplifythenetworkandthenmaketherepresentation\neasiertolearn.Wepoolavarietyofideasfrompastwork\nwithourownnovelconceptstoimproveYOLO'sperfor-\nmance.AsummaryofresultscanbefoundinTable\n2\n.\nBatchNormalization.\nBatchnormalizationleadstosig-\nimprovementsinconvergencewhileeliminatingthe\nneedforotherformsofregularization[\n7\n].Byaddingbatch\nnormalizationonalloftheconvolutionallayersinYOLO\nwegetmorethan2%improvementinmAP.Batchnormal-\nizationalsohelpsregularizethemodel.Withbatchnor-\nmalizationwecanremovedropoutfromthemodelwithout\nov\nHighResolution.\nAllstate-of-the-artdetec-\ntionmethodsusepre-trainedonImageNet[\n16\n].\nStartingwithAlexNetmostoperateoninputim-\nagessmallerthan\n256\n\n256\n[\n8\n].TheoriginalYOLOtrains\nthenetworkat\n224\n\n224\nandincreasesthereso-\nlutionto\n448\nfordetection.Thismeansthenetworkhasto\nsimultaneouslyswitchtolearningobjectdetectionandad-\njusttothenewinputresolution.\nForYOLOv2wetunethetionnetwork\natthefull\n448\n\n448\nresolutionfor10epochsonImageNet.\nThisgivesthenetworktimetoadjustitstoworkbetter\nonhigherresolutioninput.Wethentunetheresulting\nnetworkondetection.Thishighresolution\nnetworkgivesusanincreaseofalmost4%mAP.\nConvolutionalWithAnchorBoxes.\nYOLOpredicts\nthecoordinatesofboundingboxesdirectlyusingfullycon-\nnectedlayersontopoftheconvolutionalfeatureextractor.\nInsteadofpredictingcoordinatesdirectlyFasterR-CNN\npredictsboundingboxesusinghand-pickedpriors[\n15\n].Us-\ningonlyconvolutionallayerstheregionproposalnetwork\n(RPN)inFasterR-CNNpredictsoffsetsandfor\nanchorboxes.Sincethepredictionlayerisconvolutional,\ntheRPNpredictstheseoffsetsateverylocationinafeature\nmap.Predictingoffsetsinsteadofcoordinatesthe\nproblemandmakesiteasierforthenetworktolearn.\nWeremovethefullyconnectedlayersfromYOLOand\nuseanchorboxestopredictboundingboxes.Firstwe\neliminateonepoolinglayertomaketheoutputofthenet-\nwork'sconvolutionallayershigherresolution.Wealso\nshrinkthenetworktooperateon\n416\ninputimagesinstead\nof\n448\n\n448\n.Wedothisbecausewewantanoddnumberof\nlocationsinourfeaturemapsothereisasinglecentercell.\nObjects,especiallylargeobjects,tendtooccupythecenter\noftheimagesoit'sgoodtohaveasinglelocationrightat\nthecentertopredicttheseobjectsinsteadoffourlocations\nthatareallnearby.YOLO'sconvolutionallayersdownsam-\npletheimagebyafactorof32sobyusinganinputimage\nof\n416\nwegetanoutputfeaturemapof\n13\n\n13\n.\nWhenwemovetoanchorboxeswealsodecouplethe\nclasspredictionmechanismfromthespatiallocationand\ninsteadpredictclassandobjectnessforeveryanchorbox.\nFollowingYOLO,theobjectnesspredictionstillpredicts\ntheIOUofthegroundtruthandtheproposedboxandthe\nclasspredictionspredicttheconditionalprobabilityofthat\nclassgiventhatthereisanobject.\nUsinganchorboxeswegetasmalldecreaseinaccuracy.\nYOLOonlypredicts98boxesperimagebutwithanchor\nboxesourmodelpredictsmorethanathousand.Without\nanchorboxesourintermediatemodelgets\n69\n:\n5\nmAPwitha\nrecallof\n81%\n.Withanchorboxesourmodelgets\n69\n:\n2\nmAP\nwitharecallof\n88%\n.EventhoughthemAPdecreases,the\nincreaseinrecallmeansthatourmodelhasmoreroomto\nimprove.\nDimensionClusters.\nWeencountertwoissueswithan-\nchorboxeswhenusingthemwithYOLO.Theisthat\ntheboxdimensionsarehandpicked.Thenetworkcanlearn\ntoadjusttheboxesappropriatelybutifwepickbetterpriors\nforthenetworktostartwithwecanmakeiteasierforthe\nnetworktolearntopredictgooddetections.\nInsteadofchoosingpriorsbyhand,werunk-means\nclusteringonthetrainingsetboundingboxestoautomat-\n"b"Figure2:\nClusteringboxdimensionsonVOCandCOCO.\nWe\nrunk-meansclusteringonthedimensionsofboundingboxestoget\ngoodpriorsforourmodel.TheleftimageshowstheaverageIOU\nwegetwithvariouschoicesfor\nk\n.Wethat\nk\n=5\ngivesagood\ntradeoffforrecallvs.complexityofthemodel.Therightimage\nshowstherelativecentroidsforVOCandCOCO.Bothsetsofpri-\norsfavorthinner,tallerboxeswhileCOCOhasgreatervariationin\nsizethanVOC.\nicallygoodpriors.Ifweusestandardk-meanswith\nEuclideandistancelargerboxesgeneratemoreerrorthan\nsmallerboxes.However,whatwereallywantarepriors\nthatleadtogoodIOUscores,whichisindependentofthe\nsizeofthebox.Thusforourdistancemetricweuse:\nd\n(\nbox\n;\ncentroid\n)=1\n\nIOU\n(\nbox\n;\ncentroid\n)\nWerunk-meansforvariousvaluesof\nk\nandplottheav-\nerageIOUwithclosestcentroid,seeFigure\n2\n.Wechoose\nk\n=5\nasagoodtradeoffbetweenmodelcomplexityand\nhighrecall.Theclustercentroidsaredifferent\nthanhand-pickedanchorboxes.Therearefewershort,wide\nboxesandmoretall,thinboxes.\nWecomparetheaverageIOUtoclosestpriorofourclus-\nteringstrategyandthehand-pickedanchorboxesinTable\n1\n.\nAtonly5priorsthecentroidsperformsimilarlyto9anchor\nboxeswithanaverageIOUof61.0comparedto60.9.If\nweuse9centroidsweseeamuchhigheraverageIOU.This\nindicatesthatusingk-meanstogenerateourboundingbox\nstartsthemodeloffwithabetterrepresentationandmakes\nthetaskeasiertolearn.\nBoxGeneration\n#\nAvgIOU\nClusterSSE558.7\nClusterIOU561.0\nAnchorBoxes[\n15\n]960.9\nClusterIOU967.2\nTable1:\nAverageIOUofboxestoclosestpriorsonVOC2007.\nTheaverageIOUofobjectsonVOC2007totheirclosest,unmod-\npriorusingdifferentgenerationmethods.Clusteringgives\nmuchbetterresultsthanusinghand-pickedpriors.\nDirectlocationprediction.\nWhenusinganchorboxes\nwithYOLOweencounterasecondissue:modelinstability,\nespeciallyduringearlyiterations.Mostoftheinstability\ncomesfrompredictingthe\n(\nx;y\n)\nlocationsforthebox.In\nregionproposalnetworksthenetworkpredictsvalues\nt\nx\nand\nt\ny\nandthe\n(\nx;y\n)\ncentercoordinatesarecalculatedas:\nx\n=(\nt\nx\n\nw\na\n)\n\nx\na\ny\n=(\nt\ny\n\nh\na\n)\n\ny\na\nForexample,apredictionof\nt\nx\n=1\nwouldshiftthebox\ntotherightbythewidthoftheanchorbox,apredictionof\nt\nx\n=\n\n1\nwouldshiftittotheleftbythesameamount.\nThisformulationisunconstrainedsoanyanchorboxcan\nendupatanypointintheimage,regardlessofwhatloca-\ntionpredictedthebox.Withrandominitializationthemodel\ntakesalongtimetostabilizetopredictingsensibleoffsets.\nInsteadofpredictingoffsetswefollowtheapproachof\nYOLOandpredictlocationcoordinatesrelativetotheloca-\ntionofthegridcell.Thisboundsthegroundtruthtofall\nbetween\n0\nand\n1\n.Weusealogisticactivationtoconstrain\nthenetwork'spredictionstofallinthisrange.\nThenetworkpredicts5boundingboxesateachcellin\ntheoutputfeaturemap.Thenetworkpredicts5coordinates\nforeachboundingbox,\nt\nx\n,\nt\ny\n,\nt\nw\n,\nt\nh\n,and\nt\no\n.Ifthecellis\noffsetfromthetopleftcorneroftheimageby\n(\nc\nx\n;c\ny\n)\nand\ntheboundingboxpriorhaswidthandheight\np\nw\n,\np\nh\n,then\nthepredictionscorrespondto:\nb\nx\n=\n\n(\nt\nx\n)+\nc\nx\nb\ny\n=\n\n(\nt\ny\n)+\nc\ny\nb\nw\n=\np\nw\ne\nt\nw\nb\nh\n=\np\nh\ne\nt\nh\nPr\n(\nobject\n)\n\nIOU\n(\nb;\nobject\n)=\n\n(\nt\no\n)\nSinceweconstrainthelocationpredictionthe\nparametrizationiseasiertolearn,makingthenetwork\nmorestable.Usingdimensionclustersalongwithdirectly\npredictingtheboundingboxcenterlocationimproves\nYOLObyalmost5%overtheversionwithanchorboxes.\nFine-GrainedFeatures.\nThisYOLOpredicts\ndetectionsona\n13\n\n13\nfeaturemap.Whilethisissuf\ncientforlargeobjects,itmayfromgrainedfea-\nturesforlocalizingsmallerobjects.FasterR-CNNandSSD\nbothruntheirproposalnetworksatvariousfeaturemapsin\nthenetworktogetarangeofresolutions.Wetakeadiffer-\nentapproach,simplyaddingapassthroughlayerthatbrings\nfeaturesfromanearlierlayerat\n26\n\n26\nresolution.\nThepassthroughlayerconcatenatesthehigherresolution\nfeatureswiththelowresolutionfeaturesbystackingadja-\ncentfeaturesintodifferentchannelsinsteadofspatiallo-\ncations,similartotheidentitymappingsinResNet.This\n"b"Figure3:\nBoundingboxeswithdimensionpriorsandlocation\nprediction.\nWepredictthewidthandheightoftheboxasoffsets\nfromclustercentroids.Wepredictthecentercoordinatesofthe\nboxrelativetothelocationofterapplicationusingasigmoid\nfunction.\nturnsthe\n26\n\n26\n\n512\nfeaturemapintoa\n13\n\n13\n\n2048\nfeaturemap,whichcanbeconcatenatedwiththeoriginal\nfeatures.Ourdetectorrunsontopofthisexpandedfeature\nmapsothatithasaccesstograinedfeatures.Thisgives\namodest1%performanceincrease.\nMulti-ScaleTraining.\nTheoriginalYOLOusesaninput\nresolutionof\n448\n\n448\n.Withtheadditionofanchorboxes\nwechangedtheresolutionto\n416\n\n416\n.However,sinceour\nmodelonlyusesconvolutionalandpoolinglayersitcanbe\nresizedonthe.WewantYOLOv2toberobusttorunning\nonimagesofdifferentsizessowetrainthisintothemodel.\nInsteadoftheinputimagesizewechangethenet-\nworkeveryfewiterations.Every10batchesournetwork\nrandomlychoosesanewimagedimensionsize.Sinceour\nmodeldownsamplesbyafactorof32,wepullfromthe\nfollowingmultiplesof32:\nf\n320\n;\n352\n;:::;\n608\ng\n.Thusthe\nsmallestoptionis\n320\n\n320\nandthelargestis\n608\n\n608\n.\nWeresizethenetworktothatdimensionandcontinuetrain-\ning.\nThisregimeforcesthenetworktolearntopredictwell\nacrossavarietyofinputdimensions.Thismeansthesame\nnetworkcanpredictdetectionsatdifferentresolutions.The\nnetworkrunsfasteratsmallersizessoYOLOv2offersan\neasytradeoffbetweenspeedandaccuracy.\nAtlowresolutionsYOLOv2operatesasacheap,fairly\naccuratedetector.At\n288\n\n288\nitrunsatmorethan90FPS\nwithmAPalmostasgoodasFastR-CNN.Thismakesit\nidealforsmallerGPUs,highframeratevideo,ormultiple\nvideostreams.\nAthighresolutionYOLOv2isastate-of-the-artdetector\nwith78.6mAPonVOC2007whilestilloperatingabove\nreal-timespeeds.SeeTable\n3\nforacomparisonofYOLOv2\nFigure4:\nAccuracyandspeedonVOC2007.\nwithotherframeworksonVOC2007.Figure\n4\nFurtherExperiments.\nWetrainYOLOv2fordetection\nonVOC2012.Table\n4\nshowsthecomparativeperformance\nofYOLOv2versusotherstate-of-the-artdetectionsystems.\nYOLOv2achieves73.4mAPwhilerunningfarfasterthan\ncompetingmethods.WealsotrainonCOCOandcompare\ntoothermethodsinTable\n5\n.OntheVOCmetric(IOU=\n.5)YOLOv2gets44.0mAP,comparabletoSSDandFaster\nR-CNN.\n3.Faster\nWewantdetectiontobeaccuratebutwealsowantittobe\nfast.Mostapplicationsfordetection,likeroboticsorself-\ndrivingcars,relyonlowlatencypredictions.Inorderto\nmaximizeperformancewedesignYOLOv2tobefastfrom\nthegroundup.\nMostdetectionframeworksrelyonVGG-16asthebase\nfeatureextractor[\n17\n].VGG-16isapowerful,accurateclas-\nnetworkbutitisneedlesslycomplex.Thecon-\nvolutionallayersofVGG-16require30.69billion\npointoperationsforasinglepassoverasingleimageat\n224\n\n224\nresolution.\nTheYOLOframeworkusesacustomnetworkbasedon\ntheGooglenetarchitecture[\n19\n].Thisnetworkisfasterthan\nVGG-16,onlyusing8.52billionoperationsforaforward\npass.However,it'saccuracyisslightlyworsethanVGG-\n16.Forsingle-crop,top-5accuracyat\n224\n\n224\n,YOLO's\ncustommodelgets88.0%ImageNetcomparedto90.0%for\nVGG-16.\nDarknet-19\n.Weproposeanewmodelto\nbeusedasthebaseofYOLOv2.Ourmodelbuildsoffof\npriorworkonnetworkdesignaswellascommonknowl-\nedgeintheSimilartotheVGGmodelsweusemostly\n3\n\n3\nanddoublethenumberofchannelsafterev-\nerypoolingstep[\n17\n].FollowingtheworkonNetworkin\nNetwork(NIN)weuseglobalaveragepoolingtomakepre-\n"b'YOLO\nYOLOv2\nbatchnorm?\nXXXXXXX\nX\nhi-res\nXXXXXX\nX\nconvolutional?\nXXXXX\nX\nanchorboxes?\nXX\nnewnetwork?\nXXXX\nX\ndimensionpriors?\nXXX\nX\nlocationprediction?\nXXX\nX\npassthrough?\nXX\nX\nmulti-scale?\nX\nX\nhi-resdetector?\nX\nVOC2007mAP\n63.4\n65.869.569.269.674.475.476.8\n78.6\nTable2:ThepathfromYOLOtoYOLOv2.\nMostofthelisteddesigndecisionsleadtoincreasesinmAP.Two\nexceptionsareswitchingtoafullyconvolutionalnetworkwithanchorboxesandusingthenewnetwork.Switchingtothe\nanchorboxstyleapproachincreasedrecallwithoutchangingmAPwhileusingthenewnetworkcutcomputationby33%.\nDetectionFrameworksTrainmAPFPS\nFastR-CNN[\n5\n]2007+201270.00.5\nFasterR-CNNVGG-16[\n15\n]2007+201273.27\nFasterR-CNNResNet[\n6\n]2007+201276.45\nYOLO[\n14\n]2007+201263.445\nSSD300[\n11\n]2007+201274.346\nSSD500[\n11\n]2007+201276.819\nYOLOv2\n288\n\n288\n2007+201269.091\nYOLOv2\n352\n\n352\n2007+201273.781\nYOLOv2\n416\n\n416\n2007+201276.867\nYOLOv2\n480\n\n480\n2007+201277.859\nYOLOv2\n544\n\n544\n2007+2012\n78.6\n40\nTable3:\nDetectionframeworksonP\nASCAL\nVOC2007.\nYOLOv2isfasterandmoreaccuratethanpriordetectionmeth-\nods.Itcanalsorunatdifferentresolutionsforaneasytradeoff\nbetweenspeedandaccuracy.EachYOLOv2entryisactuallythe\nsametrainedmodelwiththesameweights,justevaluatedatadif-\nferentsize.AlltiminginformationisonaGeforceGTXTitanX\n(original,notPascalmodel).\ndictionsaswellas\n1\n\n1\ntocompressthefeaturerep-\nresentationbetween\n3\n\n3\nconvolutions[\n9\n].Weusebatch\nnormalizationtostabilizetraining,speedupconvergence,\nandregularizethemodel[\n7\n].\nOurmodel,calledDarknet-19,has19convolutional\nlayersand5maxpoolinglayers.Forafulldescriptionsee\nTable\n6\n.Darknet-19onlyrequires5.58billionoperations\ntoprocessanimageyetachieves\n72\n:\n9%\ntop-1accuracyand\n91\n:\n2%\ntop-5accuracyonImageNet.\nTrainingfor\nWetrainthenetworkon\nthestandardImageNet1000classiondatasetfor\n160epochsusingstochasticgradientdescentwithastarting\nlearningrateof\n0\n:\n1\n,polynomialratedecaywithapowerof\n4\n,weightdecayof\n0\n:\n0005\nandmomentumof\n0\n:\n9\nusingthe\nDarknetneuralnetworkframework[\n13\n].Duringtraining\nweusestandarddataaugmentationtricksincludingrandom\ncrops,rotations,andhue,saturation,andexposureshifts.\nAsdiscussedabove,afterourinitialtrainingonimages\nat\n224\n\n224\nwetuneournetworkatalargersize,\n448\n.\nForthistuningwetrainwiththeaboveparametersbut\nforonly10epochsandstartingatalearningrateof\n10\n\n3\n.At\nthishigherresolutionournetworkachievesatop-1accuracy\nof\n76\n:\n5%\nandatop-5accuracyof\n93\n:\n3%\n.\nTrainingfordetection.\nWemodifythisnetworkforde-\ntectionbyremovingthelastconvolutionallayerandinstead\naddingonthree\n3\n\n3\nconvolutionallayerswith\n1024\n\nterseachfollowedbya\n1\n\n1\nconvolutionallayerwith\nthenumberofoutputsweneedfordetection.ForVOCwe\npredict5boxeswith5coordinateseachand20classesper\nboxso125Wealsoaddapassthroughlayerfromthe\n\n3\n\n3\n\n512\nlayertothesecondtolastconvolutional\nlayersothatourmodelcanusegrainfeatures.\nWetrainthenetworkfor160epochswithastarting\nlearningrateof\n10\n\n3\n,dividingitby10at60and90epochs.\nWeuseaweightdecayof\n0\n:\n0005\nandmomentumof\n0\n:\n9\n.\nWeuseasimilardataaugmentationtoYOLOandSSDwith\nrandomcrops,colorshifting,etc.Weusethesametraining\nstrategyonCOCOandVOC.\n4.Stronger\nWeproposeamechanismforjointlytrainingonclassi-\nanddetectiondata.Ourmethodusesimagesla-\nbelledfordetectiontolearninformation\nlikeboundingboxcoordinatepredictionandobjectnessas\nwellashowtoclassifycommonobjects.Itusesimageswith\nonlyclasslabelstoexpandthenumberofcategoriesitcan\ndetect.\nDuringtrainingwemiximagesfrombothdetectionand\ndatasets.Whenournetworkseesanimage\nlabelledfordetectionwecanbackpropagatebasedonthe\nfullYOLOv2lossfunction.Whenitseesa\nimageweonlybackpropagatelossfromthe\n'b'Method\ndata\nmAP\naerobikebirdboatbottlebuscarcatchaircowtabledoghorsembikepersonplantsheepsofatraintv\nFastR-CNN[\n5\n]\n07++12\n68.4\n82.378.470.852.338.777.871.689.344.273.055.087.580.580.872.035.168.365.780.464.2\nFasterR-CNN[\n15\n]\n07++12\n70.4\n84.979.874.353.949.877.575.988.545.677.155.386.981.780.979.640.172.660.981.261.5\nYOLO[\n14\n]\n07++12\n57.9\n77.067.257.738.322.768.355.981.436.260.848.577.272.371.363.528.952.254.873.950.8\nSSD300[\n11\n]\n07++12\n72.4\n85.680.170.557.646.279.476.189.253.077.060.887.083.182.379.445.975.969.581.967.5\nSSD512[\n11\n]\n07++12\n74.9\n87.482.375.859.052.681.781.590.055.479.059.888.484.384.783.350.278.066.386.372.0\nResNet[\n6\n]\n07++12\n73.8\n86.581.677.258.051.078.676.693.248.680.459.092.185.384.880.748.177.366.584.765.6\nYOLOv2\n544\n07++12\n73.4\n86.382.074.859.251.879.876.590.652.178.258.589.382.583.481.349.177.262.483.868.7\nTable4:PASCALVOC2012\ntest\ndetectionresults.\nYOLOv2performsonparwithstate-of-the-artdetectorslikeFaster\nR-CNNwithResNetandSSD512andis\n2\n\n10\n\nfaster.\n0.5:0.950.50.75\nSML\n110100\nSML\nFastR-CNN[\n5\n]\ntrain\n19.735.9-\n---\n---\n---\nFastR-CNN[\n1\n]\ntrain\n20.539.919.4\n4.120.035.8\n21.329.530.1\n7.332.152.0\nFasterR-CNN[\n15\n]\ntrainval\n21.942.7-\n---\n---\n---\nION[\n1\n]\ntrain\n23.643.223.6\n6.424.138.3\n23.232.733.5\n10.137.753.6\nFasterR-CNN[\n10\n]\ntrainval\n24.245.323.5\n7.726.437.1\n23.834.034.6\n12.038.554.4\nSSD300[\n11\n]\ntrainval35k\n23.241.223.4\n5.323.239.6\n22.533.235.3\n9.637.656.5\nSSD512[\n11\n]\ntrainval35k\n26.846.527.8\n9.028.941.9\n24.837.539.8\n14.043.559.0\nYOLOv2[\n11\n]\ntrainval35k\n21.644.019.2\n5.022.435.5\n20.731.633.3\n9.836.554.4\nTable5:ResultsonCOCO\ntest-dev2015\n.Tableadaptedfrom[\n11\n]\nType\nFilters\nSize/Stride\nOutput\nConvolutional\n32\n3\n\n3\n224\n\n224\nMaxpool\n2\n\n2\n=\n2\n112\n\n112\nConvolutional\n64\n3\n\n3\n112\n\n112\nMaxpool\n2\n\n2\n=\n2\n56\n\n56\nConvolutional\n128\n3\n\n3\n56\n\n56\nConvolutional\n64\n1\n\n1\n56\n\n56\nConvolutional\n128\n3\n\n3\n56\n\n56\nMaxpool\n2\n\n2\n=\n2\n28\n\n28\nConvolutional\n256\n3\n\n3\n28\n\n28\nConvolutional\n128\n1\n\n1\n28\n\n28\nConvolutional\n256\n3\n\n3\n28\n\n28\nMaxpool\n2\n\n2\n=\n2\n14\n\n14\nConvolutional\n512\n3\n\n3\n14\n\n14\nConvolutional\n256\n1\n\n1\n14\n\n14\nConvolutional\n512\n3\n\n3\n14\n\n14\nConvolutional\n256\n1\n\n1\n14\n\n14\nConvolutional\n512\n3\n\n3\n14\n\n14\nMaxpool\n2\n\n2\n=\n2\n7\n\n7\nConvolutional\n1024\n3\n\n3\n7\n\n7\nConvolutional\n512\n1\n\n1\n7\n\n7\nConvolutional\n1024\n3\n\n3\n7\n\n7\nConvolutional\n512\n1\n\n1\n7\n\n7\nConvolutional\n1024\n3\n\n3\n7\n\n7\nConvolutional\n1000\n1\n\n1\n7\n\n7\nAvgpool\nGlobal\n1000\nSoftmax\nTable6:\nDarknet-19.\npartsofthearchitecture.\nThisapproachpresentsafewchallenges.Detection\ndatasetshaveonlycommonobjectsandgenerallabels,like\ndogorboat.datasetshaveamuchwider\nanddeeperrangeoflabels.ImageNethasmorethanahun-\ndredbreedsofdog,includingNorfolkterrier,Yorkshire\nterrier,andBedlingtonterrier.Ifwewanttotrainon\nbothdatasetsweneedacoherentwaytomergetheselabels.\nMostapproachestouseasoftmaxlayer\nacrossallthepossiblecategoriestocomputethelprob-\nabilitydistribution.Usingasoftmaxassumestheclasses\naremutuallyexclusive.Thispresentsproblemsforcombin-\ningdatasets,forexampleyouwouldnotwanttocombine\nImageNetandCOCOusingthismodelbecausetheclasses\nNorfolkterrieranddogarenotmutuallyexclusive.\nWecouldinsteaduseamulti-labelmodeltocombinethe\ndatasetswhichdoesnotassumemutualexclusion.Thisap-\nproachignoresallthestructurewedoknowaboutthedata,\nforexamplethatalloftheCOCOclassesaremutuallyex-\nclusive.\nHierarchical\nImageNetlabelsarepulled\nfromWordNet,alanguagedatabasethatstructuresconcepts\nandhowtheyrelate[\n12\n].InWordNet,Norfolkterrierand\nYorkshireterrierarebothhyponymsofterrierwhichis\natypeofhuntingdog,whichisatypeofdog,whichis\nacanine,etc.Mostapproachestoclasassumea\nstructuretothelabelshoweverforcombiningdatasets,\nstructureisexactlywhatweneed.\nWordNetisstructuredasadirectedgraph,notatree,be-\ncauselanguageiscomplex.Forexampleadogisboth\natypeofcanineandatypeofdomesticanimalwhich\narebothsynsetsinWordNet.Insteadofusingthefullgraph\nstructure,wesimplifytheproblembybuildingahierarchi-\ncaltreefromtheconceptsinImageNet.\nTobuildthistreeweexaminethevisualnounsinIma-\ngeNetandlookattheirpathsthroughtheWordNetgraphto\ntherootnode,inthiscasephysicalobject.Manysynsets\nonlyhaveonepaththroughthegraphsoweaddallof\nthosepathstoourtree.Thenweiterativelyexaminethe\nconceptswehaveleftandaddthepathsthatgrowthetree\nbyaslittleaspossible.Soifaconcepthastwopathstothe\nrootandonepathwouldaddthreeedgestoourtreeandthe\notherwouldonlyaddoneedge,wechoosetheshorterpath.\n'b"TheresultisWordTree,ahierarchicalmodelofvi-\nsualconcepts.ToperformwithWordTreewe\npredictconditionalprobabilitiesateverynodefortheprob-\nabilityofeachhyponymofthatsynsetgiventhatsynset.For\nexample,attheterriernodewepredict:\nPr\n(\nNorfolkterrier\nj\nterrier\n)\nPr\n(\nYorkshireterrier\nj\nterrier\n)\nPr\n(\nBedlingtonterrier\nj\nterrier\n)\n:::\nIfwewanttocomputetheabsoluteprobabilityforapar-\nticularnodewesimplyfollowthepaththroughthetreeto\ntherootnodeandmultiplytoconditionalprobabilities.So\nifwewanttoknowifapictureisofaNorfolkterrierwe\ncompute:\nPr\n(\nNorfolkterrier\n)=\nPr\n(\nNorfolkterrier\nj\nterrier\n)\n\nPr\n(\nterrier\nj\nhuntingdog\n)\n\n:::\n\n\nPr\n(\nmammal\nj\nPr\n(\nanimal\n)\n\nPr\n(\nanimal\nj\nphysicalobject\n)\nForpurposesweassumethatthetheimage\ncontainsanobject:\nPr\n(\nphysicalobject\n)=1\n.\nTovalidatethisapproachwetraintheDarknet-19model\nonWordTreebuiltusingthe1000classImageNet.Tobuild\nWordTree1kweaddinalloftheintermediatenodeswhich\nexpandsthelabelspacefrom1000to1369.Duringtraining\nwepropagategroundtruthlabelsupthetreesothatifanim-\nageislabelledasaNorfolkterrieritalsogetslabelledas\nadogandamammal,etc.Tocomputetheconditional\nprobabilitiesourmodelpredictsavectorof1369valuesand\nwecomputethesoftmaxoverallsysnsetsthatarehyponyms\nofthesameconcept,seeFigure\n5\n.\nUsingthesametrainingparametersasbefore,ourhi-\nerarchicalDarknet-19achieves\n71\n:\n9%\ntop-1accuracyand\n90\n:\n4%\ntop-5accuracy.Despiteadding369additionalcon-\nceptsandhavingournetworkpredictatreestructureourac-\ncuracyonlydropsmarginally.Performingin\nthismanneralsohassomePerformancedegrades\ngracefullyonneworunknownobjectcategories.Forexam-\nple,ifthenetworkseesapictureofadogbutisuncertain\nwhattypeofdogitis,itwillstillpredictdogwithhigh\nbuthavelowerspreadoutamong\nthehyponyms.\nThisformulationalsoworksfordetection.Now,in-\nsteadofassumingeveryimagehasanobject,weuse\nYOLOv2'sobjectnesspredictortogiveusthevalueof\nPr\n(\nphysicalobject\n)\n.Thedetectorpredictsaboundingbox\nFigure5:\nPredictiononImageNetvsWordTree.\nMostIma-\ngeNetmodelsuseonelargesoftmaxtopredictaprobabilitydistri-\nbution.UsingWordTreeweperformmultiplesoftmaxoperations\noverco-hyponyms.\nandthetreeofprobabilities.Wetraversethetreedown,tak-\ningthehighestpathateverysplituntilwereach\nsomethresholdandwepredictthatobjectclass.\nDatasetcombinationwithWordTree.\nWecanuse\nWordTreetocombinemultipledatasetstogetherinasen-\nsiblefashion.Wesimplymapthecategoriesinthedatasets\ntosynsetsinthetree.Figure\n6\nshowsanexampleofusing\nWordTreetocombinethelabelsfromImageNetandCOCO.\nWordNetisextremelydiversesowecanusethistechnique\nwithmostdatasets.\nJointanddetection.\nNowthatwecan\ncombinedatasetsusingWordTreewecantrainourjoint\nmodelonanddetection.Wewanttotrain\nanextremelylargescaledetectorsowecreateourcom-\nbineddatasetusingtheCOCOdetectiondatasetandthe\ntop9000classesfromthefullImageNetrelease.Wealso\nneedtoevaluateourmethodsoweaddinanyclassesfrom\ntheImageNetdetectionchallengethatwerenotalreadyin-\ncluded.ThecorrespondingWordTreeforthisdatasethas\n9418classes.ImageNetisamuchlargerdatasetsowebal-\nancethedatasetbyoversamplingCOCOsothatImageNet\nisonlylargerbyafactorof4:1.\nUsingthisdatasetwetrainYOLO9000.Weusethebase\nYOLOv2architecturebutonly3priorsinsteadof5tolimit\ntheoutputsize.Whenournetworkseesadetectionimage\nwebackpropagatelossasnormal.Forloss,we\nonlybackpropagatelossatorabovethecorrespondinglevel\nofthelabel.Forexample,ifthelabelisdogwedoassign\nanyerrortopredictionsfurtherdowninthetree,German\nShepherdversusGoldenRetriever,becausewedonot\nhavethatinformation.\n"b"Figure6:\nCombiningdatasetsusingWordTreehierarchy.\nUs-\ningtheWordNetconceptgraphwebuildahierarchicaltreeofvi-\nsualconcepts.Thenwecanmergedatasetstogetherbymapping\ntheclassesinthedatasettosynsetsinthetree.Thisisas\nviewofWordTreeforillustrationpurposes.\nWhenitseesaimageweonlybackpropa-\ngateloss.Todothiswesimplythebound-\ningboxthatpredictsthehighestprobabilityforthatclass\nandwecomputethelossonjustitspredictedtree.Wealso\nassumethatthepredictedboxoverlapswhatwouldbethe\ngroundtruthlabelbyatleast\n:\n3\nIOUandwebackpropagate\nobjectnesslossbasedonthisassumption.\nUsingthisjointtraining,YOLO9000learnstoob-\njectsinimagesusingthedetectiondatainCOCOandit\nlearnstoclassifyawidevarietyoftheseobjectsusingdata\nfromImageNet.\nWeevaluateYOLO9000ontheImageNetdetectiontask.\nThedetectiontaskforImageNetshareson44objectcate-\ngorieswithCOCOwhichmeansthatYOLO9000hasonly\nseendataforthemajorityofthetestimages,\nnotdetectiondata.YOLO9000gets19.7mAPoverallwith\n16.0mAPonthedisjoint156objectclassesthatithasnever\nseenanylabelleddetectiondatafor.ThismAPishigher\nthanresultsachievedbyDPMbutYOLO9000istrainedon\ndifferentdatasetswithonlypartialsupervision[\n4\n].Italso\nissimultaneouslydetecting9000otherobjectcategories,all\ninreal-time.\nWhenweanalyzeYOLO9000'sperformanceonIma-\ngeNetweseeitlearnsnewspeciesofanimalswellbutstrug-\ngleswithlearningcategorieslikeclothingandequipment.\ndiaper0.0\nhorizontalbar0.0\nrubbereraser0.0\nsunglasses0.0\nswimmingtrunks0.0\n...\nredpanda50.7\nfox52.1\nkoalabear54.3\ntiger61.0\narmadillo61.7\nTable7:\nYOLO9000BestandWorstClassesonImageNet.\nTheclasseswiththehighestandlowestAPfromthe156weakly\nsupervisedclasses.YOLO9000learnsgoodmodelsforavarietyof\nanimalsbutstruggleswithnewclasseslikeclothingorequipment.\nNewanimalsareeasiertolearnbecausetheobjectnesspre-\ndictionsgeneralizewellfromtheanimalsinCOCO.Con-\nversely,COCOdoesnothaveboundingboxlabelforany\ntypeofclothing,onlyforperson,soYOLO9000strugglesto\nmodelcategorieslikesunglassesorswimmingtrunks.\n5.Conclusion\nWeintroduceYOLOv2andYOLO9000,real-timede-\ntectionsystems.YOLOv2isstate-of-the-artandfaster\nthanotherdetectionsystemsacrossavarietyofdetection\ndatasets.Furthermore,itcanberunatavarietyofimage\nsizestoprovideasmoothtradeoffbetweenspeedandaccu-\nracy.\nYOLO9000isareal-timeframeworkfordetectionmore\nthan9000objectcategoriesbyjointlyoptimizingdetection\nandWeuseWordTreetocombinedatafrom\nvarioussourcesandourjointoptimizationtechniquetotrain\nsimultaneouslyonImageNetandCOCO.YOLO9000isa\nstrongsteptowardsclosingthedatasetsizegapbetweende-\ntectionand\nManyofourtechniquesgeneralizeoutsideofobjectde-\ntection.OurWordTreerepresentationofImageNetoffersa\nricher,moredetailedoutputspaceforimage\nDatasetcombinationusinghierarchicalwould\nbeusefulintheandsegmentationdomains.\nTrainingtechniqueslikemulti-scaletrainingcouldprovide\nacrossavarietyofvisualtasks.\nForfutureworkwehopetousesimilartechniquesfor\nweaklysupervisedimagesegmentation.Wealsoplanto\nimproveourdetectionresultsusingmorepowerfulmatch-\ningstrategiesforassigningweaklabelstodata\nduringtraining.Computervisionisblessedwithanenor-\nmousamountoflabelleddata.Wewillcontinuelooking\nforwaystobringdifferentsourcesandstructuresofdata\ntogethertomakestrongermodelsofthevisualworld.\n"b'References\n[1]\nS.Bell,C.L.Zitnick,K.Bala,andR.Girshick.Inside-\noutsidenet:Detectingobjectsincontextwithskip\npoolingandrecurrentneuralnetworks.\narXivpreprint\narXiv:1512.04143\n,2015.\n6\n[2]\nJ.Deng,W.Dong,R.Socher,L.-J.Li,K.Li,andL.Fei-\nFei.Imagenet:Alarge-scalehierarchicalimagedatabase.\nIn\nComputerVisionandPatternRecognition,2009.CVPR\n2009.IEEEConferenceon\n,pages248255.IEEE,2009.\n1\n[3]\nM.Everingham,L.VanGool,C.K.Williams,J.Winn,and\nA.Zisserman.Thepascalvisualobjectclasses(voc)chal-\nlenge.\nInternationaljournalofcomputervision\n,88(2):303\n338,2010.\n1\n[4]\nP.F.Felzenszwalb,R.B.Girshick,andD.McAllester.\nDiscriminativelytraineddeformablepartmodels,release4.\nhttp://people.cs.uchicago.edu/pff/latent-release4/.\n8\n[5]\nR.B.Girshick.FastR-CNN.\nCoRR\n,abs/1504.08083,2015.\n5\n,\n6\n[6]\nK.He,X.Zhang,S.Ren,andJ.Sun.Deepresiduallearn-\ningforimagerecognition.\narXivpreprintarXiv:1512.03385\n,\n2015.\n2\n,\n5\n,\n6\n[7]\nS.IoffeandC.Szegedy.Batchnormalization:Accelerating\ndeepnetworktrainingbyreducinginternalcovariateshift.\narXivpreprintarXiv:1502.03167\n,2015.\n2\n,\n5\n[8]\nA.Krizhevsky,I.Sutskever,andG.E.Hinton.Imagenet\nwithdeepconvolutionalneuralnetworks.In\nAdvancesinneuralinformationprocessingsystems\n,pages\n10971105,2012.\n2\n[9]\nM.Lin,Q.Chen,andS.Yan.Networkinnetwork.\narXiv\npreprintarXiv:1312.4400\n,2013.\n5\n[10]\nT.-Y.Lin,M.Maire,S.Belongie,J.Hays,P.Perona,D.Ra-\nmanan,P.Doll\n\nar,andC.L.Zitnick.Microsoftcoco:Com-\nmonobjectsincontext.In\nEuropeanConferenceonCom-\nputerVision\n,pages740755.Springer,2014.\n1\n,\n6\n[11]\nW.Liu,D.Anguelov,D.Erhan,C.Szegedy,andS.E.Reed.\nSSD:singleshotmultiboxdetector.\nCoRR\n,abs/1512.02325,\n2015.\n5\n,\n6\n[12]\nG.A.Miller,R.Beckwith,C.Fellbaum,D.Gross,andK.J.\nMiller.Introductiontowordnet:Anon-linelexicaldatabase.\nInternationaljournaloflexicography\n,3(4):235244,1990.\n6\n[13]\nJ.Redmon.Darknet:Opensourceneuralnetworksinc.\nhttp://pjreddie.com/darknet/\n,20132016.\n5\n[14]\nJ.Redmon,S.Divvala,R.Girshick,andA.Farhadi.You\nonlylookonce:real-timeobjectdetection.\narXiv\npreprintarXiv:1506.02640\n,2015.\n5\n,\n6\n[15]\nS.Ren,K.He,R.Girshick,andJ.Sun.Fasterr-cnn:To-\nwardsreal-timeobjectdetectionwithregionproposalnet-\nworks.\narXivpreprintarXiv:1506.01497\n,2015.\n2\n,\n3\n,\n5\n,\n6\n[16]\nO.Russakovsky,J.Deng,H.Su,J.Krause,S.Satheesh,\nS.Ma,Z.Huang,A.Karpathy,A.Khosla,M.Bernstein,\nA.C.Berg,andL.Fei-Fei.ImageNetLargeScaleVisual\nRecognitionChallenge.\nInternationalJournalofComputer\nVision(IJCV)\n,2015.\n2\n[17]\nK.SimonyanandA.Zisserman.Verydeepconvolutional\nnetworksforlarge-scaleimagerecognition.\narXivpreprint\narXiv:1409.1556\n,2014.\n2\n,\n4\n[18]\nC.Szegedy,S.Ioffe,andV.Vanhoucke.Inception-v4,\ninception-resnetandtheimpactofresidualconnectionson\nlearning.\nCoRR\n,abs/1602.07261,2016.\n2\n[19]\nC.Szegedy,W.Liu,Y.Jia,P.Sermanet,S.Reed,\nD.Anguelov,D.Erhan,V.Vanhoucke,andA.Rabinovich.\nGoingdeeperwithconvolutions.\nCoRR\n,abs/1409.4842,\n2014.\n4\n[20]\nB.Thomee,D.A.Shamma,G.Friedland,B.Elizalde,K.Ni,\nD.Poland,D.Borth,andL.-J.Li.Yfcc100m:Thenew\ndatainmultimediaresearch.\nCommunicationsoftheACM\n,\n59(2):6473,2016.\n1\n'