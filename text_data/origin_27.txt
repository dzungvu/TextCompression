b'DIGITALVISION\nIEEE SIGNAL PROCESSING MAGAZINE[48]MARCH 20081053-5888/08/$25.002008IEEERecent results in compressive sampling have shown that sparse signals can berecovered from a small number of random measurements. This property\nraises the question of whether random measurements can provide an effi-\ncient representation of sparse signals in an information-theoretic sense.\nThrough both theoretical and experimental results, we show that encoding asparse signal through simple scalar quantization of random measurements incurs a sig-\nnificant penalty relative to direct or adaptive encoding of the sparse signal. Information\ntheory provides alternative quantization strategies, but they come at the cost of much\ngreater estimation complexity.\n[Vivek K Goyal, \nAlyson K. Fletcher, and \n\nSundeep Rangan]Compressive Sampling\nand Lossy Compression \n[Do random measurements provide an efficient \nmethod of representing sparse signals?\n]Digital Object Identifier 10.1109/MSP.2007.915001\n'b'IEEE SIGNAL PROCESSING MAGAZINE[49]MARCH 2008BACKGROUNDSPARSE SIGNALS\nSince the 1990s, modeling signals through sparsity has emerged\nas an important and widely applicable technique in signal pro-\ncessing. Its most well-known success is in image processing,\nwhere great advances in compression and estimation have come\nfrom modeling images as sparse in a wavelet domain [1].In this article we use a simple, abstract model for sparsesignals. Consider an N-dimensional vector xthat can be repre-sented as x=Vu, where Vis some orthogonal N-by-Nmatrixand uRNhas only Knonzero entries. In this case, we saythat uis K-sparseand that xis K-sparse with respect toV. Theset of positions of nonzeros coefficients in uis called the spar-sity pattern, and we call =K/Nthe sparsity ratio.Knowing that xis K-sparse with respect to a given basis Vcan be extremely valuable for signal processing. For example, incompression, xcan be represented by the Kpositions and valuesof the nonzero elements in u, as opposed to the \nNelements of x.When the sparsity ratio is small, the compression gain can besignificant. Similarly, in estimating \nxin the presence of noise,one only has to estimate Kas opposed to Nreal parameters.Another important property of sparse signals has recently beenuncovered: they can be recovered in a computationally tractable\nmanner from a relatively small number of random samples. The\nmethod, known as compressive sampling(sometimes called com-pressed sensingor compressive sensing), was developed in [2], [3]and [4]and is detailed in other articles in this issue.\nA basic model for compressive sampling is shown in Figure 1.The N-dimensional signal \nxis assumed to be K-sparse with\nrespect to some orthogonal matrix V. The sampling of xis rep-resented as a linear transformation by a matrix yielding a sam-ple vector y=x. Let the size of be M-by-N, so yhas Melements; we call each element of ya measurement of x. Adecoder must recover the signal xfrom yknowing Vand , butnot necessarily the sparsity pattern of the unknown signal u.Since uis K-sparse, xmust belong to one of NKsubspaces inRN. Similarly, \nymust belong to one of NKsubspaces in RM.For almost all s with MK+1, an exhaustive searchthrough the subspaces can determine which subspace xbelongsto and thereby recover the signals sparsity pattern and values.\n\nTherefore, in principle, a Ksparse signal can be recovered fromas few as M=K+1random samples.Unfortunately, the exhaustive search described above is not\ntractable for interesting sizes of problems since the number of\nsubspaces to search, NK, can be enormous; if is held constantas Nis increased, the number of subspaces grows exponentiallywith N. The remarkable main result of compressive sampling isto exhibit recovery methods that are computationally feasible,\nnumerically stable, and robust against noise while requiring a\nnumber of measurements not much larger than K.SIGNAL RECOVERY WITH COMPRESSIVE SAMPLING\nCompressive sampling is based on recovering xvia convex opti-mization. When we observe y=xand xis sparse with respectto V, we are seeking xconsistent with yand such that V1xhasfew nonzero entries. To try to minimize the number of nonzero\n\nentries directly yields an intractable problem [5]. Instead, solv-\ning the optimization problem(LPreconstruction)\nxLP=argmin\nx:y=xV1x1often gives exactly the desired signal recovery, and there are\nsimple conditions that guarantee exact recovery. Following pio-\n\nneering work by Logan in the 1960s, Donoho and Stark [6]\nobtained results that apply, for example, when \nVis the N-by-Nidentity matrix and the rows of are taken from the matrix rep-resentation of the length-Ndiscrete Fourier transform (DFT).Subsequent works considered randomly selected rows from the\nDFT matrix [2]and then certain other random matrix ensem-\n\nbles [3], [4]. In this article, we will concentrate on the case whenhas independent Gaussian entries.A central question is: How many measurements Mare need-ed for linear program (LP) reconstruction to be successful?\n\nSince is random, there is always a chance that reconstructionwill fail. We are interested in how \nMshould scale with signaldimension Nand sparsity Kso that the probability of successapproaches one. A result of Donoho and Tanner [7]indicates\n\nthat M2Klog(N/K)is a sharp threshold for successfulrecovery. Compared to the intractable exhaustive search\n\nthrough all possible subspaces, LP recovery requires only a fac-\ntor 2log\n(N/K)more measurements.If measurements are subject to additive Gaussian noise so\nthaty=x+is observed, with \nN(02), then the LPreconstruction should be adjusted to allow slack in the con-\nstraint y=x. A typical method for reconstruction is the fol-lowing convex optimization:(Lassoreconstruction)\nxLasso=argmin\nxyx22+V1x1,where the parameter 0trades off data fidelity and reconstruc-tion sparsity. The best choice for \ndepends on the variance of the[FIG1]Block diagram representation of compressive sampling.\nThe signal xis sparse with respect to \nV, meaning that u=V1xhas only a few nonzero entries. \ny=xis compressed in that it\nis shorter than x. (White boxes represent zero elements.)\nuxy\nVEstimationx'b'noise and problem size parameters.Wainwright [8]has shown that the scaling\nM2Klog(NK)+Kis a sharpthreshold for V1xLassoto have the correctsparsity pattern with high probability. While\n\nthis Mmay be much smaller than N, it is sig-nificantly more measurements than required\nin the noiseless case.COMPRESSIVE SAMPLINGAS SOURCE CODING\nIn the remainder of this article, we will be\nconcerned with the tradeoff between quality\nof approximation and the number of bits of\nstorage for a signal xthat is K-sparse withrespect to orthonormal basis V. An immediate distinction fromthe Background section is that the currency in which we\ndenominate the cost of a representation is bits rather than real\ncoefficients.In any compression involving scalar quantization, the choiceof coordinates is key. Traditionally, signals to be compressed are\nmodeled as jointly Gaussian vectors. These vectors can be visu-alized as lying in an ellipsoid, since this is the shape of the level\ncurves of their probability density [see Figure 2(a)]. Source cod-\ning theory for jointly Gaussian vectors suggests to choose\northogonal coordinates aligned with the principal axes of the\nellipsoid (the KarhunenLo`eve basis) and then allocate bits tothe dimensions based on their variances. This gives a codinggainrelative to arbitrary coordinates [9]. For high-quality (lowdistortion) coding, the coding gain is a constant number of bits\nper dimension that depends on the eccentricity of the ellipse.Sparse signal models are geometrically quite different thanjointly Gaussian vector models. Instead of being visualized as\nellipses, they yield unions of subspaces [see Figure 2(b)]. A nat-\nural encoding method for a signal \nxthat is K-sparse withrespect to Vis to identify the subspace containing xand thenquantize within the subspace, spending a number of bits pro-\nportional to K. Note that doing this requires the encoder to\nknow Vand that there is a cost to communicating the subspaceindex, denoted J, that will be detailed later. With all the proper\naccounting, when KN, the savings is more dramatic thanjust a constant number of bits.Following the compressive sampling framework one obtainsa rather different way to compress x:quantize the measurements y=x,with and Vknown to the decoder.\nSince spreads the energy of thesignal uniformly across the measure-\nments, each measurement should be\nallocated the same number of bits.\nThe decoder should estimate xas wellas it can; we will not limit the com-\n\nputational capability of the decoder.\nHow well will compressive sam-pling work? It depends both on how\nmuch it matters to use the best basis\n(V) rather than a set of random vec-tors () and how much the quantiza-tion of yaffects the ability of thedecoder to infer the correct subspace.\nWe separate these issues, and our\n[FIG2](a) Depiction of Gaussian random vectors as an ellipsoid.Classical rate-distortion theory and transform coding results are\n\nfor this sort of source, which serves as a good model for discrete\n\ncosine transform (DCT) coefficients of an image or MDCT\n\ncoefficients of audio. (b) Depiction of two sparse signals in \nR3,which form a union of three subspaces. This serves as a good\n\nconceptual model for wavelet coefficients of images.\n(a)(b)\n[FIG3]Block diagram representation of the compressive sampling scenario analyzed\ninformation theoretically. \nVis a random orthogonal matrix, uis a K-sparse vector withN(0,1)nonzero entries, and \nis a Gaussian measurement matrix. More specifically, the\nsparsity pattern of \nuis represented by \nJand the nonzero entries are denoted \nuK. In the initialanalysis, the encoding of y=xis by scalar quantization and scalar entropy coding.\nSourceof\nRandomnessVJuKuVxyEncoder\nBitsDecoderxyQ()Entropy\nCoding\nBits\nIEEE SIGNAL PROCESSING MAGAZINE[50]MARCH 2008ENCODER(SPARSIFYING (RANDOM \nBASIS) USES VMEASUREMENTS) USES KNOWS Jc22Rc22(RR)A PRIORIDECODERIS TOLD \nJc22(RH)\nc22(RH\nR)INFERS JclogN)22R[TABLE 1]  PERFORMANCE SUMMARY: DISTORTIONS FOR SEVERAL SCENARIOS\nWHEN NIS LARGE WITH =K/NHELD CONSTANT. RATE \nRANDDISTORTION \nDARE BOTH NORMALIZED BY K. JREPRESENTS THESPARSITY PATTERN OF \nu. THE BOXED RED ENTRY IS A HEURISTIC\nANALYSIS OF THE COMPRESSIVE SAMPLING CASE. \nH()REPRESENTSTHE BINARY ENTROPY FUNCTION AND THE ROTATIONAL LOSS \nRSATISFIES \nR=O(logR).'b'results are previewed and summa-rized in Table 1. We will derive the\n\nresults in blue and then the result in\nred, which requires much more\nexplanation. But first we establish the\nsetting more concretely.\nMODELING ASSUMPTIONSTo reflect the concept that the orthonormal basis \nVis not used inthe sensor/encoder, we model \nVas random and available only atthe estimator/decoder. It is chosen uniformly at random from the\n\nset of orthogonal matrices. The source vector xis also random;to model it as K-sparse with respect to V, we let x=VuwhereuRNhas Knonzero entries in positions chosen uniformly atrandom. As depicted in Figure 3, we denote the nonzero entries\n\nof uby uKRKand let the discrete random variable Jrepresentthe sparsity pattern. Note that both Vand can be consideredside information available at the decoder but not at the encoder.\nLet the components of uKbe independent and GaussianN(0,1). Observe that E[u2]=K, and since Vis orthogonal wealso have E[x2]=K. For the measurement matrix , let theentries be independent N(0,1/K)and independent of Vand u.This normalization makes the entries of yeach have unit variance.Let us now establish some notation to describe scalarquantization. When scalar yiis quantized to yield yi, it is con-venient to define the relative quantization error=E[|yiyi|2]/E[|yi|2]and then further define =1and vi=yiyi. These definitions yield a gain-plus-noisenotation yi=yi+vi, where2v=E[|vi|2]=1E[|yi|2],(1)to describe the effect of quantization. Quantizers with optimal(centroid) decoders result in vbeing uncorrelated with y[10,Lemma 5.1]; other precise justifications are also possible [11].In subsequent analyses, we will want to relate to the rate(number of bits) of the quantizer. The exact value of \ndependsnot only on the rate Rbut also on the distribution of yiand theparticular quantization method. However, the scaling of \nwithRis as 22Runder many different scenarios (see Quantizer\nPerformance and Quantization Error). We will write\n=c22R(2)without repeatedly specifying the constant c1.With the established notation, the overall quantizer outputvector can be written asy=Vu+v=Au+v,(3)where A=V. The overall source coding and decodingprocess, with the gain-plus-noise representation for quantiza-\ntion, is depicted in Figure 4. Our use of (3) is to enable easy\n\nanalysis of linear estimation of xfrom y.QUANTIZER PERFORMANCE AND QUANTIZATION ERROR\nA quantity that takes on uncountably many valueslike a realnumbercannot have an exact digital representation. Thus digi-\ntal processing always involves quantized values. The relation-\nships between the number of bits in a representation (rate R),the accuracy of a representation (distortion D), and properties ofquantization error are central to this article and are developed\nin this sidebar.\nThe simplest form of quantizationuniform scalar quan-tizationis to round xRto the nearest multiple of somefixed resolution parameter to obtain quantized versionx. For this type of quantizer, rate and distortion can be eas-\nily related through the step size . Suppose xhas a smoothdistribution over an interval of length C. Then the quantiz-er produces about Cintervals, which can be indexedwith Rlog2(Cb. The error xxis approximately uni-formly distributed over [2/\n2], so the mean-squarederror is D=E[(xx)2](1/122. Eliminating , weobtain D(1/12)C222R.The 22Rdependence on rate is fundamental for compressionwith respect to MSE distortion. For any distribution of x, the bestpossible distortion as a function of rate (obtained with high-\ndimensional vector quantization [25]) satisfies(2e)122h22RD(R)222R,where hand 2are the differential entropy and variance of \nx.Also, under high resolution assumptions and with entropy cod-\n\ning, D(R)(1/12)22h22Rperformance is obtained with uni-form scalar quantization, which for a Gaussian random variable\nis D(R)(1/6e222R. Covering all of these variationstogether, we write the performance as \nD(R)=c222Rwithoutspecifying the constant c.More subtle is to understand the quantization error e=xx.With uniform scalar quantization, eis in the interval[2/\n2], and it is convenient to think of it as a uniform ran-dom variable over this interval, independent of x.  This is merely\na convenient fiction, since xis a deterministic function of x.  In\nfact, as long as quantizers are regular and estimation procedures\nuse linear combinations of many quantized values, second-order\nstatistics (which are well understood [11]) are sufficient for\n\nunderstanding estimation performance. When xis Gaussian, arather counterintuitive model where eis Gaussian and inde-pendent of xcan be justified precisely: optimal quantization of alarge block of samples is described by the optimal test channel,which is additive Gaussian [28].IEEE SIGNAL PROCESSING MAGAZINE[51]MARCH 2008[FIG4]Source coding of \nxwith additive noise representation for quantization.\nuVx=VuMeasurementy=xQuantizationy=y+vDecodingx'b'ANALYSES\nSince the sparsity level Kis the inherent number of degrees offreedom in the signal, we will let there be KRbits available forthe encoding of xand also normalize the distortion byK:D=(1/K)E[xx2]. Where applicable, the number ofmeasurements Mis a design parameter that can optimized togive the best distortion-rate tradeoff. In particular, increasing \nMgives better conditioning of certain matrices, but it reduces thenumber of quantization bits per measurement.Before analyzing the compressive sampling scenario(Figure 3), we consider some simpler alternatives, yielding the\nblue entries in Table 1.\nSIGNAL IN A KNOWN SUBSPACE\nIf the sparsifying basis Vand subspace Jare fixed and known toboth encoder and decoder, the communication of \nxcan beaccomplished by sending quantized versions of the nonzero\n\nentries of V1x. Each of the Knonzero entries has unitvariance and is allotted Rb, so D(R)=c22Rperformance isobtained, as given by the first entry in Table 1.\nADAPTIVE ENCODING WITH COMMUNICATION OF \nJNow suppose that Vis known to both encoder and decoder, but\nthe subspace index Jis random, uniformly selected from the NKpossibilities. A natural adaptiveapproach is to spend log2NKbitsto communicate Jand the remaining available bits to quantizethe nonzero entries of V1x. Defining R0=(1/K)log2NK, theencoder has KRKR0b for the Knonzero entries of V1xandthus attains performanceDadaptive=c22(RR0),RR0.(4)When Kand Nare large with the ratio =K/Nheld constant,log2NKNH)\nwhere H(p)=plog2p(1p)log2(1p)is the binary entropy function[12, p. 530]. ThusR0H\n, giving a second entry in Table 1.\nIf Rdoes not exceed R0, then the derivation above doesnot make sense, and even if Rexceeds R0by a small amount,it may not pay to communicate J. A directapproach is tosimply quantize each component of xwith KR/Nb. Sincethe components of xhave variance K/N, performance ofE[(xixi)2]c(K/N)22KR/Ncan be obtained, yieldingoverall performanceDdirect(R)=c22KR/N.(5)By choosing the better between (4) and (5) for a given rate, oneobtains a simple baseline for the performance using Vat theencoder. A convexification by time sharing could also be applied,\n\nand more sophisticated techniques are presented in [13].LOSS FROM RANDOM MEASUREMENTSNow let us try to understand in isolation the effect of observingxonly through x. The encoder sends a quantized version ofy=x, and the decoder knows Vand the sparsity pattern J.From (3), the decoder has y=Vu+vand knows whichKelements of uare nonzero. The performance of a linear esti-mate of the form x=F(J)ywill depend on the singular valuesof the M-by-Kmatrix formed by the Krelevant columns of V.(One should expect a small improvementroughly a multiplica-\ntion of the distortion by K/Mfrom the use of a nonlinear esti-mate that exploits boundedness of quantization noise [14], [15].\nThe dependence on Vis roughly unchanged [16].) Using ele-mentary results from random matrix theory, one can find how\n\nthe distortion varies with Mand R. (The distortion does notdepend on Nbecause the zero components of uare known.) Theanalysis given in [17]shows that for moderate to high \nR, the dis-tortion is minimized when K/M1((2ln2\n)R)1. Choosingthe number of measurements accordingly gives performanceDJ(R)2(ln2\n)eRc22R=c22(RR)(6)where R=(1/2)log2(2(ln2\n)eR), giving the final blue entry inTable 1. Comparing to \nc22R, we see that having access only torandom measurements induces a significant performance loss.One interpretation of this analysis is that the coding rate haseffectively been reduced by Rb per degree of freedom. Since Rgrows sublinearly with R, the situation is not too badat least theperformance does not degrade with increasing Kor N. The analy-sis when Jis not known at the decoderi.e., it must be inferredfrom yreveals a much worse situation.LOSS FROM SPARSITY RECOVERY\nAs we have mentioned before, compressive sampling is motivated\nby the idea that the sparsity pattern Jcan be detected, through acomputationally tractable convex optimization, with a small\nnumber of measurements M. However, the number of measure-\nments required depends on the noise level. We saw\nM2Klog(NK)+Kscaling is required by lasso reconstruc-tion; if the noise is from quantization and we are trying to code\n\nwith KRtotal bits, this scaling leads to a vanishing number of bitsper measurement.Unfortunately, the problem is more fundamental than subopti-\nmality of lasso decoding. We will show that trying to code with\nKRtotal bits makes reliable recovery of the sparsity patternimpossible as the signal dimension Nincreases. In this analysis,we assume the sparsity ratio =K/Nis held constant as theproblems scale, and we see that no number of measurements Mcan give good performance.To see why the sparsity pattern cannot be recovered, consider the\nproblem of estimating the sparsity pattern of ufrom the noisy meas-urement yin (3). Let Esignal=E[Au2]and Enoise=E[v2]bethe signal and noise energies, respectively, and define the signal-to-\n\nnoise ratio (SNR) as SNR=Esignal/Enoise. The number of meas-urements Mrequired to recover the sparsity pattern of ufrom ycanbe bounded below with the following theorem.THEOREM 1Consider any estimator for recovering the sparsity pattern of aK-sparse vector ufrom measurements yof the form (3), where vIEEE SIGNAL PROCESSING MAGAZINE[52]MARCH 2008'b'is a white Gaussian vector uncorrelated with y. Let Perrorbe theprobability of misdetecting the sparsity pattern, averaged\nover the realizations of the random matrix Aand noise v.Suppose M, K, and NKapproach infinity withM<KSNR[(1log(NK)1](7)for some 0. Then Perror1,i.e., the estimator will asymp-totically always fail.The main ideas of a proof of Theorem 1 are given in Proof\nSketch for Theorem 1. Under certain assumptions, the quanti-\nzation error vin our problem will be asymptotically Gaussian, sowe can apply the bound (see Quantizer Performance and\nQuantization Error). The theorem shows that to attain any\nnon-vanishing probability of success, we need the scalingMKSNR[(1log(NK)1].(8)Now, using the normalization assumptions described above, the\nexpression =1, and 2vgiven in (1), it can be shown thatthe signal and noise energies are given by Esignal=M(12and Enoise=M1. Therefore, the SNR isSNR=(1.(\n9)Now, let \n=K/Mbe the measurement ratio, i.e., the ratio ofdegrees of freedom in the unknown signal to number of meas-\nurements. From (2), 22Rfor any quantizer, and there-\nfore, from (9), SNR22R1. Substituting this bound for theSNR into (8), we see that for the probability of error to vanish\n(or even become a value less than one) will require 22R(1\n+1>log(NK).(\n10)Notice that, for any fixed R, the left hand side of (10) is boundedabove uniformly over all (0,1]. However, if the sparsity ratio\n=K/Nis fixed and N, then log(NK).Consequently, the bound (10) is impossible to satisfy. We conclude\n\nthat: for a fixed rate Rand sparsity ratio , as N, there isno number of measurements Mthat can guarantee reliable spar-sity recovery. In fact, the probability of detecting the sparsity pat-\n\ntern correctly approaches zero. This conclusion applies not just tocompressive sampling with basis pursuit or matching pursuit\ndetection, but even to exhaustive search methods.How bad is this result for compressive sampling? We have\nshown that exact sparsity recovery is fundamentally impossible\nwhen the total number of bits scales linearly with the degrees of\nfreedom of the signal and the quantization is regular. However,\n\nexact sparsity recovery may not be necessary for good perform-\n\nance. What if the decoder can detect, say, 90% of the elements in\n\nthe sparsity pattern correctly? One might think that the result-\ning distortion might still be small.Unfortunately, when we translate the best known error\nbounds for reconstruction from nonadaptively encoded under-\nsampled data, we do not even obtain distortion that approachesPROOF SKETCH FOR THEOREM 1Since the vector uRNhas Knonzero components, Aubelongs to one of the NKsubspaces, each subspace beingspanned by Kof the \nNcolumns of ARMN. Let Vbe the setof all such subspaces and let V0Vbe the true K-dimen-sional subspacethe one that contains Au. The detector withthe minimum probability of error would search over all the\nsubspaces for the one with the maximum energy of the\nreceived noisy vector y. For the estimator to detect the correctsubspace, the true subspace must have the maximum energy.\n\nThat is,PV0y2PVy2,VV,(13)where PSdenotes the projection operator onto the subspaceS. We can show (7) from (13) as follows.\nThe true subspace is spanned by Kcolumns of A,which we\nwill denote by \na1,...,\naK. Since V0contains Au, it must con-tain the entire signal energy Esignal. It also contains a fraction=K/Mof the noise energy, \nEnoise. So the average energy inthe subspace V0is PV0y2=Esignal+Enoise. Although thisexpression is technically only true in expectation, it is asymp-\ntotically exact for large M. So here and in the remainder ofthe proof, we omit the expectations in the formulas.Now remove the vector a1, and let V1be the subspacespanned by the remaining K1vectors {aj}Kj=2. Since the vec-tors ajare i.i.d. and spherically symmetrically distributed, theenergy of yin V1relative to the energy in V0is given byPV0y2PV1y2=1KPV0y2=1KEsignal+Enoise.(14)Now let aj, j=K+1,...,\nNbe the remaining NKcolumnsof A. The MKdimensional subspaceV0V1contains afraction 1K/Mof the noise energy Enoise. Each column ajisindependent of the signal in V0. When Mis large, it can be\nshown that adding one of the columns will add a random\namount of energy described by(1K/M)Enoiseu2j/(MK)=Enoiseu2j/M, where ujis an N(0,1)Gaussian random variable. Let Vbe the subspace spanned byV1and the vector ajwith the maximum energy. The new sub-\nspace Vis spanned by Kcolumns of A, so VV. Also, theenergy in Vwill bePVy2PV1y2=1MEnoisemaxj=K+1,...,\nNu2j.For any 0, it can be shown thatPrmaxj=K+1,...,\nNu2j>(1log(NK)1,as NK. Therefore, as NK, with high proba-bility,\nPVy2PV1y2>1M(1Enoiselog(NK).(\n15)Combining (13), (14), and (15) shows (7).IEEE SIGNAL PROCESSING MAGAZINE[53]MARCH 2008'b'zero as the rate is increased with K, M, and Nfixed. [Rememberthat without undersampling, one can at least obtain the per-\nformance (5).]For example, Cands and Tao [18]prove that an\n\nestimator similar to the lasso estimator attains a distortion1Kxx2c1KM(logN2,(11)with large probability, from \nMmeasurements with noise vari-ance 2, provided that the number of measurements is ade-quate.  There is a constant \n(0,1)such that M=Kissufficient for (11) to hold with probability approaching one as Nis increased with K/Nheld constant; but for any finite N, thereis a nonzero probability of failure. Spreading \nRKbits amongstthe measurements and relating the number of bits to the quan-\ntization noise variance givesD=1KE[xx2]c2logN)22R+Derr,(12)where Derris the distortion due to the failure event. (Haupt andNowak [19] consider optimal estimators and obtain a bound simi-\nlar to (12) in that it has a term that is constant with respect to the\nnoise variance. See also [20] for related results.) Thus if \nDerrisnegligible, the distortion will decrease exponentially in the rate,\nbut with an exponent reduced by a factor . However, as \nNincreas-es to infinity, the distortion bound increases and is not useful.\nNUMERICAL SIMULATION\nTo get some idea of the possible performance, we performed\n\nthe following numerical experiment. We fixed the signal\n\ndimensions to N=100and K=10, so the signal has a sparsi-ty of =K/N=0.1. We varied the quantization rate \nRfrom4 to 12 b per degree of freedom, which spans low to high rate\nsince (1/K)log2NK4.4. The resulting simulated perform-ance of compressive sampling is shown in Figure 5.The per-\n\nformance of direct quantization [Ddirect(R)from (5)] andbaseline quantization with time sharing [see (4) and (5)] areshown for comparison.The distortion of compressive sampling was simulated as fol-lows: For both lasso and orthogonal matching pursuit (OMP)\nreconstruction and for integer rates R, the number of measure-ments Mwas varied from Kto Nin steps of ten. At each value ofM, the distortion was estimated by averaging 500 Monte Carlo tri-als with random encoder matrices and quantization noise vec-tors v. To give the best-case performance of compressive sampling,\nthe distortion was taken to be the minimum distortion over the\ntested values of Mand, for lasso, over several values of the regular-ization parameter . The optimal Mis not necessarily the mini-mum Mto guarantee sparsity recovery. Instead, optimizing \nMtrades off errors in the sparsity pattern against errors in the esti-mated values for the components. The optimal value does not\nresult in small probability of subspace misdetection. More exten-\nsive sets of simulations consistent with these are presented in [21].From (4), the distortion with adaptive quantization decreases\nexponentially with the rate Rthrough the multiplicative factor22R. This appears in Figure 5as a decrease in distortion of approx-\nimately 6 dB/b. In contrast, simple direct quantization achieves a\ndistortion given by (5), which in this case is only 0.6 dB/b. Thus,\nthere is potentially a large gap between direct quantization that\ndoes not exploit the sparsity and adaptive quantization that does.Both compressive sampling methods, lasso and OMP, are able\nto perform slightly better than simple direct quantization,\nachieving approximately 1.41.6 dB/b. (A finer analysis that\nallows computation of the largest possible in (12) might predictthis slope.) Thus, compressive sampling is able to exploit the\nsparsity to some degree and narrow the gap between linear and\nadaptive quantization. However, neither algorithm is able to\n\ncome close to the performance of the baseline encoder that can\nuse adaptive quantization. Indeed, comparing to the baseline\nquantization, there a multiplicative rate penalty in this simula-\ntion of approximately a factor of four. This is large by source cod-\n\ning standards, and we can conclude that compressive sampling\ndoes not achieve performance similar to adaptive quantization.INFORMATION THEORY TO THE RESCUE?\nWe have thus far used information theory to provide context and\n\nanalysis tools. It has shown us that compressing sparse signals\nby scalar quantization of random measurements incurs a signifi-\ncant penalty. Can information theory also suggest alternatives to\n\ncompressive sampling? In fact, it does provide techniques that\nwould give much better performance for source coding, but the\ncomplexity of decoding algorithms becomes even higher.\nLet us return to Figure 3and interpret it as a communica-\ntion problem where xis to be reproduced approximately and thenumber of bits that can be used is limited. We would like to\n\nextract source coding with side informationand distributedsource codingproblems from this setup. This will lead to resultsmuch more positive than those developed above.In developing the baseline quantization method, we discussedhow an encoder that knows Vcan recover Jand uKfrom xandthus send Jexactly and uKapproximately. Compressive sampling\n[FIG5]Rate-distortion performance of compressive sampling\nusing reconstruction via OMP and lasso. At each rate, the\n\nnumber of measurements \nMis optimized to minimize thedistortion. Also plotted are the theoretical distortion curves for\n\ndirect and baseline quantization. In all simulations\n(K,N)=(10,100).4567891011120RateMSE (dB)DirectOMPLassoBaselineIEEE SIGNAL PROCESSING MAGAZINE[54]MARCH 2008'b'is to apply when the encoder does not know (or want to use) thesparsifying basis V. In this case, an information theorist would saythat we have a problem of lossy source coding of xwith side infor-mation Vavailable at the decoderan instance of the Wyner-Ziv\nproblem[22]. In contrast to the analogous lossless coding prob-lem (see Slepian-Wolf Coding), the unavailability of the side\n\ninformation at the encoder does in general hurt the best possible\nperformance. Specifically, let \nL(D)denote the rate loss (increasedrate because Vis unavailable) to achieve distortion D. Then thereare upper bounds to L(D)that depend only on the source alpha-bet, the way distortion is measured, and the value of the distor-\ntionnot on the distribution of the source or side information\n[23]. For the scenario of interest to us [(continuous-valued source\nand mean-squared error(MSE) distortion)], L(D)0.5b for allD. The techniques to achieve this are complicated, but note theconstant additive rate penaltyis in dramatic contrast to Figure 5.Compressive sampling not only allows side information Vtobe available only at the decoder, it also allows the components of\n\nthe measurement vector yto be encoded separately. The way to\ninterpret this information theoretically is to considery1,y2,...,\nyMas distributed sources whose joint distributiondepends on side information (V)\navailable at the decoder.\nImposing a constraint of distributed encoding of y(while allow-ing joint decoding) generally creates a degradation of the best\npossible performance. (Again, there is no performance penalty in\nthe lossless case; see Slepian-Wolf Coding.) Let us sketch a par-\n\nticular strategy that is not necessarily optimal but exhibits only a\nsmall additive rate penalty. This is inspired by [23] and [24].\nSuppose that each of Mdistributed encoders performs scalarquantization of its own yito yield q(yi). Before this seemed toimmediately get us in trouble (recall our interpretation of\nTheorem 1), but now we will do further encoding. The quantized\nvalues give us a lossless distributed compression problem with side\ninformation (V)\navailable at the decoder. Using Slepian-Wolf\ncoding, a total rate arbitrarily close to \nH(q(y))can be achieved.\nThe remaining question is how the rate and distortion relate.For the sake of analysis, let us assume that the encoder and\ndecoder share some randomness Zso that the scalar quantizationabove can be subtractively dithered (see, e.g., [25]). Then follow-\ning the analysis in [24] and [26],encoding the quantized samples\nq(y)at rate H(q(y)|V,Z)is within 0.755 b of the conditionalrate-distortion bound for source xgiven V. Thus the combinationof universal dithered quantization with Slepian-Wolf coding gives\n\na method of distributed coding with only a constant additive rate\npenalty. These methods inspired by information theory depend on\n\ncoding across independent signal acquisition instances, and they\ngenerally incur large decoding complexity.\nLet us finally interpret the quantization plus Slepian-Wolf\napproach described above when limited to a single instance.\nSuppose the yisare separately quantized as described above. Themain negative result of this article indicates that ideal separate\nentropy coding of each q(yi)is not nearly enough to get to goodperformance. The rate must be reduced by replacing an ordinary\nentropy code with one that collapses some distinct quantized val-\nues to the same index. The hope has to be that in the joint decod-ing of q(y), the dependence between components will save theday. This is equivalent to saying that the quantizers in use are\n\nnot regular [25], much like multiple description quantizers [27].\nThis approach is developed and simulated in [21].CONCLUSIONSWHITHER COMPRESSIVE SAMPLING?To an information theorist, compression is the efficient repre-\n\nsentation of data with bits. In this article, we have looked at\n\ncompressive sampling from this perspective, to see if random\n\nmeasurements of sparse signals provide an efficient method of\nrepresenting sparse signals.The source coding performance depends sharply on how therandom measurements are encoded into bits. Using familiar\n\nforms of quantization (regular quantizers; see [25]) even veryweak forms of universality are precluded. One would want to\nSLEPIAN-WOLF CODINGWhen two related quantities are to be compressed, there isgenerally an advantage to doing the compression jointly.\n\nWhat does jointly mean? On its face, jointly would seem\nto mean that the quantities are inseparably mapped to a bit\nstring. However, Slepian and Wolf [29] remarkably showed\n\nthat it can be good enough for the decoding to be joint\nthe encoding can be separate.To understand the result precisely, suppose a sequence of\nindependent replicas (X(1)1,X(1)2),(\nX(2)1,X(2)2),...\n, of the pairof jointly distributed discrete random variables (X1,X2)is tobe compressed. The minimum possible rate is H(X1,X2), thejoint entropy of X1and X2. The normal way to approach this\nminimum rate is to treat (X1,X2)as a single discrete randomvariable (over an alphabet that is the Cartesian product of the\nalphabets of X1and X2) and apply an entropy code to thisrandom variable. This requires an encoder that operates onX1and X2together. The main result of [29] indicates that this\ntotal rate can be approached with encoders that see X1andX2separately as long as the decoding is joint. The recovery ofthe Xksis perfect (or has vanishing error probability) without\nrequiring any excess total rate (or arbitrarily small excess rate):R1+R2=H(X1,X2). The individual rates need only satisfyR1H(X1|X2)and R2H(X2|X1).As a very simple example, suppose X1has any distributionon the integers; and X2X1equals zero or one with equalprobability, independent of \nX1. Then (X1,X2)has preciselyone more bit of information than \nX1alone. The optimal totalrate R1+R2=H(X1)+1can be achieved by having Encoder1 compress X1as if communicating X1were the only goal andhaving Encoder 2 send only the parity of X2.Slepian-Wolf coding can be extended to any number of cor-\nrelated sources with no penalty in the rate [30, Thm.\n14.4.2]. Also, simpler than Slepian-Wolf coding is for one of\n\nthe sources (say, \nX2) to be available to the decoder but not tothe encoder. Then a rate of \nR1=H(X1|X2)is sufficient to\nallow the decoder to recover \nX1, even though the encodingof X1is done without knowledge of \nX2. The main text usesthese results to give information-theoretic bounds for encod-\ning of random measurements.IEEE SIGNAL PROCESSING MAGAZINE[55]MARCH 2008'b'spend a number of bits proportional to the number of degrees offreedom of the sparse signal, but this does not lead to good per-\nformance. In this case, we can conclude analytically that recovery\nof the sparsity pattern is asymptotically impossible. Furthermore,\nsimulations show that the MSE performance is far from optimal.Information theory provides alternatives based on universalversions of distributed lossless coding (Slepian-Wolf coding)\n\nand entropy-coded dithered quantization. These information-\ntheoretic constructions indicate that it is reasonable to ask for\ngood performance with merely linear scaling of the number of\nbits with the sparsity of the signal. However, practical imple-\n\nmentation of such schemes remains an open problem.It is important to keep our mainly negative results in propercontext. We have shown that compressive sampling combined\n\nwith ordinary quantization is a bad compression technique, but\nour results say nothing about whether compressive sampling is an\n\neffective initial step in data acquisition. A good analogy within the\nrealm of signal acquisition is oversampling in analog-to-digital\nconversion (ADC). Since MSE distortion in oversampled ADC\ndrops only polynomially (not exponentially) with the oversampling\nfactor, high oversampling alonewithout other processingleads\n\nto poor rate-distortion performance. Nevertheless, oversampling is\nubiquitous. Similarly, compressive sampling is useful in contexts\n\nwhere sampling itself is very expensive, but the subsequent storage\nand communication of quantized samples is less constricted.ACKNOWLEDGMENTSThe authors thank Sourav Dey, Lav Varshney, Claudio\n\nWeidmann, Joseph Yeh, and an anonymous reviewer for\n\nthoughtful comments that helped us improve the article.AUTHORSVivek K Goyal\n(vgoyal@mit.edu) received the B.S. degree in mathe-matics and the B.S.E. degree in electrical engineering from the\n\nUniversity of Iowa and the M.S. and Ph.D. degrees in electrical engi-\nneering from the University of California, Berkeley. He is currently\n\nEsther and Harold E. Edgerton Assistant Professor of Electrical\nEngineering at the Massachusetts Institute of Technology. He has\n\nreceived the UC-Berkeley Eliahu Jury Award for outstanding\n\nachievement in systems, communications, control, or signal pro-\ncessing, the IEEE SPS Magazine Award and an NSF CAREER\n\nAward. He is on the IEEE SPS Image and Multiple Dimensional\n\nSignal Processing Technical Committee and cochair of the SPIE\n\nWavelets conference series. He is a Senior Member of the IEEE.\nAlyson K. Fletcher\nreceived the B.S. degree in mathematics\nfrom the University of Iowa and the M.A. degree in mathematics\n\nand the M.S. and Ph.D. degrees in electrical engineering from the\n\nUniversity of California, Berkeley.  She is currently a Presidents\n\nPostdoctoral Fellow at the University of California, Berkeley. She\n\nis a member of SWE, SIAM, and Sigma Xi.  She has been awarded\n\nthe University of California Eugene L. Lawler Award, the Henry\n\nLuce Foundations Clare Boothe Luce Fellowship, and the\n\nSoroptimist Dissertation Fellowship. Her research interests\ninclude estimation, image processing, statistical signal process-\ning, sparse approximation, wavelets, and control theory.\nSundeep Ranganreceived the B.A.Sc. degree in electrical engi-neering from the University of Waterloo, Canada, and the M.S. and\n\nPh.D. degrees in electrical engineering from the University of\n\nCalifornia, Berkeley. He was a postdoctoral research fellow at the\n\nUniversity of Michigan. He then joined the Wireless Research\n\nCenter at Bell Laboratories, and in 2000, he co-founded Flarion\nTechnologies with four others. He is currently a director of engi-\n\nneering at Qualcomm Technologies, where he is involved in the\n\ndevelopment of next generation cellular wireless systems.REFERENCES[1]D.L.Donoho, M.Vetterli, R.A.DeVore, and I.Daubechies, Data compression\nand harmonic analysis, IEEE Trans. Inform. Theory\n, vol. 44, no. 6, pp.24352476, Oct.1998.\n[2]E.J.Cande\n`s, J.Romberg, and T.Tao, Robust uncertainty principles: Exact sig-\nnal reconstruction from highly incomplete frequency information, IEEE Trans.\nInform. Theory, vol. 52, no. 2, pp. 489509, Feb.2006.\n[3]E.J.Cande\n`sand T.Tao, Near-optimal signal recovery from random projections:\nUniversal encoding strategies? IEEE Trans. Inform. Theory\n, vol. 52, no. 12, pp.54065425, Dec.2006.\n[4]D.L.Donoho, Compressed sensing, \nIEEE Trans. Inform. Theory\n, vol. 52, no.\n4, pp. 12891306, Apr.2006.\n[5]B.K.Natarajan, Sparse approximate solutions to linear systems, \nSIAM J.Comput., vol. 24, no. 2, pp. 227234, Apr.1995.\n[6]D.L.Donohoand P.B.Stark, Uncertainty principles and signal recovery, \nSIAMJ. Appl. Math., vol. 49, no. 3, pp. 906931, June1989.\n[7]D.L.Donohoand J.Tanner, Counting faces of randomly-projected polytopes\nwhen the projection radically lowers dimension, submitted for publication. [8]M.J.Wainwright, Sharp thresholds for high-dimensional and noisy recovery of\nsparsity, Univ. California, Berkeley, Dept. of Statistics, Tech. Report\n\n#arXiv:math.ST/0605740 v1 30, May2006.\n[9]V.K.Goyal, Theoretical foundations of transform coding, \nIEEE SignalProcessing Mag., vol. 18, no. 5, pp. 921, Sept.2001.\n[10]A.K.Fletcher, A jump linear framework for estimation and robust communi-\ncation with Markovian source and channel dynamics. Ph.D. dissertation, Dept. of\nElectrical Eng. Comp. Sci., Univ. California, Berkeley, Nov.2005. \n[11]H.Viswanathanand R.Zamir, On the whiteness of high-resolution quantiza-\ntion errors, IEEE Trans. Inform. Theory\n, vol. 47, no. 5, pp. 20292038, July2001.\n[12]R.G.Gallager, \nInformation Theory and Reliable Communication. New York:\nWiley, 1968.\n[13] C.Weidmannand M. Vetterli, Rate-distortion analysis of spike processes, in\nProc. IEEE Data Compression Conf., Snowbird, Utah, Mar.1999, pp. 8291.\n[14]V.K.Goyal, M.Vetterli, and N.T.Thao, Quantized overcomplete expansions\nin RN: Analysis, synthesis, and algorithms, IEEE Trans. Inform. Theory\n, vol. 44,no. 1, pp. 1631, Jan.1998.\n[15]S.Ranganand V.K.Goyal, Recursive consistent estimation with bounded\nnoise, IEEE Trans. Inform. Theory\n, vol. 47, no. 1, pp. 457464, Jan.2001.\n[16]V.K.Goyal, J.Kova\ncevic, and J.A.Kelner, Quantized frame expansions with\nerasures, Appl. Comput. Harmon. Anal., vol. 10, no. 3, pp. 203233, May2001.\n[17]A.K.Fletcher, S.Rangan, and V.K.Goyal, On the rate-distortion performance\nof compressed sensing, in Proc. IEEE Int. Conf. Acoustics, Speech, SignalProcessing, Honolulu, HI, vol. 3, Apr.2007, pp. 885888.\n[18]E.J.Cande\n`sand T.Tao, The Dantzig selector: Statistical estimation when \npismuch larger than n, submitted for publication. [19]J.Hauptand R.Nowak, Signal reconstruction from noisy random projec-\ntions, IEEE Trans. Inform. Theory\n, vol. 52, no. 9, pp. 40364048, Sept.2006.\n[20]N.Meinshausenand B. Yu, Lasso-type recovery of sparse representations for\nhigh-dimensional data, Univ. of California, Berkeley, Dept. of Statistics,Sept.2007. \n[21]R.J.Pai, Nonadaptive lossy encoding of sparse signals, M.S thesis, Dept. of\nElectrical Eng. Comp. Sci., MIT, Cambridge, MA, Aug.2006. \n[22]A.D.Wynerand J.Ziv, The rate-distortion function for source coding with\nside information at the decoder, \nIEEE Trans. Inform. Theory\n, vol. IT-22, no. 1,\npp. 110, Jan.1976.\n[23]R.Zamir, The rate loss in the WynerZiv problem, \nIEEE Trans. Inform.\nTheory, vol. 42, no. 6, pp. 20732084, Nov.1996.\n[24]J.Ziv, On universal quantization, \nIEEE Trans. Inform. Theory\n, vol. IT-31,\nno. 3, pp. 344347, May1985.\n[25]R.M.Grayand D.L.Neuhoff, Quantization, \nIEEE Trans. Inform. Theory\n, vol.44, no. 6, pp. 23252383, Oct.1998.\n[26]R.Zamirand M.Feder, On universal quantization by randomized\nuniform/lattice quantization, IEEE Trans. Inform. Theory\n, vol. 38, no. 2, pp. 428436, Mar.1992.\n[27]V.K.Goyal, Multiple description coding: Compression meets the network,\nIEEE Signal Processing Mag., vol. 18, no. 5, pp. 7493, Sept.2001.\n[28]T.Berger, \nRate Distortion Theory. Englewood Cliffs, NJ: Prentice-Hall, 1971.[29]D.Slepianand J.K.Wolf, Noiseless coding of correlated information sources,\nIEEE Trans. Inform. Theory\n, vol. IT-19, no. 4, pp. 471480, July1973.\n[30]T.M.Coverand J.A.Thomas, \nElements of Information Theory. New York:\nWiley, 1991.\nIEEE SIGNAL PROCESSING MAGAZINE[56]MARCH 2008[SP]'