b'AConvolutionalNeuralNetworkforModellingSentences\nNalKalchbrennerEdwardGrefenstette\nf\nnal.kalchbrenner,edward.grefenstette,phil.blunsom\ng\n@cs.ox.ac.uk\nDepartmentofComputerScience\nUniversityofOxford\nPhilBlunsom\nAbstract\nTheabilitytoaccuratelyrepresentsen-\ntencesiscentraltolanguageunderstand-\ning.Wedescribeaconvolutionalarchitec-\nturedubbedtheDynamicConvolutional\nNeuralNetwork(DCNN)thatweadopt\nforthesemanticmodellingofsentences.\nThenetworkusesDynamic\nk\n-MaxPool-\ning,aglobalpoolingoperationoverlin-\nearsequences.Thenetworkhandlesinput\nsentencesofvaryinglengthandinduces\nafeaturegraphoverthesentencethatis\ncapableofexplicitlycapturingshortand\nlong-rangerelations.Thenetworkdoes\nnotrelyonaparsetreeandiseasilyap-\nplicabletoanylanguage.Wetestthe\nDCNNinfourexperiments:smallscale\nbinaryandmulti-classsentimentpredic-\ntion,six-wayquestionand\nTwittersentimentpredictionbydistantsu-\npervision.Thenetworkachievesexcellent\nperformanceinthethreetasksanda\ngreaterthan\n25%\nerrorreductioninthelast\ntaskwithrespecttothestrongestbaseline.\n1Introduction\nTheaimofasentencemodelistoanalyseand\nrepresentthesemanticcontentofasentencefor\npurposesoforgeneration.Thesen-\ntencemodellingproblemisatthecoreofmany\ntasksinvolvingadegreeofnaturallanguagecom-\nprehension.Thesetasksincludesentimentanaly-\nsis,paraphrasedetection,entailmentrecognition,\nsummarisation,discourseanalysis,machinetrans-\nlation,groundedlanguagelearningandimagere-\ntrieval.Sinceindividualsentencesarerarelyob-\nservedornotobservedatall,onemustrepresent\nasentenceintermsoffeaturesthatdependonthe\nwordsandshort\nn\n-gramsinthesentencethatare\nfrequentlyobserved.Thecoreofasentencemodel\ninvolvesafeaturefunctionthattheprocess\nFigure1:Subgraphofafeaturegraphinduced\noveraninputsentenceinaDynamicConvolu-\ntionalNeuralNetwork.Thefullinducedgraph\nhasmultiplesubgraphsofthiskindwithadistinct\nsetofedges;subgraphsmaymergeatdifferent\nlayers.Theleftdiagramemphasisesthepooled\nnodes.Thewidthoftheconvolutionalersis3\nand2respectively.Withdynamicpooling,a\nterwithsmallwidthatthehigherlayerscanrelate\nphrasesfarapartintheinputsentence.\nbywhichthefeaturesofthesentenceareextracted\nfromthefeaturesofthewordsor\nn\n-grams.\nVarioustypesofmodelsofmeaninghavebeen\nproposed.Compositionbasedmethodshavebeen\nappliedtovectorrepresentationsofwordmeaning\nobtainedfromco-occurrencestatisticstoobtain\nvectorsforlongerphrases.Insomecases,com-\npositionisbyalgebraicoperationsover\nwordmeaningvectorstoproducesentencemean-\ningvectors(ErkandPad\n\no,2008;Mitchelland\nLapata,2008;MitchellandLapata,2010;Tur-\nney,2012;Erk,2012;Clarke,2012).Inother\ncases,acompositionfunctionislearnedandei-\nthertiedtoparticularsyntacticrelations(Guevara,\n2010;Zanzottoetal.,2010)ortoparticularword\ntypes(BaroniandZamparelli,2010;Coeckeet\nal.,2010;GrefenstetteandSadrzadeh,2011;Kart-\nsaklisandSadrzadeh,2013;Grefenstette,2013).\nAnotherapproachrepresentsthemeaningofsen-\ntencesbywayofautomaticallyextractedlogical\nforms(ZettlemoyerandCollins,2005).\narXiv:1404.2188v1  [cs.CL]  8 Apr 2014'b'Acentralclassofmodelsarethosebasedon\nneuralnetworks.Theserangefrombasicneu-\nralbag-of-wordsorbag-of-\nn\n-gramsmodelstothe\nmorestructuredrecursiveneuralnetworksand\ntotime-delayneuralnetworksbasedonconvo-\nlutionaloperations(CollobertandWeston,2008;\nSocheretal.,2011;KalchbrennerandBlunsom,\n2013b).Neuralsentencemodelshaveanum-\nberofadvantages.Theycanbetrainedtoobtain\ngenericvectorsforwordsandphrasesbypredict-\ning,forinstance,thecontextsinwhichthewords\nandphrasesoccur.Throughsupervisedtraining,\nneuralsentencemodelscanthesevec-\ntorstoinformationthatistoacertain\ntask.Besidescomprisingpowerfulclassias\npartoftheirarchitecture,neuralsentencemodels\ncanbeusedtoconditionaneurallanguagemodel\ntogeneratesentenceswordbyword(Schwenk,\n2012;MikolovandZweig,2012;Kalchbrenner\nandBlunsom,2013a).\nWeaconvolutionalneuralnetworkarchi-\ntectureandapplyittothesemanticmodellingof\nsentences.Thenetworkhandlesinputsequences\nofvaryinglength.Thelayersinthenetworkin-\nterleaveone-dimensionalconvolutionallayersand\ndynamic\nk\n-maxpoolinglayers.Dynamic\nk\n-max\npoolingisageneralisationofthemaxpoolingop-\nerator.Themaxpoolingoperatorisanon-linear\nsubsamplingfunctionthatreturnsthemaximum\nofasetofvalues(LeCunetal.,1998).Theop-\neratorisgeneralisedintworespects.First,\nk\n-\nmaxpoolingoveralinearsequenceofvaluesre-\nturnsthesubsequenceof\nk\nmaximumvaluesinthe\nsequence,insteadofthesinglemaximumvalue.\nSecondly,thepoolingparameter\nk\ncanbedynam-\nicallychosenbymaking\nk\nafunctionofotheras-\npectsofthenetworkortheinput.\nTheconvolutionallayersapplyone-\ndimensionalacrosseachrowoffeaturesin\nthesentencematrix.Convolvingthesame\nwiththe\nn\n-gramateverypositioninthesentence\nallowsthefeaturestobeextractedindependently\noftheirpositioninthesentence.Aconvolutional\nlayerfollowedbyadynamicpoolinglayerand\nanon-linearityformafeaturemap.Likeinthe\nconvolutionalnetworksforobjectrecognition\n(LeCunetal.,1998),weenrichtherepresentation\ninthelayerbycomputingmultiplefeature\nmapswithdifferentappliedtotheinput\nsentence.Subsequentlayersalsohavemultiple\nfeaturemapscomputedbyconvolvingwith\nallthemapsfromthelayerbelow.Theweightsat\ntheselayersformanorder-4tensor.Theresulting\narchitectureisdubbedaDynamicConvolutional\nNeuralNetwork.\nMultiplelayersofconvolutionalanddynamic\npoolingoperationsinduceastructuredfeature\ngraphovertheinputsentence.Figure1illustrates\nsuchagraph.Smallathigherlayerscancap-\nturesyntacticorsemanticrelationsbetweennon-\ncontinuousphrasesthatarefarapartintheinput\nsentence.Thefeaturegraphinducesahierarchical\nstructuresomewhatakintothatinasyntacticparse\ntree.Thestructureisnottiedtopurelysyntactic\nrelationsandisinternaltotheneuralnetwork.\nWeexperimentwiththenetworkinfourset-\ntings.Thetwoexperimentsinvolvepredict-\ningthesentimentofmoviereviews(Socheret\nal.,2013b).Thenetworkoutperformsotherap-\nproachesinboththebinaryandthemulti-classex-\nperiments.Thethirdexperimentinvolvesthecat-\negorisationofquestionsinsixquestiontypesin\ntheTRECdataset(LiandRoth,2002).Thenet-\nworkmatchestheaccuracyofotherstate-of-the-\nartmethodsthatarebasedonlargesetsofen-\ngineeredfeaturesandhand-codedknowledgere-\nsources.Thefourthexperimentinvolvespredict-\ningthesentimentofTwitterpostsusingdistantsu-\npervision(Goetal.,2009).Thenetworkistrained\non1.6milliontweetslabelledautomaticallyac-\ncordingtotheemoticonthatoccursinthem.On\nthehand-labelledtestset,thenetworkachievesa\ngreaterthan\n25%\nreductioninthepredictionerror\nwithrespecttothestrongestunigramandbigram\nbaselinereportedinGoetal.(2009).\nTheoutlineofthepaperisasfollows.Section2\ndescribesthebackgroundtotheDCNNincluding\ncentralconceptsandrelatedneuralsentencemod-\nels.Section3therelevantoperatorsand\nthelayersofthenetwork.Section4treatsofthe\ninducedfeaturegraphandotherpropertiesofthe\nnetwork.Section5discussestheexperimentsand\ninspectsthelearntfeaturedetectors.\n1\n2Background\nThelayersoftheDCNNareformedbyaconvo-\nlutionoperationfollowedbyapoolingoperation.\nWebeginwithareviewofrelatedneuralsentence\nmodels.Thenwedescribetheoperationof\none-\ndimensionalconvolution\nandtheclassicalTime-\nDelayNeuralNetwork(TDNN)(Hinton,1989;\nWaibeletal.,1990).Byaddingamaxpooling\n1\nCodeavailableat\nwww.nal.co\n'b'layertothenetwork,theTDNNcanbeadoptedas\nasentencemodel(CollobertandWeston,2008).\n2.1RelatedNeuralSentenceModels\nVariousneuralsentencemodelshavebeende-\nscribed.Ageneralclassofbasicsentencemodels\nisthatofNeuralBag-of-Words(NBoW)models.\nThesegenerallyconsistofaprojectionlayerthat\nmapswords,sub-wordunitsor\nn\n-gramstohigh\ndimensionalembeddings;thelatterarethencom-\nbinedcomponent-wisewithanoperationsuchas\nsummation.Theresultingcombinedvectorisclas-\nthroughoneormorefullyconnectedlayers.\nAmodelthatadoptsamoregeneralstructure\nprovidedbyanexternalparsetreeistheRecursive\nNeuralNetwork(RecNN)(Pollack,1990;K\n\nuchler\nandGoller,1996;Socheretal.,2011;Hermann\nandBlunsom,2013).Ateverynodeinthetreethe\ncontextsattheleftandrightchildrenofthenode\narecombinedbyaclassicallayer.Theweightsof\nthelayeraresharedacrossallnodesinthetree.\nThelayercomputedatthetopnodegivesarepre-\nsentationforthesentence.TheRecurrentNeural\nNetwork(RNN)isaspecialcaseoftherecursive\nnetworkwherethestructurethatisfollowedisa\nsimplelinearchain(GersandSchmidhuber,2001;\nMikolovetal.,2011).TheRNNisprimarilyused\nasalanguagemodel,butmayalsobeviewedasa\nsentencemodelwithalinearstructure.Thelayer\ncomputedatthelastwordrepresentsthesentence.\nFinally,afurtherclassofneuralsentencemod-\nelsisbasedontheconvolutionoperationandthe\nTDNNarchitecture(CollobertandWeston,2008;\nKalchbrennerandBlunsom,2013b).Certaincon-\nceptsusedinthesemodelsarecentraltothe\nDCNNandwedescribethemnext.\n2.2Convolution\nThe\none-dimensionalconvolution\nisanoperation\nbetweenavectorofweights\nm\n2\nR\nm\nandavector\nofinputsviewedasasequence\ns\n2\nR\ns\n.Thevector\nm\nisthe\n\noftheconvolution.Concretely,we\nthinkof\ns\nastheinputsentenceand\ns\ni\n2\nR\nisasin-\nglefeaturevalueassociatedwiththe\ni\n-thwordin\nthesentence.Theideabehindtheone-dimensional\nconvolutionistotakethedotproductofthevector\nm\nwitheach\nm\n-graminthesentence\ns\ntoobtain\nanothersequence\nc\n:\nc\nj\n=\nm\n|\ns\nj\n\nm\n+1:\nj\n(1)\nEquation1givesrisetotwotypesofconvolution\ndependingontherangeoftheindex\nj\n.The\nnarrow\ntypeofconvolutionrequiresthat\ns\n\nm\nandyields\nFigure2:Narrowandwidetypesofconvolution.\nThe\nm\nhassize\nm\n=5\n.\nasequence\nc\n2\nR\ns\n\nm\n+1\nwith\nj\nrangingfrom\nm\nto\ns\n.The\nwide\ntypeofconvolutiondoesnothave\nrequirementson\ns\nor\nm\nandyieldsasequence\nc\n2\nR\ns\n+\nm\n\n1\nwheretheindex\nj\nrangesfrom\n1\nto\ns\n+\nm\n\n1\n.Out-of-rangeinputvalues\ns\ni\nwhere\ni<\n1\nor\ni>s\naretakentobezero.Theresultofthe\nnarrowconvolutionisasubsequenceoftheresult\nofthewideconvolution.Thetwotypesofone-\ndimensionalconvolutionareillustratedinFig.2.\nThetrainedweightsinthe\nm\ncorrespond\ntoalinguisticfeaturedetectorthatlearnstorecog-\nniseaclassof\nn\n-grams.These\nn\n-grams\nhavesize\nn\n\nm\n,where\nm\nisthewidthofthe\n.Applyingtheweights\nm\ninawideconvo-\nlutionhassomeadvantagesoverapplyingthemin\nanarrowone.Awideconvolutionensuresthatall\nweightsinthereachtheentiresentence,in-\ncludingthewordsatthemargins.Thisisparticu-\nlarlysiwhen\nm\nissettoarelativelylarge\nvaluesuchas8or10.Inaddition,awideconvo-\nlutionguaranteesthattheapplicationofthe\nm\ntotheinputsentence\ns\nalwaysproducesavalid\nnon-emptyresult\nc\n,independentlyofthewidth\nm\nandthesentencelength\ns\n.Wenextdescribethe\nclassicalconvolutionallayerofaTDNN.\n2.3Time-DelayNeuralNetworks\nATDNNconvolvesasequenceofinputs\ns\nwitha\nsetofweights\nm\n.AsintheTDNNforphoneme\nrecognition(Waibeletal.,1990),thesequence\ns\nisviewedashavingatimedimensionandthecon-\nvolutionisappliedoverthetimedimension.Each\ns\nj\nisoftennotjustasinglevalue,butavectorof\nd\nvaluessothat\ns\n2\nR\nd\n\ns\n.Likewise,\nm\nisama-\ntrixofweightsofsize\nd\n\nm\n.Eachrowof\nm\nis\nconvolvedwiththecorrespondingrowof\ns\nandthe\nconvolutionisusuallyofthenarrowtype.Multi-\npleconvolutionallayersmaybestackedbytaking\ntheresultingsequence\nc\nasinputtothenextlayer.\nTheMax-TDNNsentencemodelisbasedonthe\narchitectureofaTDNN(CollobertandWeston,\n2008).Inthemodel,aconvolutionallayerofthe\nnarrowtypeisappliedtothesentencematrix\ns\n,\nwhereeachcolumncorrespondstothefeaturevec-\n'b'tor\nw\ni\n2\nR\nd\nofawordinthesentence:\ns\n=\n2\n4\nw\n1\n:::\nw\ns\n3\n5\n(2)\nToaddresstheproblemofvaryingsentence\nlengths,theMax-TDNNtakesthemaximumof\neachrowintheresultingmatrix\nc\nyieldingavector\nof\nd\nvalues:\nc\nmax\n=\n2\n6\n4\nmax(\nc\n1\n;\n:\n)\n.\n.\n.\nmax(\nc\nd;\n:\n)\n3\n7\n5\n(3)\nTheaimistocapturethemostrelevantfeature,i.e.\ntheonewiththehighestvalue,foreachofthe\nd\nrowsoftheresultingmatrix\nc\n.Theed-sized\nvector\nc\nmax\nisthenusedasinputtoafullycon-\nnectedlayerfor\nTheMax-TDNNmodelhasmanydesirable\nproperties.Itissensitivetotheorderofthewords\ninthesentenceanditdoesnotdependonexternal\nfeaturessuchasdependencyor\nconstituencyparsetrees.Italsogiveslargelyuni-\nformimportancetothesignalcomingfromeach\nofthewordsinthesentence,withtheexception\nofwordsatthemarginsthatareconsideredfewer\ntimesinthecomputationofthenarrowconvolu-\ntion.Butthemodelalsohassomelimitingas-\npects.Therangeofthefeaturedetectorsislim-\nitedtothespan\nm\noftheweights.Increasing\nm\nor\nstackingmultipleconvolutionallayersofthenar-\nrowtypemakestherangeofthefeaturedetectors\nlarger;atthesametimeitalsoexacerbatesthene-\nglectofthemarginsofthesentenceandincreases\ntheminimumsize\ns\noftheinputsentencerequired\nbytheconvolution.Forthisreasonhigher-order\nandlong-rangefeaturedetectorscannotbeeasily\nincorporatedintothemodel.Themaxpoolingop-\nerationhassomedisadvantagestoo.Itcannotdis-\ntinguishwhetherarelevantfeatureinoneofthe\nrowsoccursjustoneormultipletimesanditfor-\ngetstheorderinwhichthefeaturesoccur.More\ngenerally,thepoolingfactorbywhichthesignal\nofthematrixisreducedatoncecorrespondsto\ns\n\nm\n+1\n;evenformoderatevaluesof\ns\nthepool-\ningfactorcanbeexcessive.Theaimofthenext\nsectionistoaddresstheselimitationswhilepre-\nservingtheadvantages.\n3ConvolutionalNeuralNetworkswith\nDynamic\nk\n-MaxPooling\nWemodelsentencesusingaconvolutionalarchi-\ntecturethatalternateswideconvolutionallayers\nFigure3:ADCNNforthesevenwordinputsen-\ntence.Wordembeddingshavesize\nd\n=4\n.The\nnetworkhastwoconvolutionallayerswithtwo\nfeaturemapseach.Thewidthsoftheatthe\ntwolayersarerespectively3and2.The(dynamic)\nk\n-maxpoolinglayershavevalues\nk\nof\n5\nand3.\nwithdynamicpoolinglayersgivenby\ndynamic\nk\n-\nmaxpooling\n.Inthenetworkthewidthofafeature\nmapatanintermediatelayervariesdependingon\nthelengthoftheinputsentence;theresultingar-\nchitectureistheDynamicConvolutionalNeural\nNetwork.Figure3representsaDCNN.Wepro-\nceedtodescribethenetworkindetail.\n3.1WideConvolution\nGivenaninputsentence,toobtaintherstlayerof\ntheDCNNwetaketheembedding\nw\ni\n2\nR\nd\nfor\neachwordinthesentenceandconstructthesen-\ntencematrix\ns\n2\nR\nd\n\ns\nasinEq.2.Thevalues\nintheembeddings\nw\ni\nareparametersthatareop-\ntimisedduringtraining.Aconvolutionallayerin\nthenetworkisobtainedbyconvolvingamatrixof\nweights\nm\n2\nR\nd\n\nm\nwiththematrixofactivations\natthelayerbelow.Forexample,thesecondlayer\nisobtainedbyapplyingaconvolutiontothesen-\ntencematrix\ns\nitself.Dimension\nd\nandwidth\nm\narehyper-parametersofthenetwork.Weletthe\noperationsbe\nwide\none-dimensionalconvolutions\nasdescribedinSect.2.2.Theresultingmatrix\nc\nhasdimensions\nd\n\n(\ns\n+\nm\n\n1)\n.\n'b'3.2\nk\n-MaxPooling\nWenextdescribeapoolingoperationthatisagen-\neralisationofthemaxpoolingoverthetimedi-\nmensionusedintheMax-TDNNsentencemodel\nanddifferentfromthelocalmaxpoolingopera-\ntionsappliedinaconvolutionalnetworkforobject\nrecognition(LeCunetal.,1998).Givenavalue\nk\nandasequence\np\n2\nR\np\noflength\np\n\nk\n,\nk\n-\nmaxpooling\nselectsthesubsequence\np\nk\nmax\nofthe\nk\nhighestvaluesof\np\n.Theorderofthevaluesin\np\nk\nmax\ncorrespondstotheiroriginalorderin\np\n.\nThe\nk\n-maxpoolingoperationmakesitpossible\ntopoolthe\nk\nmostactivefeaturesin\np\nthatmaybe\nanumberofpositionsapart;itpreservestheorder\nofthefeatures,butisinsensitivetotheir\npositions.Itcanalsodiscernmorethenum-\nberoftimesthefeatureishighlyactivatedin\np\nandtheprogressionbywhichthehighactivations\nofthefeaturechangeacross\np\n.The\nk\n-maxpooling\noperatorisappliedinthenetworkafterthetopmost\nconvolutionallayer.Thisguaranteesthattheinput\ntothefullyconnectedlayersisindependentofthe\nlengthoftheinputsentence.But,asweseenext,at\nintermediateconvolutionallayersthepoolingpa-\nrameter\nk\nisnoted,butisdynamicallyselected\ninordertoallowforasmoothextractionofhigher-\norderandlonger-rangefeatures.\n3.3Dynamic\nk\n-MaxPooling\nA\ndynamic\nk\n-maxpooling\noperationisa\nk\n-max\npoolingoperationwherewelet\nk\nbeafunctionof\nthelengthofthesentenceandthedepthofthenet-\nwork.Althoughmanyfunctionsarepossible,we\nsimplymodelthepoolingparameterasfollows:\nk\nl\n=max(\nk\ntop\n;\nd\nL\n\nl\nL\ns\ne\n)\n(4)\nwhere\nl\nisthenumberofthecurrentconvolutional\nlayertowhichthepoolingisappliedand\nL\nisthe\ntotalnumberofconvolutionallayersinthenet-\nwork;\nk\ntop\nistheedpoolingparameterforthe\ntopmostconvolutionallayer(Sect.3.2).Forin-\nstance,inanetworkwiththreeconvolutionallay-\nersand\nk\ntop\n=3\n,foraninputsentenceoflength\ns\n=18\n,thepoolingparameteratthelayer\nis\nk\n1\n=12\nandthepoolingparameteratthesec-\nondlayeris\nk\n2\n=6\n;thethirdlayerhastheed\npoolingparameter\nk\n3\n=\nk\ntop\n=3\n.Equation4\nisamodelofthenumberofvaluesneededtode-\nscribetherelevantpartsoftheprogressionofan\nl\n-thorderfeatureoverasentenceoflength\ns\n.For\nanexampleinsentimentprediction,accordingto\ntheequationaorderfeaturesuchasaposi-\ntivewordoccurs\natmost\nk\n1\ntimesinasentenceof\nlength\ns\n,whereasasecondorderfeaturesuchasa\nnegatedphraseorclauseoccursatmost\nk\n2\ntimes.\n3.4Non-linearFeatureFunction\nAfter(dynamic)\nk\n-maxpoolingisappliedtothe\nresultofaconvolution,abias\nb\n2\nR\nd\nandanon-\nlinearfunction\ng\nareappliedcomponent-wiseto\nthepooledmatrix.Thereisasinglebiasvaluefor\neachrowofthepooledmatrix.\nIfwetemporarilyignorethepoolinglayer,we\nmaystatehowonecomputeseach\nd\n-dimensional\ncolumn\na\ninthematrix\na\nresultingaftertheconvo-\nlutionalandnon-linearlayers.\nM\ntobethe\nmatrixofdiagonals:\nM\n=[\ndiag\n(\nm\n:\n;\n1\n)\n;:::;\ndiag\n(\nm\n:\n;m\n)]\n(5)\nwhere\nm\naretheweightsofthe\nd\nofthewide\nconvolution.Thenafterthepairofaconvolu-\ntionalandanon-linearlayer,eachcolumn\na\ninthe\nmatrix\na\nisobtainedasfollows,forsomeindex\nj\n:\na\n=\ng\n0\nB\n@\nM\n2\n6\n4\nw\nj\n.\n.\n.\nw\nj\n+\nm\n\n1\n3\n7\n5\n+\nb\n1\nC\nA\n(6)\nHere\na\nisacolumnoforderfeatures.Sec-\nondorderfeaturesaresimilarlyobtainedbyap-\nplyingEq.6toasequenceoforderfeatures\na\nj\n;:::;a\nj\n+\nm\n0\n\n1\nwithanotherweightmatrix\nM\n0\n.\nBarringpooling,Eq.6representsacoreaspect\nofthefeatureextractionfunctionandhasarather\ngeneralformthatwereturntobelow.Together\nwithpooling,thefeaturefunctioninducesposition\ninvarianceandmakestherangeofhigher-order\nfeaturesvariable.\n3.5MultipleFeatureMaps\nSofarwehavedescribedhowoneappliesawide\nconvolution,a(dynamic)\nk\n-maxpoolinglayerand\nanon-linearfunctiontotheinputsentencema-\ntrixtoobtainaorder\nfeaturemap\n.Thethree\noperationscanberepeatedtoyieldfeaturemaps\nofincreasingorderandanetworkofincreasing\ndepth.Wedenoteafeaturemapofthe\ni\n-thorder\nby\nF\ni\n.Asinconvolutionalnetworksforobject\nrecognition,toincreasethenumberoflearntfea-\nturedetectorsofacertainorder,multiplefeature\nmaps\nF\ni\n1\n;:::;\nF\ni\nn\nmaybecomputedinparallelat\nthesamelayer.Eachfeaturemap\nF\ni\nj\niscomputed\nbyconvolvingadistinctsetofarrangedin\namatrix\nm\ni\nj;k\nwitheachfeaturemap\nF\ni\n\n1\nk\nofthe\nlowerorder\ni\n\n1\nandsummingtheresults:\n'b'F\ni\nj\n=\nn\nX\nk\n=1\nm\ni\nj;k\n\nF\ni\n\n1\nk\n(7)\nwhere\n\nindicatesthewideconvolution.The\nweights\nm\ni\nj;k\nformanorder-4tensor.Afterthe\nwideconvolution,dynamic\nk\n-maxpooling\nandthenthenon-linearfunctionareappliedindi-\nviduallytoeachmap.\n3.6Folding\nIntheformulationofthenetworksofar,feature\ndetectorsappliedtoanindividualrowofthesen-\ntencematrix\ns\ncanhavemanyordersandcreate\ncomplexdependenciesacrossthesamerowsin\nmultiplefeaturemaps.Featuredetectorsindiffer-\nentrows,however,areindependentofeachother\nuntilthetopfullyconnectedlayer.Fulldepen-\ndencebetweendifferentrowscouldbeachieved\nbymaking\nM\ninEq.5afullmatrixinsteadof\nasparsematrixofdiagonals.Hereweexplorea\nsimplermethodcalled\nfolding\nthatdoesnotintro-\nduceanyadditionalparameters.Afteraconvo-\nlutionallayerandbefore(dynamic)\nk\n-maxpool-\ning,onejustsumseverytworowsinafeaturemap\ncomponent-wise.Foramapof\nd\nrows,foldingre-\nturnsamapof\nd=\n2\nrows,thushalvingthesizeof\ntherepresentation.Withafoldinglayer,afeature\ndetectorofthe\ni\n-thorderdependsnowontworows\noffeaturevaluesinthelowermapsoforder\ni\n\n1\n.\nThisendsthedescriptionoftheDCNN.\n4PropertiesoftheSentenceModel\nWedescribesomeofthepropertiesofthesentence\nmodelbasedontheDCNN.Wedescribetheno-\ntionofthe\nfeaturegraph\ninducedoverasentence\nbythesuccessionofconvolutionalandpooling\nlayers.Werelatethepropertiestothoseof\notherneuralsentencemodels.\n4.1Wordand\nn\n-GramOrder\nOneofthebasicpropertiesissensitivitytotheor-\nderofthewordsintheinputsentence.Formost\napplicationsandinordertolearnfea-\nturedetectors,itisforamodeltobeable\ntodiscriminatewhethera\nn\n-gramoccurs\nintheinput.Likewise,itisforamodel\ntobeabletotellthe\nrelative\npositionofthemost\nrelevant\nn\n-grams.Thenetworkisdesignedtocap-\nturethesetwoaspects.The\nm\nofthewide\nconvolutioninthelayercanlearntorecognise\n\nn\n-gramsthathavesizelessorequaltothe\nwidth\nm\n;asweseeintheexperiments,\nm\nin\nthelayerisoftensettoarelativelylargevalue\nsuchas\n10\n.Thesubsequenceof\nn\n-gramsextracted\nbythegeneralisedpoolingoperationinducesin-\nvariancetoabsolutepositions,butmaintainstheir\norderandrelativepositions.\nAsregardstheotherneuralsentencemodels,the\nclassofNBoWmodelsisbyinsensitive\ntowordorder.Asentencemodelbasedonarecur-\nrentneuralnetworkissensitivetowordorder,but\nithasabiastowardsthelatestwordsthatittakesas\ninput(Mikolovetal.,2011).ThisgivestheRNN\nexcellentperformanceatlanguagemodelling,but\nitissuboptimalforrememberingatoncethe\nn\n-\ngramsfurtherbackintheinputsentence.Sim-\nilarly,arecursiveneuralnetworkissensitiveto\nwordorderbuthasabiastowardsthetopmost\nnodesinthetree;shallowertreesmitigatethisef-\nfecttosomeextent(Socheretal.,2013a).Asseen\ninSect.2.3,theMax-TDNNissensitivetoword\norder,butmaxpoolingonlypicksoutasingle\nn\n-\ngramfeatureineachrowofthesentencematrix.\n4.2InducedFeatureGraph\nSomesentencemodelsuseinternalorexternal\nstructuretocomputetherepresentationforthein-\nputsentence.InaDCNN,theconvolutionand\npoolinglayersinduceaninternalfeaturegraph\novertheinput.Anodefromalayerisconnected\ntoanodefromthenexthigherlayerifthelower\nnodeisinvolvedintheconvolutionthatcomputes\nthevalueofthehighernode.Nodesthatarenot\nselectedbythepoolingoperationatalayerare\ndroppedfromthegraph.Afterthelastpooling\nlayer,theremainingnodesconnecttoasingletop-\nmostroot.Theinducedgraphisaconnected,di-\nrectedacyclicgraphwithweightededgesanda\nrootnode;twoequivalentrepresentationsofan\ninducedgrapharegiveninFig.1.InaDCNN\nwithoutfoldinglayers,eachofthe\nd\nrowsofthe\nsentencematrixinducesasubgraphthatjoinsthe\nothersubgraphsonlyattherootnode.Eachsub-\ngraphmayhaveadifferentshapethatthe\nkindofrelationsthataredetectedinthatsubgraph.\nTheeffectoffoldinglayersistojoinpairsofsub-\ngraphsatlowerlayersbeforethetoprootnode.\nConvolutionalnetworksforobjectrecognition\nalsoinduceafeaturegraphovertheinputimage.\nWhatmakesthefeaturegraphofaDCNNpecu-\nliaristheglobalrangeofthepoolingoperations.\nThe(dynamic)\nk\n-maxpoolingoperatorcandraw\ntogetherfeaturesthatcorrespondtowordsthatare\nmanypositionsapartinthesentence.Higher-order\nfeatureshavehighlyvariablerangesthatcanbeei-\n'b'thershortandfocusedorglobalandlongasthe\ninputsentence.Likewise,theedgesofasubgraph\nintheinducedgraphthesevaryingranges.\nThesubgraphscaneitherbelocalisedtooneor\nmorepartsofthesentenceorspreadmorewidely\nacrossthesentence.Thisstructureisinternalto\nthenetworkandisbytheforwardpropa-\ngationoftheinputthroughthenetwork.\nOftheothersentencemodels,theNBoWisa\nshallowmodelandtheRNNhasalinearchain\nstructure.ThesubgraphsinducedintheMax-\nTDNNmodelhaveasingleed-rangefeatureob-\ntainedthroughmaxpooling.Therecursiveneural\nnetworkfollowsthestructureofanexternalparse\ntree.Featuresofvariablerangearecomputedat\neachnodeofthetreecombiningoneormoreof\nthechildrenofthetree.UnlikeinaDCNN,where\nonelearnsaclearhierarchyoffeatureorders,in\naRecNNloworderfeatureslikethoseofsin-\nglewordscanbedirectlycombinedwithhigher\norderfeaturescomputedfromentireclauses.A\nDCNNgeneralisesmanyofthestructuralaspects\nofaRecNN.Thefeatureextractionfunctionas\nstatedinEq.6hasamoregeneralformthanthat\ninaRecNN,wherethevalueof\nm\nisgenerally2.\nLikewise,theinducedgraphstructureinaDCNN\nismoregeneralthanaparsetreeinthatitisnot\nlimitedtosyntacticallydictatedphrases;thegraph\nstructurecancaptureshortorlong-rangeseman-\nticrelationsbetweenwordsthatdonotnecessar-\nilycorrespondtothesyntacticrelationsinaparse\ntree.TheDCNNhasinternalinput-dependent\nstructureanddoesnotrelyonexternallyprovided\nparsetrees,whichmakestheDCNNdirectlyap-\nplicabletohard-to-parsesentencessuchastweets\nandtosentencesfromanylanguage.\n5Experiments\nWetestthenetworkonfourdifferentexperiments.\nWebeginbyspecifyingaspectsoftheimplemen-\ntationandthetrainingofthenetwork.Wethenre-\nlatetheresultsoftheexperimentsandweinspect\nthelearntfeaturedetectors.\n5.1Training\nIneachoftheexperiments,thetoplayerofthe\nnetworkhasafullyconnectedlayerfollowedby\nasoftmaxnon-linearitythatpredictstheprobabil-\nitydistributionoverclassesgiventheinputsen-\ntence.Thenetworkistrainedtominimisethe\ncross-entropyofthepredictedandtruedistribu-\ntions;theobjectiveincludesan\nL\n2\nregularisation\nFine-grained(%)Binary(%)\nNB41.081.8\nB\nI\nNB41.983.1\nSVM40.779.4\nR\nEC\nNTN45.785.4\nM\nAX\n-TDNN37.477.1\nNB\nO\nW42.480.5\nDCNN\n48.586.8\nTable1:Accuracyofsentimentpredictioninthe\nmoviereviewsdataset.Thefourresultsare\nreportedfromSocheretal.(2013b).Thebaselines\nNBandB\nI\nNBareNaiveBayeswith,\nrespectively,unigramfeaturesandunigramandbi-\ngramfeatures.SVMisasupportvectormachine\nwithunigramandbigramfeatures.R\nEC\nNTNisa\nrecursiveneuralnetworkwithatensor-basedfea-\nturefunction,whichreliesonexternalstructural\nfeaturesgivenbyaparsetreeandperformsbest\namongtheRecNNs.\ntermovertheparameters.Thesetofparameters\ncomprisesthewordembeddings,theweights\nandtheweightsfromthefullyconnectedlayers.\nThenetworkistrainedwithmini-batchesbyback-\npropagationandthegradient-basedoptimisationis\nperformedusingtheAdagradupdaterule(Duchi\netal.,2011).Usingthewell-knownconvolution\ntheorem,wecancomputefastone-dimensional\nlinearconvolutionsatallrowsofaninputmatrix\nbyusingFastFourierTransforms.Toexploitthe\nparallelismoftheoperations,wetrainthenetwork\nonaGPU.AMatlabimplementationprocesses\nmultiplemillionsofinputsentencesperhouron\noneGPU,dependingprimarilyonthenumberof\nlayersusedinthenetwork.\n5.2SentimentPredictioninMovieReviews\nThetwoexperimentsconcerntheprediction\nofthesentimentofmoviereviewsintheStanford\nSentimentTreebank(Socheretal.,2013b).The\noutputvariableisbinaryinoneexperimentand\ncanhavevepossibleoutcomesintheother:neg-\native,somewhatnegative,neutral,somewhatposi-\ntive,positive.Inthebinarycase,weusethegiven\nsplitsof6920training,872developmentand1821\ntestsentences.Likewise,inthecase,\nweusethestandard8544/1101/2210splits.La-\nbelledphrasesthatoccurassubpartsofthetrain-\ningsentencesaretreatedasindependenttraining\ninstances.Thesizeofthevocabularyis15448.\nTable1detailstheresultsoftheexperiments.\n'b'FeaturesAcc.(%)\nH\nIER\nunigram,POS,headchunks91.0\nNE,semanticrelations\nM\nAX\nE\nNT\nunigram,bigram,trigram92.6\nPOS,chunks,NE,supertags\nCCGparser,WordNet\nM\nAX\nE\nNT\nunigram,bigram,trigram93.6\nPOS,wh-word,headword\nwordshape,parser\nhypernyms,WordNet\nSVM\nunigram,POS,wh-word95.0\nheadword,parser\nhypernyms,WordNet\n60hand-codedrules\nM\nAX\n-TDNNunsupervisedvectors84.4\nNB\nO\nWunsupervisedvectors88.2\nDCNNunsupervisedvectors93.0\nTable2:Accuracyofsix-wayquestion\ntionontheTRECquestionsdataset.Thesecond\ncolumndetailstheexternalfeaturesusedinthe\nvariousapproaches.Thefourresultsarere-\nspectivelyfromLiandRoth(2002),Blunsometal.\n(2006),Huangetal.(2008)andSilvaetal.(2011).\nInthethreeneuralsentencemodelstheMax-\nTDNN,theNBoWandtheDCNNthewordvec-\ntorsareparametersofthemodelsthatareran-\ndomlyinitialised;theirdimension\nd\nissetto48.\nTheMax-TDNNhasaofwidth\n6\ninitsnar-\nrowconvolutionatthelayer;shorterphrases\narepaddedwithzerovectors.Theconvolu-\ntionallayerisfollowedbyanon-linearity,amax-\npoolinglayerandasoftmaxlayer.\nTheNBoWsumsthewordvectorsandappliesa\nnon-linearityfollowedbyasoftmax\nlayer.Theadoptednon-linearityisthe\ntanh\nfunc-\ntion.ThehyperparametersoftheDCNNareas\nfollows.ThebinaryresultisbasedonaDCNN\nthathasawideconvolutionallayerfollowedbya\nfoldinglayer,adynamic\nk\n-maxpoolinglayerand\nanon-linearity;ithasasecondwideconvolutional\nlayerfollowedbyafoldinglayer,a\nk\n-maxpooling\nlayerandanon-linearity.Thewidthoftheconvo-\nlutionalis7and5,respectively.Thevalue\nof\nk\nforthetop\nk\n-maxpoolingis4.Thenum-\nberoffeaturemapsattheconvolutionallayer\nis6;thenumberofmapsatthesecondconvolu-\ntionallayeris14.Thenetworkistoppedbyasoft-\nmaxlayer.TheDCNNforthe\ngrainedresulthasthesamearchitecture,butthe\nhavesize10and7,thetoppoolingparame-\nter\nk\nis5andthenumberofmapsis,respectively,\n6and12.Thenetworksusethe\ntanh\nnon-linear\nAccuracy(%)\nSVM81.6\nB\nI\nNB82.7\nM\nAX\nE\nNT\n83.0\nM\nAX\n-TDNN78.8\nNB\nO\nW80.9\nDCNN\n87.4\nTable3:AccuracyontheTwittersentiment\ndataset.Thethreenon-neuralarebased\nonunigramandbigramfeatures;theresultsarere-\nportedfrom(Goetal.,2009).\nfunction.Attrainingtimeweapplydropouttothe\npenultimatelayerafterthelast\ntanh\nnon-linearity\n(Hintonetal.,2012).\nWeseethattheDCNNoutper-\nformstheotherneuralandnon-neuralmodels.\nTheNBoWperformssimilarlytothenon-neural\nn\n-grambasedTheMax-TDNNper-\nformsworsethantheNBoWlikelyduetotheex-\ncessivepoolingofthemaxpoolingoperation;the\nlatterdiscardsmostofthesentimentfeaturesofthe\nwordsintheinputsentence.BesidestheRecNN\nthatusesanexternalparsertoproducestructural\nfeaturesforthemodel,theothermodelsuse\nn\n-\ngrambasedorneuralfeaturesthatdonotrequire\nexternalresourcesoradditionalannotations.Inthe\nnextexperimentwecomparetheperformanceof\ntheDCNNwiththoseofmethodsthatuseheavily\nengineeredresources.\n5.3QuestionType\nAsanaidtoquestionanswering,aquestionmay\nbeasbelongingtooneofmanyquestion\ntypes.TheTRECquestionsdatasetinvolvessix\ndifferentquestiontypes,e.g.whetherthequestion\nisaboutalocation,aboutapersonoraboutsome\nnumericinformation(LiandRoth,2002).The\ntrainingdatasetconsistsof5452labelledquestions\nwhereasthetestdatasetconsistsof500questions.\nTheresultsarereportedinTab.2.Thenon-\nneuralapproachesuseaoveralarge\nnumberofmanuallyengineeredfeaturesand\nhand-codedresources.Forinstance,Blunsomet\nal.(2006)presentaMaximumEntropymodelthat\nrelieson26setsofsyntacticandsemanticfea-\nturesincludingunigrams,bigrams,trigrams,POS\ntags,namedentitytags,structuralrelationsfrom\naCCGparseandWordNetsynsets.Weevaluate\nthethreeneuralmodelsonthisdatasetwithmostly\nthesamehyper-parametersasinthebinarysenti-\n'b"Figure4:Topve\n7\n-gramsatfourfeaturedetectorsinthelayerofthenetwork.\nmentexperimentofSect.5.2.Asthedatasetis\nrathersmall,weuselower-dimensionalwordvec-\ntorswith\nd\n=32\nthatareinitialisedwithembed-\ndingstrainedinanunsupervisedwaytopredict\ncontextsofoccurrence(Turianetal.,2010).The\nDCNNusesasingleconvolutionallayerwith\ntersofsize8and5featuremaps.Thedifference\nbetweentheperformanceoftheDCNNandthatof\ntheotherhigh-performingmethodsinTab.2isnot\n(\np<\n0\n:\n09\n).Giventhattheonlylabelled\ninformationusedtotrainthenetworkisthetrain-\ningsetitself,itisnotablethatthenetworkmatches\ntheperformanceofstate-of-the-artthat\nrelyonlargeamountsofengineeredfeaturesand\nrulesandhand-codedresources.\n5.4TwitterSentimentPredictionwith\nDistantSupervision\nInourexperiment,wetrainthemodelsona\nlargedatasetoftweets,whereatweetisautomat-\nicallylabelledaspositiveornegativedepending\nontheemoticonthatoccursinit.Thetrainingset\nconsistsof1.6milliontweetswithemoticon-based\nlabelsandthetestsetofabout400hand-annotated\ntweets.Wepreprocessthetweetsminimallyfol-\nlowingtheproceduredescribedinGoetal.(2009);\ninaddition,wealsolowercaseallthetokens.This\nresultsinavocabularyof76643wordtypes.The\narchitectureoftheDCNNandoftheotherneural\nmodelsisthesameastheoneusedinthebinary\nexperimentofSect.5.2.Therandomlyinitialised\nwordembeddingsareincreasedinlengthtoadi-\nmensionof\nd\n=60\n.Table3reportstheresultsof\ntheexperiments.Weseeaincreasein\ntheperformanceoftheDCNNwithrespecttothe\nnon-neural\nn\n-grambasedinthepres-\nenceoflargeamountsoftrainingdatatheseclas-\nconstituteparticularlystrongbaselines.We\nseethattheabilitytotrainasentimenton\nautomaticallyextractedemoticon-basedlabelsex-\ntendstotheDCNNandresultsinhighlyaccurate\nperformance.Thedifferenceinperformancebe-\ntweentheDCNNandtheNBoWfurthersuggests\nthattheabilityoftheDCNNtobothcapturefea-\nturesbasedonlong\nn\n-gramsandtohierarchically\ncombinethesefeaturesishighly\n5.5VisualisingFeatureDetectors\nAintheDCNNisassociatedwithafeature\ndetectororneuronthatlearnsduringtrainingto\nbeparticularlyactivewhenpresentedwithaspe-\nsequenceofinputwords.Inthelayer,the\nsequenceisacontinuous\nn\n-gramfromtheinput\nsentence;inhigherlayers,sequencescanbemade\nofmultipleseparate\nn\n-grams.Wevisualisethe\nfeaturedetectorsinthelayerofthenetwork\ntrainedonthebinarysentimenttask(Sect.5.2).\nSincethehavewidth7,foreachofthe288\nfeaturedetectorswerankall\n7\n-gramsoccurringin\nthevalidationandtestsetsaccordingtotheirac-\ntivationofthedetector.Figure5.2presentsthe\ntopve\n7\n-gramsforfourfeaturedetectors.Be-\nsidestheexpecteddetectorsforpositiveandnega-\ntivesentiment,wedetectorsforparticlessuch\nas`not'thatnegatesentimentandsuchas`too'\nthatpotentiatesentiment.Wedetectorsfor\nmultipleothernotableconstructsincluding`all',\n`or',`with...that',`as...as'.Thefeaturedetectors\nlearntorecognisenotjustsingle\nn\n-grams,butpat-\nternswithin\nn\n-gramsthathavesyntactic,semantic\norstructural\n6Conclusion\nWehavedescribedadynamicconvolutionalneural\nnetworkthatusesthedynamic\nk\n-maxpoolingop-\neratorasanon-linearsubsamplingfunction.The\nfeaturegraphinducedbythenetworkisableto\ncapturewordrelationsofvaryingsize.Thenet-\nworkachieveshighperformanceonquestionand\nsentimentwithoutrequiringexternal\nfeaturesasprovidedbyparsersorotherresources.\nAcknowledgements\nWethankNandodeFreitasandYeeWhyeTeh\nforgreatdiscussionsonthepaper.Thisworkwas\nsupportedbyaXeroxFoundationAward,EPSRC\ngrantnumberEP/F042728/1,andEPSRCgrant\nnumberEP/K036580/1.\n"b"References\nMarcoBaroniandRobertoZamparelli.2010.Nouns\narevectors,adjectivesarematrices:Representing\nadjective-nounconstructionsinsemanticspace.In\nEMNLP\n,pages11831193.ACL.\nPhilBlunsom,KrystleKocik,andJamesR.Curran.\n2006.Questionwithlog-linearmod-\nels.In\nSIGIR'06:Proceedingsofthe29than-\nnualinternationalACMSIGIRconferenceonRe-\nsearchanddevelopmentininformationretrieval\n,\npages615616,NewYork,NY,USA.ACM.\nDaoudClarke.2012.Acontext-theoreticframe-\nworkforcompositionalityindistributionalseman-\ntics.\nComputationalLinguistics\n,38(1):4171.\nBobCoecke,MehrnooshSadrzadeh,andStephen\nClark.2010.MathematicalFoundationsforaCom-\npositionalDistributionalModelofMeaning.March.\nRonanCollobertandJasonWeston.2008.A\narchitecturefornaturallanguageprocessing:Deep\nneuralnetworkswithmultitasklearning.In\nInterna-\ntionalConferenceonMachineLearning,ICML\n.\nJohnDuchi,EladHazan,andYoramSinger.2011.\nAdaptivesubgradientmethodsforonlinelearning\nandstochasticoptimization.\nJ.Mach.Learn.Res.\n,\n12:21212159,July.\nKatrinErkandSebastianPad\n\no.2008.Astructured\nvectorspacemodelforwordmeaningincontext.\nProceedingsoftheConferenceonEmpiricalMeth-\nodsinNaturalLanguageProcessing-EMNLP'08\n,\n(October):897.\nKatrinErk.2012.Vectorspacemodelsofwordmean-\ningandphrasemeaning:Asurvey.\nLanguageand\nLinguisticsCompass\n,6(10):635653.\nFelixA.GersandJrgenSchmidhuber.2001.Lstm\nrecurrentnetworkslearnsimplecontext-freeand\ncontext-sensitivelanguages.\nIEEETransactionson\nNeuralNetworks\n,12(6):13331340.\nAlecGo,RichaBhayani,andLeiHuang.2009.Twit-\ntersentimentusingdistantsupervision.\nProcessing\n,pages16.\nEdwardGrefenstetteandMehrnooshSadrzadeh.2011.\nExperimentalsupportforacategoricalcomposi-\ntionaldistributionalmodelofmeaning.In\nProceed-\ningsoftheConferenceonEmpiricalMethodsinNat-\nuralLanguageProcessing\n,pages13941404.Asso-\nciationforComputationalLinguistics.\nEdwardGrefenstette.2013.Category-theoretic\nquantitativecompositionaldistributionalmodels\nofnaturallanguagesemantics.\narXivpreprint\narXiv:1311.1539\n.\nEmilianoGuevara.2010.ModellingAdjective-Noun\nCompositionalitybyRegression.\nESSLLI'10Work-\nshoponCompositionalityandDistributionalSe-\nmanticModels\n.\nKarlMoritzHermannandPhilBlunsom.2013.The\nRoleofSyntaxinVectorSpaceModelsofComposi-\ntionalSemantics.In\nProceedingsofthe51stAnnual\nMeetingoftheAssociationforComputationalLin-\nguistics(Volume1:LongPapers)\n,Bulgaria,\nAugust.AssociationforComputationalLinguistics.\nForthcoming.\nGeoffreyE.Hinton,NitishSrivastava,Alex\nKrizhevsky,IlyaSutskever,andRuslanSalakhut-\ndinov.2012.Improvingneuralnetworksby\npreventingco-adaptationoffeaturedetectors.\nCoRR\n,abs/1207.0580.\nGeoffreyE.Hinton.1989.Connectionistlearningpro-\ncedures.\nArtif.Intell.\n,40(1-3):185234.\nZhihengHuang,MarcusThint,andZengchangQin.\n2008.Questionusingheadwordsand\ntheirhypernyms.In\nProceedingsoftheConference\nonEmpiricalMethodsinNaturalLanguagePro-\ncessing\n,EMNLP'08,pages927936,Stroudsburg,\nPA,USA.AssociationforComputationalLinguis-\ntics.\nNalKalchbrennerandPhilBlunsom.2013a.Recur-\nrentcontinuoustranslationmodels.In\nProceedings\nofthe2013ConferenceonEmpiricalMethodsin\nNaturalLanguageProcessing\n,Seattle,October.As-\nsociationforComputationalLinguistics.\nNalKalchbrennerandPhilBlunsom.2013b.Recur-\nrentConvolutionalNeuralNetworksforDiscourse\nCompositionality.In\nProceedingsoftheWorkshop\nonContinuousVectorSpaceModelsandtheirCom-\npositionality\n,Bulgaria,August.Association\nforComputationalLinguistics.\nDimitriKartsaklisandMehrnooshSadrzadeh.2013.\nPriordisambiguationofwordtensorsforconstruct-\ningsentencevectors.In\nProceedingsofthe2013\nConferenceonEmpiricalMethodsinNaturalLan-\nguageProcessing(EMNLP)\n,Seattle,USA,October.\nAndreasK\n\nuchlerandChristophGoller.1996.Induc-\ntivelearninginsymbolicdomainsusingstructure-\ndrivenrecurrentneuralnetworks.InG\n\nuntherG\n\norz\nandSteffenH\n\nolldobler,editors,\nKI\n,volume1137of\nLectureNotesinComputerScience\n,pages183197.\nSpringer.\nYannLeCun,L\n\neonBottou,YoshuaBengio,andPatrick\nHaffner.1998.Gradient-basedlearningappliedto\ndocumentrecognition.\nProceedingsoftheIEEE\n,\n86(11):22782324,November.\nXinLiandDanRoth.2002.Learningquestionclas-\nIn\nProceedingsofthe19thinternational\nconferenceonComputationallinguistics-Volume1\n,\npages17.AssociationforComputationalLinguis-\ntics.\nTomasMikolovandGeoffreyZweig.2012.Context\ndependentrecurrentneuralnetworklanguagemodel.\nIn\nSLT\n,pages234239.\n"b'TomasMikolov,StefanKombrink,LukasBurget,Jan\nCernock\n\ny,andSanjeevKhudanpur.2011.Exten-\nsionsofrecurrentneuralnetworklanguagemodel.\nIn\nICASSP\n,pages55285531.IEEE.\nJeffMitchellandMirellaLapata.2008.Vector-based\nmodelsofsemanticcomposition.In\nProceedingsof\nACL\n,volume8.\nJeffMitchellandMirellaLapata.2010.Composition\nindistributionalmodelsofsemantics.\nCognitiveSci-\nence\n,34(8):13881429.\nJordanB.Pollack.1990.Recursivedistributedrepre-\nsentations.\nIntelligence\n,46:77105.\nHolgerSchwenk.2012.Continuousspacetranslation\nmodelsforphrase-basedstatisticalmachinetransla-\ntion.In\nCOLING(Posters)\n,pages10711080.\nJooSilva,LusaCoheur,AnaCristinaMendes,andAn-\ndreasWichert.2011.Fromsymbolictosub-\nsymbolicinformationinquestion\nAr-\nIntelligenceReview\n,35(2):137154.\nRichardSocher,JeffreyPennington,EricH.Huang,\nAndrewY.Ng,andChristopherD.Manning.2011.\nSemi-SupervisedRecursiveAutoencodersforPre-\ndictingSentimentDistributions.In\nProceedingsof\nthe2011ConferenceonEmpiricalMethodsinNat-\nuralLanguageProcessing(EMNLP)\n.\nRichardSocher,QuocV.Le,ChristopherD.Manning,\nandAndrewY.Ng.2013a.GroundedComposi-\ntionalSemanticsforFindingandDescribingImages\nwithSentences.In\nTransactionsoftheAssociation\nforComputationalLinguistics(TACL)\n.\nRichardSocher,AlexPerelygin,JeanWu,Jason\nChuang,ChristopherD.Manning,AndrewY.Ng,\nandChristopherPotts.2013b.Recursivedeepmod-\nelsforsemanticcompositionalityoverasentiment\ntreebank.In\nProceedingsofthe2013Conferenceon\nEmpiricalMethodsinNaturalLanguageProcess-\ning\n,pages16311642,Stroudsburg,PA,October.\nAssociationforComputationalLinguistics.\nJosephTurian,LevRatinov,andYoshuaBengio.2010.\nWordrepresentations:asimpleandgeneralmethod\nforsemi-supervisedlearning.In\nProceedingsofthe\n48thAnnualMeetingoftheAssociationforCompu-\ntationalLinguistics\n,pages384394.Associationfor\nComputationalLinguistics.\nPeterTurney.2012.Domainandfunction:Adual-\nspacemodelofsemanticrelationsandcompositions.\nJ.Artif.Intell.Res.(JAIR)\n,44:533585.\nAlexanderWaibel,ToshiyukiHanazawa,GeofreyHin-\nton,KiyohiroShikano,andKevinJ.Lang.1990.\nReadingsinspeechrecognition.chapterPhoneme\nRecognitionUsingTime-delayNeuralNetworks,\npages393404.MorganKaufmannPublishersInc.,\nSanFrancisco,CA,USA.\nFabioMassimoZanzotto,IoannisKorkontzelos,\nFrancescaFallucchi,andSureshManandhar.2010.\nEstimatinglinearmodelsforcompositionaldistri-\nbutionalsemantics.In\nProceedingsofthe23rdIn-\nternationalConferenceonComputationalLinguis-\ntics\n,pages12631271.AssociationforComputa-\ntionalLinguistics.\nLukeS.ZettlemoyerandMichaelCollins.2005.\nLearningtomapsentencestologicalform:Struc-\nturedwithprobabilisticcategorial\ngrammars.In\nUAI\n,pages658666.AUAIPress.\n'