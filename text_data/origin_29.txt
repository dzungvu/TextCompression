b'DeepResidualLearningforImageRecognition\nKaimingHeXiangyuZhangShaoqingRenJianSun\nMicrosoftResearch\n{kahe,v-xiangz,v-shren,jiansun\n}@microsoft.comAbstractDeeperneuralnetworksaremoredifculttotrain.We\npresentaresiduallearningframeworktoeasethetraining\n\nofnetworksthataresubstantiallydeeperthanthoseused\n\npreviously.Weexplicitlyreformulatethelayersaslearn-\n\ningresidualfunctionswithreferencetothelayerinputs,in-\n\nsteadoflearningunreferencedfunctions.Weprovidecom-\n\nprehensiveempiricalevidenceshowingthattheseresidual\n\nnetworksareeasiertooptimize,andcangainaccuracyfrom\n\nconsiderablyincreaseddepth.OntheImageNetdatasetwe\n\nevaluateresidualnetswithadepthofupto152layers8\ndeeperthanVGGnets[40]butstillhavinglowercomplex-\n\nity.Anensembleoftheseresidualnetsachieves3.57%error\n\nontheImageNet\ntestset.Thisresultwonthe1stplaceonthe\nILSVRC2015classicationtask.Wealsopresentanalysis\n\nonCIFAR-10with100and1000layers.\nThedepthofrepresentationsisofcentralimportance\nformanyvisualrecognitiontasks.Solelyduetoourex-\n\ntremelydeeprepresentations,weobtaina28%relativeim-\n\nprovementontheCOCOobjectdetectiondataset.Deep\n\nresidualnetsarefoundationsofoursubmissionstoILSVRC\n\n&COCO2015competitions\n1,wherewealsowonthe1st\nplacesonthetasksofImageNetdetection,ImageNetlocal-\n\nization,COCOdetection,andCOCOsegmentation.\n\n1.Introduction\nDeepconvolutionalneuralnetworks[22,21]haveled\ntoaseriesofbreakthroughsforimageclassication[21,\n\n49,39].Deepnetworksnaturallyintegratelow/mid/high-\n\nlevelfeatures[49]andclassiersinanend-to-endmulti-\n\nlayerfashion,andthelevelsoffeaturescanbeenriched\n\nbythenumberofstackedlayers(depth).Recentevidence\n\n[40,43]revealsthatnetworkdepthisofcrucialimportance,\n\nandtheleadingresults[40,43,12,16]onthechallenging\n\nImageNetdataset[35]allexploitverydeep[40]models,\n\nwithadepthofsixteen[40]tothirty[16].Manyothernon-\n\ntrivialvisualrecognitiontasks[7,11,6,32,27]havealso\n1http://image-net.org/challenges/LSVRC/2015/\nandhttp://mscoco.org/dataset/#detections-challenge2015\n.012345601020iter. (1e4)\ntraining error (%)\n012345601020iter. (1e4)\ntest error (%)\n56-layer\n\n20-layer\n56-layer\n\n20-layer\nFigure1.Trainingerror(left)andtesterror(right)onCIFAR-10\n\nwith20-layerand56-layerplainnetworks.Thedeepernetwork\n\nhashighertrainingerror,andthustesterror.Similarphenomena\n\nonImageNetispresentedinFig.4.\n\ngreatlybenetedfromverydeepmodels.\nDrivenbythesignicanceofdepth,aquestionarises:\nIslearningbetternetworksaseasyasstackingmorelayers?\n\nAnobstacletoansweringthisquestionwasthenotorious\n\nproblemofvanishing/explodinggradients[14,1,8],which\n\nhamperconvergencefromthebeginning.Thisproblem,\n\nhowever,hasbeenlargelyaddressedbynormalizedinitial-\n\nization[23,8,36,12]andintermediatenormalizationlayers\n\n[16],whichenablenetworkswithtensoflayerstostartcon-\n\nvergingforstochasticgradientdescent(SGD)withback-\n\npropagation[22].\nWhendeepernetworksareabletostartconverging,a\ndegradation\nproblemhasbeenexposed:withthenetwork\ndepthincreasing,accuracygetssaturated(whichmightbe\n\nunsurprising)andthendegradesrapidly.Unexpectedly,\n\nsuchdegradationis\nnotcausedbyovertting\n,andadding\nmorelayerstoasuitablydeepmodelleadsto\nhighertrain-\ningerror\n,asreportedin[10,41]andthoroughlyveriedby\nourexperiments.Fig.1showsatypicalexample.\nThedegradation(oftrainingaccuracy)indicatesthatnot\nallsystemsaresimilarlyeasytooptimize.Letusconsidera\n\nshallowerarchitectureanditsdeepercounterpartthatadds\n\nmorelayersontoit.Thereexistsasolution\nbyconstruction\ntothedeepermodel:theaddedlayersare\nidentitymapping,andtheotherlayersarecopiedfromthelearnedshallower\n\nmodel.Theexistenceofthisconstructedsolutionindicates\n\nthatadeepermodelshouldproducenohighertrainingerror\n\nthanitsshallowercounterpart.Butexperimentsshowthat\n\nourcurrentsolversonhandareunabletondsolutionsthat\n1770\n'b'\n\n\n\n\n\nF(x)+xxF(x)xFigure2.Residuallearning:abuildingblock.\narecomparablygoodorbetterthantheconstructedsolution\n\n(orunabletodosoinfeasibletime).\nInthispaper,weaddressthedegradationproblemby\nintroducinga\ndeepresiduallearning\nframework.In-\nsteadofhopingeachfewstackedlayersdirectlyta\n\ndesiredunderlyingmapping,weexplicitlylettheselay-\n\nerstaresidualmapping.Formally,denotingthedesired\n\nunderlyingmappingas\nH(x),weletthestackednonlinear\nlayerstanothermappingof\nF(x):=\nH(x)x.Theorig-\ninalmappingisrecastinto\nF(x)+x.Wehypothesizethatit\niseasiertooptimizetheresidualmappingthantooptimize\n\ntheoriginal,unreferencedmapping.Totheextreme,ifan\n\nidentitymappingwereoptimal,itwouldbeeasiertopush\n\ntheresidualtozerothantotanidentitymappingbyastack\n\nofnonlinearlayers.\nTheformulationof\nF(x)+xcanberealizedbyfeedfor-\nwardneuralnetworkswithshortcutconnections(Fig.2).\n\nShortcutconnections[2,33,48]arethoseskippingoneor\n\nmorelayers.Inourcase,theshortcutconnectionssimply\n\nperformidentitymapping,andtheiroutputsareaddedto\ntheoutputsofthestackedlayers(Fig.2).Identityshort-\n\ncutconnectionsaddneitherextraparameternorcomputa-\n\ntionalcomplexity.Theentirenetworkcanstillbetrained\n\nend-to-endbySGDwithbackpropagation,andcanbeeas-\n\nilyimplementedusingcommonlibraries(\ne.g\n.,Caffe[19])\nwithoutmodifyingthesolvers.\nWepresentcomprehensiveexperimentsonImageNet\n[35]toshowthedegradationproblemandevaluateour\n\nmethod.Weshowthat:1)Ourextremelydeepresidualnets\n\nareeasytooptimize,butthecounterpartplainnets(that\n\nsimplystacklayers)exhibithighertrainingerrorwhenthe\n\ndepthincreases;2)Ourdeepresidualnetscaneasilyenjoy\n\naccuracygainsfromgreatlyincreaseddepth,producingre-\n\nsultssubstantiallybetterthanpreviousnetworks.\nSimilarphenomenaarealsoshownontheCIFAR-10set\n[20],suggestingthattheoptimizationdifcultiesandthe\n\neffectsofourmethodarenotjustakintoaparticulardataset.\n\nWepresentsuccessfullytrainedmodelsonthisdatasetwith\n\nover100layers,andexploremodelswithover1000layers.\nOntheImageNetclassicationdataset[35],weobtain\nexcellentresultsbyextremelydeepresidualnets.Our152-\n\nlayerresidualnetisthedeepestnetworkeverpresentedon\n\nImageNet,whilestillhavinglowercomplexitythanVGG\n\nnets[40].Ourensemblehas\n3.57%top-5erroronthe\nImageNettestset,and\nwonthe1stplaceintheILSVRC\n2015classicationcompetition\n.Theextremelydeeprep-\nresentationsalsohaveexcellentgeneralizationperformance\n\nonotherrecognitiontasks,andleadustofurther\nwinthe\n1stplaceson:ImageNetdetection,ImageNetlocalization,\n\nCOCOdetection,andCOCOsegmentation\ninILSVRC&\nCOCO2015competitions.Thisstrongevidenceshowsthat\n\ntheresiduallearningprincipleisgeneric,andweexpectthat\n\nitisapplicableinothervisionandnon-visionproblems.\n2.RelatedWork\n\nResidualRepresentations.\nInimagerecognition,VLAD\n[18]isarepresentationthatencodesbytheresidualvectors\n\nwithrespecttoadictionary,andFisherVector[30]canbe\n\nformulatedasaprobabilisticversion[18]ofVLAD.Both\n\nofthemarepowerfulshallowrepresentationsforimagere-\n\ntrievalandclassication[4,47].Forvectorquantization,\n\nencodingresidualvectors[17]isshowntobemoreeffec-\n\ntivethanencodingoriginalvectors.\nInlow-levelvisionandcomputergraphics,forsolv-\ningPartialDifferentialEquations(PDEs),thewidelyused\n\nMultigridmethod[3]reformulatesthesystemassubprob-\n\nlemsatmultiplescales,whereeachsubproblemisrespon-\n\nsiblefortheresidualsolutionbetweenacoarserandaner\n\nscale.AnalternativetoMultigridishierarchicalbasispre-\n\nconditioning[44,45],whichreliesonvariablesthatrepre-\n\nsentresidualvectorsbetweentwoscales.Ithasbeenshown\n\n[3,44,45]thatthesesolversconvergemuchfasterthanstan-\n\ndardsolversthatareunawareoftheresidualnatureofthe\n\nsolutions.Thesemethodssuggestthatagoodreformulation\n\norpreconditioningcansimplifytheoptimization.\n\nShortcutConnections.\nPracticesandtheoriesthatleadto\nshortcutconnections[2,33,48]havebeenstudiedforalong\n\ntime.Anearlypracticeoftrainingmulti-layerperceptrons\n\n(MLPs)istoaddalinearlayerconnectedfromthenetwork\n\ninputtotheoutput[33,48].In[43,24],afewinterme-\n\ndiatelayersaredirectlyconnectedtoauxiliaryclassiers\n\nforaddressingvanishing/explodinggradients.Thepapers\n\nof[38,37,31,46]proposemethodsforcenteringlayerre-\n\nsponses,gradients,andpropagatederrors,implementedby\n\nshortcutconnections.In[43],aninceptionlayeriscom-\n\nposedofashortcutbranchandafewdeeperbranches.\nConcurrentwithourwork,highwaynetworks[41,42]\npresentshortcutconnectionswithgatingfunctions[15].\n\nThesegatesaredata-dependentandhaveparameters,in\n\ncontrasttoouridentityshortcutsthatareparameter-free.\n\nWhenagatedshortcutisclosed(approachingzero),the\n\nlayersinhighwaynetworksrepresent\nnon-residual\nfunc-tions.Onthecontrary,ourformulationalwayslearns\n\nresidualfunctions;ouridentityshortcutsareneverclosed,\n\nandallinformationisalwayspassedthrough,withaddi-\n\ntionalresidualfunctionstobelearned.Inaddition,high-\n2771\n'b'waynetworkshavenotdemonstratedaccuracygainswith\n\nextremelyincreaseddepth(\ne.g\n.,over100layers).\n3.DeepResidualLearning\n\n3.1.ResidualLearning\nLetusconsider\nH(x)asanunderlyingmappingtobe\ntbyafewstackedlayers(notnecessarilytheentirenet),\n\nwithxdenotingtheinputstotherstoftheselayers.Ifone\nhypothesizesthatmultiplenonlinearlayerscanasymptoti-\n\ncallyapproximatecomplicatedfunctions\n2,thenitisequiv-\nalenttohypothesizethattheycanasymptoticallyapproxi-\n\nmatetheresidualfunctions,\ni.e.,H(x)x(assumingthat\ntheinputandoutputareofthesamedimensions).So\n\nratherthanexpectstackedlayerstoapproximate\nH(x),we\nexplicitlylettheselayersapproximatearesidualfunction\n\nF(x):=\nH(x)x.Theoriginalfunctionthusbecomes\nF(x)+x.Althoughbothformsshouldbeabletoasymptot-\nicallyapproximatethedesiredfunctions(ashypothesized),\n\ntheeaseoflearningmightbedifferent.\nThisreformulationismotivatedbythecounterintuitive\nphenomenaaboutthedegradationproblem(Fig.1,left).As\n\nwediscussedintheintroduction,iftheaddedlayerscan\n\nbeconstructedasidentitymappings,adeepermodelshould\n\nhavetrainingerrornogreaterthanitsshallowercounter-\n\npart.Thedegradationproblemsuggeststhatthesolvers\n\nmighthavedifcultiesinapproximatingidentitymappings\n\nbymultiplenonlinearlayers.Withtheresiduallearningre-\n\nformulation,ifidentitymappingsareoptimal,thesolvers\n\nmaysimplydrivetheweightsofthemultiplenonlinearlay-\n\nerstowardzerotoapproachidentitymappings.\nInrealcases,itisunlikelythatidentitymappingsareop-\ntimal,butourreformulationmayhelptopreconditionthe\n\nproblem.Iftheoptimalfunctionisclosertoanidentity\n\nmappingthantoazeromapping,itshouldbeeasierforthe\n\nsolvertondtheperturbationswithreferencetoanidentity\n\nmapping,thantolearnthefunctionasanewone.Weshow\n\nbyexperiments(Fig.7)thatthelearnedresidualfunctionsin\n\ngeneralhavesmallresponses,suggestingthatidentitymap-\n\npingsprovidereasonablepreconditioning.\n3.2.IdentityMappingbyShortcuts\nWeadoptresiduallearningtoeveryfewstackedlayers.\nAbuildingblockisshowninFig.2.Formally,inthispaper\n\nweconsiderabuildingblockdenedas:\ny=F(x,{Wi})+x.(1)Herexandyaretheinputandoutputvectorsofthelay-\nersconsidered.Thefunction\nF(x,{Wi})representsthe\nresidualmappingtobelearned.FortheexampleinFig.2\n\nthathastwolayers,\nF=W2(W1x)inwhich\ndenotes2Thishypothesis,however,isstillanopenquestion.See[28].\nReLU[29]andthebiasesareomittedforsimplifyingno-\n\ntations.Theoperation\nF+xisperformedbyashortcut\nconnectionandelement-wiseaddition.Weadoptthesec-\n\nondnonlinearityaftertheaddition(\ni.e.,(y),seeFig.2).\nTheshortcutconnectionsinEqn.(1)introduceneitherex-\ntraparameternorcomputationcomplexity.Thisisnotonly\n\nattractiveinpracticebutalsoimportantinourcomparisons\n\nbetweenplainandresidualnetworks.Wecanfairlycom-\n\npareplain/residualnetworksthatsimultaneouslyhavethe\n\nsamenumberofparameters,depth,width,andcomputa-\n\ntionalcost(exceptforthenegligibleelement-wiseaddition).\nThedimensionsof\nxandFmustbeequalinEqn.(1).\nIfthisisnotthecase(\ne.g\n.,whenchangingtheinput/output\nchannels),wecanperformalinearprojection\nWsbythe\nshortcutconnectionstomatchthedimensions:\ny=F(x,{Wi})+Wsx.(2)Wecanalsouseasquarematrix\nWsinEqn.(1).Butwewill\nshowbyexperimentsthattheidentitymappingissufcient\n\nforaddressingthedegradationproblemandiseconomical,\n\nandthus\nWsisonlyusedwhenmatchingdimensions.\nTheformoftheresidualfunction\nFisexible.Exper-\nimentsinthispaperinvolveafunction\nFthathastwoor\nthreelayers(Fig.5),whilemorelayersarepossible.Butif\n\nFhasonlyasinglelayer,Eqn.(1)issimilartoalinearlayer:\ny=W1x+x,forwhichwehavenotobservedadvantages.\nWealsonotethatalthoughtheabovenotationsareabout\nfully-connectedlayersforsimplicity,theyareapplicableto\n\nconvolutionallayers.Thefunction\nF(x,{Wi})canrepre-\nsentmultipleconvolutionallayers.Theelement-wiseaddi-\n\ntionisperformedontwofeaturemaps,channelbychannel.\n\n3.3.NetworkArchitectures\nWehavetestedvariousplain/residualnets,andhaveob-\nservedconsistentphenomena.Toprovideinstancesfordis-\n\ncussion,wedescribetwomodelsforImageNetasfollows.\n\nPlainNetwork.\nOurplainbaselines(Fig.3,middle)are\nmainlyinspiredbythephilosophyofVGGnets[40](Fig.3,\n\nleft).Theconvolutionallayersmostlyhave3\n3ltersand\nfollowtwosimpledesignrules:(i)forthesameoutput\n\nfeaturemapsize,thelayershavethesamenumberofl-\n\nters;and(ii)ifthefeaturemapsizeishalved,thenum-\n\nberofltersisdoubledsoastopreservethetimecom-\n\nplexityperlayer.Weperformdownsamplingdirectlyby\n\nconvolutionallayersthathaveastrideof2.Thenetwork\n\nendswithaglobalaveragepoolinglayeranda1000-way\n\nfully-connectedlayerwithsoftmax.Thetotalnumberof\n\nweightedlayersis34inFig.3(middle).\nItisworthnoticingthatourmodelhas\nfewer\nltersand\nlowercomplexitythanVGGnets[40](Fig.3,left).Our34-\nlayerbaselinehas3.6billionFLOPs(multiply-adds),which\n\nisonly18%ofVGG-19(19.6billionFLOPs).\n3772\n'b'\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure3.ExamplenetworkarchitecturesforImageNet.\nLeft:the\nVGG-19model[40](19.6billionFLOPs)asareference.\nMid-dle:aplainnetworkwith34parameterlayers(3.6billionFLOPs).\nRight:aresidualnetworkwith34parameterlayers(3.6billion\nFLOPs).Thedottedshortcutsincreasedimensions.\nTable1\nshows\nmoredetailsandothervariants.\nResidualNetwork.\nBasedontheaboveplainnetwork,we\ninsertshortcutconnections(Fig.3,right)whichturnthe\n\nnetworkintoitscounterpartresidualversion.Theidentity\n\nshortcuts(Eqn.(1))canbedirectlyusedwhentheinputand\n\noutputareofthesamedimensions(solidlineshortcutsin\n\nFig.3).Whenthedimensionsincrease(dottedlineshortcuts\n\ninFig.3),weconsidertwooptions:(A)Theshortcutstill\n\nperformsidentitymapping,withextrazeroentriespadded\n\nforincreasingdimensions.Thisoptionintroducesnoextra\n\nparameter;(B)TheprojectionshortcutinEqn.(2)isusedto\n\nmatchdimensions(doneby1\n1convolutions).Forboth\noptions,whentheshortcutsgoacrossfeaturemapsoftwo\n\nsizes,theyareperformedwithastrideof2.\n\n3.4.Implementation\nOurimplementationforImageNetfollowsthepractice\nin[21,40].Theimageisresizedwithitsshortersideran-\n\ndomlysampledin\n[256,480]forscaleaugmentation[40].\nA224\n224cropisrandomlysampledfromanimageorits\nhorizontalip,withtheper-pixelmeansubtracted[21].The\n\nstandardcoloraugmentationin[21]isused.Weadoptbatch\n\nnormalization(BN)[16]rightaftereachconvolutionand\n\nbeforeactivation,following[16].Weinitializetheweights\n\nasin[12]andtrainallplain/residualnetsfromscratch.We\n\nuseSGDwithamini-batchsizeof256.Thelearningrate\n\nstartsfrom0.1andisdividedby10whentheerrorplateaus,\n\nandthemodelsaretrainedforupto\n60104iterations.We\nuseaweightdecayof0.0001andamomentumof0.9.We\n\ndonotusedropout[13],followingthepracticein[16].\nIntesting,forcomparisonstudiesweadoptthestandard\n10-croptesting[21].Forbestresults,weadoptthefully-\n\nconvolutionalformasin[40,12],andaveragethescores\n\natmultiplescales(imagesareresizedsuchthattheshorter\n\nsideisin\n{224,256,384,480,640}).4.Experiments\n\n4.1.ImageNetClassication\nWeevaluateourmethodontheImageNet2012classi-\ncationdataset[35]thatconsistsof1000classes.Themodels\n\naretrainedonthe1.28milliontrainingimages,andevalu-\n\natedonthe50kvalidationimages.Wealsoobtainanal\n\nresultonthe100ktestimages,reportedbythetestserver.\n\nWeevaluatebothtop-1andtop-5errorrates.\n\nPlainNetworks.\nWerstevaluate18-layerand34-layer\nplainnets.The34-layerplainnetisinFig.3(middle).The\n\n18-layerplainnetisofasimilarform.SeeTable1forde-\n\ntailedarchitectures.\nTheresultsinTable2showthatthedeeper34-layerplain\nnethashighervalidationerrorthantheshallower18-layer\n\nplainnet.Torevealthereasons,inFig.4(left)wecom-\n\nparetheirtraining/validationerrorsduringthetrainingpro-\n\ncedure.Wehaveobservedthedegradationproblem-the\n4773\n'b'layername\noutputsize\n18-layer34-layer50-layer101-layer152-layerconv1\n11211277,64,stride2\nconv2\nx565633maxpool,stride2\n33,64\n33,64\n233,64\n33,64\n3\n11,64\n33,64\n11,256\n\n3\n11,64\n33,64\n11,256\n\n3\n11,64\n33,64\n11,256\n\n3conv3\nx282833,128\n33,128\n233,128\n33,128\n4\n11,128\n33,128\n11,512\n\n4\n11,128\n33,128\n11,512\n\n4\n11,128\n33,128\n11,512\n\n8conv4\nx141433,256\n33,256\n233,256\n33,256\n6\n11,256\n33,256\n11,1024\n\n6\n11,256\n33,256\n11,1024\n\n23\n11,256\n33,256\n11,1024\n\n36conv5\nx7733,512\n33,512\n233,512\n33,512\n3\n11,512\n33,512\n11,2048\n\n3\n11,512\n33,512\n11,2048\n\n3\n11,512\n33,512\n11,2048\n\n311averagepool,1000-dfc,softmax\nFLOPs1.81093.61093.81097.610911.3109Table1.ArchitecturesforImageNet.Buildingblocksareshowninbrackets(seealsoFig.5),withthenumbersofblocksstacked.Down-\n\nsamplingisperformedbyconv3\n1,conv4\n1,andconv5\n1withastrideof2.\n010203040502030405060iter. (1e4)\nerror (%)\nplain-18\nplain-34\n010203040502030405060iter. (1e4)\nerror (%)\nResNet-18\nResNet-34\n18-layer\n34-layer\n18-layer\n34-layer\nFigure4.Trainingon\nImageNet.Thincurvesdenotetrainingerror,andboldcurvesdenotevalidationerrorofthecentercrops.Left:plain\nnetworksof18and34layers.Right:ResNetsof18and34layers.Inthisplot,theresidualnetworkshavenoextraparametercomparedto\n\ntheirplaincounterparts.\nplainResNet18layers\n27.9427.8834layers\n28.5425.03Table2.Top-1error(%,10-croptesting)onImageNetvalidation.\n\nHeretheResNetshavenoextraparametercomparedtotheirplain\n\ncounterparts.Fig.4showsthetrainingprocedures.\n\n34-layerplainnethashigher\ntraining\nerrorthroughoutthe\nwholetrainingprocedure,eventhoughthesolutionspace\n\nofthe18-layerplainnetworkisasubspaceofthatofthe\n\n34-layerone.\nWearguethatthisoptimizationdifcultyis\nunlikely\ntobecausedbyvanishinggradients.Theseplainnetworksare\n\ntrainedwithBN[16],whichensuresforwardpropagated\n\nsignalstohavenon-zerovariances.Wealsoverifythatthe\n\nbackwardpropagatedgradientsexhibithealthynormswith\n\nBN.Soneitherforwardnorbackwardsignalsvanish.In\n\nfact,the34-layerplainnetisstillabletoachievecompet-\n\nitiveaccuracy(Table3),suggestingthatthesolverworks\n\ntosomeextent.Weconjecturethatthedeepplainnetsmay\n\nhaveexponentiallylowconvergencerates,whichimpactthe\nreducingofthetrainingerror\n3.Thereasonforsuchopti-\nmizationdifcultieswillbestudiedinthefuture.\n\nResidualNetworks.\nNextweevaluate18-layerand34-\nlayerresidualnets(\nResNets).Thebaselinearchitectures\narethesameastheaboveplainnets,expectthatashortcut\n\nconnectionisaddedtoeachpairof3\n3ltersasinFig.3\n(right).Intherstcomparison(Table2andFig.4right),\n\nweuseidentitymappingforallshortcutsandzero-padding\n\nforincreasingdimensions(optionA).Sotheyhave\nnoextra\nparameter\ncomparedtotheplaincounterparts.\nWehavethreemajorobservationsfromTable2and\nFig.4.First,thesituationisreversedwithresiduallearn-\n\ningthe34-layerResNetisbetterthanthe18-layerResNet\n\n(by2.8%).Moreimportantly,the34-layerResNetexhibits\n\nconsiderablylowertrainingerrorandisgeneralizabletothe\n\nvalidationdata.Thisindicatesthatthedegradationproblem\n\niswelladdressedinthissettingandwemanagetoobtain\n\naccuracygainsfromincreaseddepth.\nSecond,comparedtoitsplaincounterpart,the34-layer\n3Wehaveexperimentedwithmoretrainingiterations(3\n)andstillob-\nservedthedegradationproblem,suggestingthatthisproblemcannotbe\n\nfeasiblyaddressedbysimplyusingmoreiterations.\n5774\n'b'modeltop-1err.top-5err.\nVGG-16[40]\n28.079.33\nGoogLeNet[43]\n-9.15\nPReLU-net[12]\n24.277.38\nplain-3428.5410.02\nResNet-34A\n25.037.76\nResNet-34B\n24.527.46\nResNet-34C\n24.197.40\nResNet-5022.856.71\nResNet-10121.756.05\nResNet-15221.435.71\nTable3.Errorrates(%,\n10-crop\ntesting)onImageNetvalidation.\nVGG-16isbasedonourtest.ResNet-50/101/152areofoptionB\n\nthatonlyusesprojectionsforincreasingdimensions.\nmethodtop-1err.top-5err.\nVGG[40](ILSVRC14)\n-8.43\nGoogLeNet[43](ILSVRC14)\n-7.89\nVGG[40]\n(v5)24.47.1\nPReLU-net[12]\n21.595.71\nBN-inception[16]\n21.995.81\nResNet-34B\n21.845.71\nResNet-34C\n21.535.60\nResNet-5020.745.25\nResNet-10119.874.60\nResNet-15219.384.49\nTable4.Errorrates(%)of\nsingle-modelresultsontheImageNet\nvalidationset(except\nreportedonthetestset).\nmethodtop-5err.(\ntest)VGG[40](ILSVRC14)\n7.32GoogLeNet[43](ILSVRC14)\n6.66VGG[40]\n(v5)6.8PReLU-net[12]\n4.94BN-inception[16]\n4.82ResNet(ILSVRC15)\n3.57Table5.Errorrates(%)of\nensembles.Thetop-5errorisonthe\ntestsetofImageNetandreportedbythetestserver.\n\nResNetreducesthetop-1errorby3.5%(Table2),resulting\n\nfromthesuccessfullyreducedtrainingerror(Fig.4right\nvs.left).Thiscomparisonveriestheeffectivenessofresidual\n\nlearningonextremelydeepsystems.\nLast,wealsonotethatthe18-layerplain/residualnets\narecomparablyaccurate(Table2),butthe18-layerResNet\n\nconvergesfaster(Fig.4right\nvs.left).Whenthenetisnot\noverlydeep(18layershere),thecurrentSGDsolverisstill\n\nabletondgoodsolutionstotheplainnet.Inthiscase,the\n\nResNeteasestheoptimizationbyprovidingfasterconver-\n\ngenceattheearlystage.\n\nIdentityvs.ProjectionShortcuts.\nWehaveshownthat\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure5.Adeeperresidualfunction\nFforImageNet.Left:a\nbuildingblock(on56\n56featuremaps)asinFig.3forResNet-\n34.Right:abottleneckbuildingblockforResNet-50/101/152.\n\nparameter-free,identityshortcutshelpwithtraining.Next\n\nweinvestigateprojectionshortcuts(Eqn.(2)).InTable3we\n\ncomparethreeoptions:(A)zero-paddingshortcutsareused\n\nforincreasingdimensions,andallshortcutsareparameter-\n\nfree(thesameasTable2andFig.4right);(B)projec-\n\ntionshortcutsareusedforincreasingdimensions,andother\n\nshortcutsareidentity;and(C)allshortcutsareprojections.\nTable3showsthatallthreeoptionsareconsiderablybet-\nterthantheplaincounterpart.BisslightlybetterthanA.We\n\narguethatthisisbecausethezero-paddeddimensionsinA\n\nindeedhavenoresiduallearning.Cismarginallybetterthan\n\nB,andweattributethistotheextraparametersintroduced\n\nbymany(thirteen)projectionshortcuts.Butthesmalldif-\n\nferencesamongA/B/Cindicatethatprojectionshortcutsare\n\nnotessentialforaddressingthedegradationproblem.Sowe\n\ndonotuseoptionCintherestofthispaper,toreducemem-\n\nory/timecomplexityandmodelsizes.Identityshortcutsare\n\nparticularlyimportantfornotincreasingthecomplexityof\n\nthebottleneckarchitecturesthatareintroducedbelow.\n\nDeeperBottleneckArchitectures.\nNextwedescribeour\ndeepernetsforImageNet.Becauseofconcernsonthetrain-\n\ningtimethatwecanafford,wemodifythebuildingblock\n\nasa\nbottleneck\ndesign4.Foreachresidualfunction\nF,we\nuseastackof3layersinsteadof2(Fig.5).Thethreelayers\n\nare1\n1,3\n3,and1\n1convolutions,wherethe1\n1layers\nareresponsibleforreducingandthenincreasing(restoring)\n\ndimensions,leavingthe3\n3layerabottleneckwithsmaller\ninput/outputdimensions.Fig.5showsanexample,where\n\nbothdesignshavesimilartimecomplexity.\nTheparameter-freeidentityshortcutsareparticularlyim-\nportantforthebottleneckarchitectures.Iftheidentityshort-\n\ncutinFig.5(right)isreplacedwithprojection,onecan\n\nshowthatthetimecomplexityandmodelsizearedoubled,\n\nastheshortcutisconnectedtothetwohigh-dimensional\n\nends.Soidentityshortcutsleadtomoreefcientmodels\n\nforthebottleneckdesigns.\n50-layerResNet:\nWereplaceeach2-layerblockinthe\n4Deepernon-bottleneckResNets(\ne.g\n.,Fig.5left)alsogainaccuracy\nfromincreaseddepth(asshownonCIFAR-10),butarenotaseconomical\n\nasthebottleneckResNets.Sotheusageofbottleneckdesignsismainlydue\n\ntopracticalconsiderations.Wefurthernotethatthedegradationproblem\n\nofplainnetsisalsowitnessedforthebottleneckdesigns.\n6775\n'b'34-layernetwiththis3-layerbottleneckblock,resultingin\n\na50-layerResNet(Table1).WeuseoptionBforincreasing\n\ndimensions.Thismodelhas3.8billionFLOPs.\n101-layerand152-layerResNets:\nWeconstruct101-\nlayerand152-layerResNetsbyusingmore3-layerblocks\n\n(Table1).Remarkably,althoughthedepthissignicantly\n\nincreased,the152-layerResNet(11.3billionFLOPs)still\n\nhaslowercomplexity\nthanVGG-16/19nets(15.3/19.6bil-\nlionFLOPs).\nThe50/101/152-layerResNetsaremoreaccuratethan\nthe34-layeronesbyconsiderablemargins(Table3and4).\n\nWedonotobservethedegradationproblemandthusen-\n\njoysignicantaccuracygainsfromconsiderablyincreased\n\ndepth.Thebenetsofdeptharewitnessedforallevaluation\n\nmetrics(Table3and4).\n\nComparisonswithState-of-the-artMethods.\nInTable4\nwecomparewiththepreviousbestsingle-modelresults.\n\nOurbaseline34-layerResNetshaveachievedverycompet-\n\nitiveaccuracy.Our152-layerResNethasasingle-model\n\ntop-5validationerrorof4.49%.Thissingle-modelresult\n\noutperformsallpreviousensembleresults(Table5).We\n\ncombinesixmodelsofdifferentdepthtoformanensemble\n\n(onlywithtwo152-layeronesatthetimeofsubmitting).\n\nThisleadsto\n3.57%top-5erroronthetestset(Table5).\nThisentrywonthe1stplaceinILSVRC2015.\n\n4.2.CIFAR-10andAnalysis\nWeconductedmorestudiesontheCIFAR-10dataset\n[20],whichconsistsof50ktrainingimagesand10ktest-\n\ningimagesin10classes.Wepresentexperimentstrained\n\nonthetrainingsetandevaluatedonthetestset.Ourfocus\n\nisonthebehaviorsofextremelydeepnetworks,butnoton\n\npushingthestate-of-the-artresults,soweintentionallyuse\n\nsimplearchitecturesasfollows.\nTheplain/residualarchitecturesfollowtheforminFig.3\n(middle/right).Thenetworkinputsare32\n32images,with\ntheper-pixelmeansubtracted.Therstlayeris3\n3convo-\nlutions.Thenweuseastackof\n6nlayerswith3\n3convo-\nlutionsonthefeaturemapsofsizes\n{32,16,8}respectively,\nwith2\nnlayersforeachfeaturemapsize.Thenumbersof\nltersare\n{16,32,64}respectively.Thesubsamplingisper-\nformedbyconvolutionswithastrideof2.Thenetworkends\n\nwithaglobalaveragepooling,a10-wayfully-connected\n\nlayer,andsoftmax.Therearetotally6\nn+2stackedweighted\nlayers.Thefollowingtablesummarizesthearchitecture:\noutputmapsize\n3232161688#layers\n1+2n2n2n#lters\n163264Whenshortcutconnectionsareused,theyareconnected\n\ntothepairsof3\n3layers(totally\n3nshortcuts).Onthis\ndatasetweuseidentityshortcutsinallcases(\ni.e.,optionA),\nmethoderror(%)\nMaxout[9]\n9.38NIN[25]\n8.81DSN[24]\n8.22#layers\n#params\nFitNet[34]\n192.5M8.39Highway[41,42]\n192.3M7.54(7.720.16)Highway[41,42]\n321.25M8.80ResNet200.27M8.75ResNet320.46M7.51ResNet440.66M7.17ResNet560.85M6.97ResNet1101.7M6.43(6.610.16)ResNet120219.4M7.93Table6.Classicationerroronthe\nCIFAR-10\ntestset.Allmeth-\nodsarewithdataaugmentation.ForResNet-110,werunit5times\n\nandshowbest(mean\nstd)asin[42].\nsoourresidualmodelshaveexactlythesamedepth,width,\n\nandnumberofparametersastheplaincounterparts.\nWeuseaweightdecayof0.0001andmomentumof0.9,\nandadopttheweightinitializationin[12]andBN[16]but\n\nwithnodropout.Thesemodelsaretrainedwithamini-\n\nbatchsizeof128ontwoGPUs.Westartwithalearning\n\nrateof0.1,divideitby10at32kand48kiterations,and\n\nterminatetrainingat64kiterations,whichisdeterminedon\n\na45k/5ktrain/valsplit.Wefollowthesimpledataaugmen-\n\ntationin[24]fortraining:4pixelsarepaddedoneachside,\n\nanda32\n32cropisrandomlysampledfromthepadded\nimageoritshorizontalip.Fortesting,weonlyevaluate\n\nthesingleviewoftheoriginal32\n32image.\nWecompare\nn={3,5,7,9},leadingto20,32,44,and\n56-layernetworks.Fig.6(left)showsthebehaviorsofthe\n\nplainnets.Thedeepplainnetssufferfromincreaseddepth,\n\nandexhibithighertrainingerrorwhengoingdeeper.This\n\nphenomenonissimilartothatonImageNet(Fig.4,left)and\n\nonMNIST(see[41]),suggestingthatsuchanoptimization\n\ndifcultyisafundamentalproblem.\nFig.6(middle)showsthebehaviorsofResNets.Also\nsimilartotheImageNetcases(Fig.4,right),ourResNets\n\nmanagetoovercometheoptimizationdifcultyanddemon-\n\nstrateaccuracygainswhenthedepthincreases.\nWefurtherexplore\nn=18\nthatleadstoa110-layer\nResNet.Inthiscase,wendthattheinitiallearningrate\n\nof0.1isslightlytoolargetostartconverging\n5.Soweuse\n0.01towarmupthetraininguntilthetrainingerrorisbelow\n\n80%(about400iterations),andthengobackto0.1andcon-\n\ntinuetraining.Therestofthelearningscheduleisasdone\n\npreviously.This110-layernetworkconvergeswell(Fig.6,\n\nmiddle).Ithas\nfewer\nparametersthanotherdeepandthin\n5Withaninitiallearningrateof0.1,itstartsconverging(\n<90%error)\nafterseveralepochs,butstillreachessimilaraccuracy.\n7776\n'b'0123456051020iter. (1e4)\nerror (%)\nplain-20\nplain-32\nplain-44\nplain-56\n0123456051020iter. (1e4)\nerror (%)\nResNet-20\nResNet-32\nResNet-44\nResNet-56\nResNet-110\n56-layer\n\n20-layer\n110-layer\n20-layer\n4560151020iter. (1e4)\nerror (%)\nresidual-110\nresidual-1202\nFigure6.Trainingon\nCIFAR-10\n.Dashedlinesdenotetrainingerror,andboldlinesdenotetestingerror.\nLeft:plainnetworks.Theerror\nofplain-110ishigherthan60%andnotdisplayed.\nMiddle:ResNets.\nRight:ResNetswith110and1202layers.\n020406080100123layer index (sorted by magnitude)\nstd\nplain-20\nplain-56\nResNet-20\nResNet-56\nResNet-110\n020406080100123layer index (original)\nstd\nplain-20\nplain-56\nResNet-20\nResNet-56\nResNet-110\nFigure7.Standarddeviations(std)oflayerresponsesonCIFAR-\n\n10.Theresponsesaretheoutputsofeach3\n3layer,afterBNand\nbeforenonlinearity.\nTop\n:thelayersareshownintheiroriginal\norder.\nBottom:theresponsesarerankedindescendingorder.\nnetworkssuchasFitNet[34]andHighway[41](Table6),\n\nyetisamongthestate-of-the-artresults(6.43%,Table6).\n\nAnalysisofLayerResponses.\nFig.7showsthestandard\ndeviations(std)ofthelayerresponses.Theresponsesare\n\ntheoutputsofeach3\n3layer,afterBNandbeforeother\nnonlinearity(ReLU/addition).ForResNets,thisanaly-\n\nsisrevealstheresponsestrengthoftheresidualfunctions.\n\nFig.7showsthatResNetshavegenerallysmallerresponses\n\nthantheirplaincounterparts.Theseresultssupportourba-\n\nsicmotivation(Sec.3.1)thattheresidualfunctionsmight\n\nbegenerallyclosertozerothanthenon-residualfunctions.\n\nWealsonoticethatthedeeperResNethassmallermagni-\n\ntudesofresponses,asevidencedbythecomparisonsamong\n\nResNet-20,56,and110inFig.7.Whentherearemore\n\nlayers,anindividuallayerofResNetstendstomodifythe\n\nsignalless.\n\nExploringOver1000layers.\nWeexploreanaggressively\ndeepmodelofover1000layers.Weset\nn=200\nthatleadstoa1202-layernetwork,whichistrainedasdescribed\n\nabove.Ourmethodshows\nnooptimizationdifculty\n,and\nthis103-layernetworkisabletoachieve\ntrainingerror\n<0.1%(Fig.6,right).Itstesterrorisstillfairlygood\n(7.93%,Table6).\nButtherearestillopenproblemsonsuchaggressively\ndeepmodels.Thetestingresultofthis1202-layernetwork\n\nisworsethanthatofour110-layernetwork,althoughboth\ntrainingdata\n07+1207++12testdata\nVOC07test\nVOC12test\nVGG-16\n73.270.4ResNet-10176.473.8Table7.ObjectdetectionmAP(%)onthePASCALVOC\n\n2007/2012testsetsusing\nbaselineFasterR-CNN.Seealsoap-\npendixforbetterresults.\nmetricmAP@.5mAP@[.5,.95]\nVGG-16\n41.521.2ResNet-10148.427.2Table8.ObjectdetectionmAP(%)ontheCOCOvalidationset\n\nusingbaselineFasterR-CNN.Seealsoappendixforbetterresults.\nhavesimilartrainingerror.Wearguethatthisisbecauseof\n\novertting.The1202-layernetworkmaybeunnecessarily\n\nlarge(19.4M)forthissmalldataset.Strongregularization\n\nsuchasmaxout[9]ordropout[13]isappliedtoobtainthe\n\nbestresults([9,25,24,34])onthisdataset.Inthispaper,we\n\nusenomaxout/dropoutandjustsimplyimposeregulariza-\n\ntionviadeepandthinarchitecturesbydesign,withoutdis-\n\ntractingfromthefocusonthedifcultiesofoptimization.\n\nButcombiningwithstrongerregularizationmayimprove\n\nresults,whichwewillstudyinthefuture.\n\n4.3.ObjectDetectiononPASCALandMSCOCO\nOurmethodhasgoodgeneralizationperformanceon\notherrecognitiontasks.Table7and8showtheobjectde-\n\ntectionbaselineresultsonPASCALVOC2007and2012\n\n[5]andCOCO[26].Weadopt\nFasterR-CNN\n[32]asthede-\ntectionmethod.Hereweareinterestedintheimprovements\n\nofreplacingVGG-16[40]withResNet-101.Thedetection\n\nimplementation(seeappendix)ofusingbothmodelsisthe\n\nsame,sothegainscanonlybeattributedtobetternetworks.\n\nMostremarkably,onthechallengingCOCOdatasetweob-\n\ntaina6.0%increaseinCOCOsstandardmetric(mAP@[.5,\n\n.95]),whichisa28%relativeimprovement.Thisgainis\n\nsolelyduetothelearnedrepresentations.\nBasedondeepresidualnets,wewonthe1stplacesin\nseveraltracksinILSVRC&COCO2015competitions:Im-\n\nageNetdetection,ImageNetlocalization,COCOdetection,\n\nandCOCOsegmentation.Thedetailsareintheappendix.\n8777\n'b'References\n[1]Y.Bengio,P.Simard,andP.Frasconi.Learninglong-termdependen-\ncieswithgradientdescentisdifcult.\nIEEETransactionsonNeural\nNetworks,5(2):157166,1994.\n[2]C.M.Bishop.\nNeuralnetworksforpatternrecognition\n.Oxford\nuniversitypress,1995.\n[3]W.L.Briggs,S.F.McCormick,etal.\nAMultigridTutorial\n.Siam,\n2000.[4]K.Chateld,V.Lempitsky,A.Vedaldi,andA.Zisserman.Thedevil\nisinthedetails:anevaluationofrecentfeatureencodingmethods.\n\nInBMVC,2011.\n[5]M.Everingham,L.VanGool,C.K.Williams,J.Winn,andA.Zis-\nserman.ThePascalVisualObjectClasses(VOC)Challenge.\nIJCV,pages303338,2010.\n[6]R.Girshick.FastR-CNN.In\nICCV,2015.\n[7]R.Girshick,J.Donahue,T.Darrell,andJ.Malik.Richfeaturehier-\narchiesforaccurateobjectdetectionandsemanticsegmentation.In\n\nCVPR,2014.\n[8]X.GlorotandY.Bengio.Understandingthedifcultyoftraining\ndeepfeedforwardneuralnetworks.In\nAISTATS\n,2010.\n[9]I.J.Goodfellow,D.Warde-Farley,M.Mirza,A.Courville,and\nY.Bengio.Maxoutnetworks.\narXiv:1302.4389,2013.\n[10]K.HeandJ.Sun.Convolutionalneuralnetworksatconstrainedtime\ncost.In\nCVPR,2015.\n[11]K.He,X.Zhang,S.Ren,andJ.Sun.Spatialpyramidpoolingindeep\nconvolutionalnetworksforvisualrecognition.In\nECCV,2014.\n[12]K.He,X.Zhang,S.Ren,andJ.Sun.Delvingdeepintorectiers:\nSurpassinghuman-levelperformanceonimagenetclassication.In\n\nICCV,2015.\n[13]G.E.Hinton,N.Srivastava,A.Krizhevsky,I.Sutskever,and\nR.R.Salakhutdinov.Improvingneuralnetworksbypreventingco-\n\nadaptationoffeaturedetectors.\narXiv:1207.0580,2012.\n[14]S.Hochreiter.Untersuchungenzudynamischenneuronalennetzen.\nDiplomathesis,TUMunich\n,1991.\n[15]S.HochreiterandJ.Schmidhuber.Longshort-termmemory.\nNeural\ncomputation,9(8):17351780,1997.\n[16]S.IoffeandC.Szegedy.Batchnormalization:Acceleratingdeep\nnetworktrainingbyreducinginternalcovariateshift.In\nICML,2015.\n[17]H.Jegou,M.Douze,andC.Schmid.Productquantizationfornearest\nneighborsearch.\nTPAMI\n,33,2011.\n[18]H.Jegou,F.Perronnin,M.Douze,J.Sanchez,P.Perez,and\nC.Schmid.Aggregatinglocalimagedescriptorsintocompactcodes.\n\nTPAMI\n,2012.\n[19]Y.Jia,E.Shelhamer,J.Donahue,S.Karayev,J.Long,R.Girshick,\nS.Guadarrama,andT.Darrell.Caffe:Convolutionalarchitecturefor\n\nfastfeatureembedding.\narXiv:1408.5093,2014.\n[20]A.Krizhevsky.Learningmultiplelayersoffeaturesfromtinyim-\nages.TechReport\n,2009.\n[21]A.Krizhevsky,I.Sutskever,andG.Hinton.Imagenetclassication\nwithdeepconvolutionalneuralnetworks.In\nNIPS,2012.\n[22]Y.LeCun,B.Boser,J.S.Denker,D.Henderson,R.E.Howard,\nW.Hubbard,andL.D.Jackel.Backpropagationappliedtohand-\n\nwrittenzipcoderecognition.\nNeuralcomputation\n,1989.\n[23]Y.LeCun,L.Bottou,G.B.Orr,andK.-R.M\nuller.Efcientbackprop.\nInNeuralNetworks:TricksoftheTrade\n,pages950.Springer,1998.\n[24]C.-Y.Lee,S.Xie,P.Gallagher,Z.Zhang,andZ.Tu.Deeply-\nsupervisednets.\narXiv:1409.5185,2014.\n[25]M.Lin,Q.Chen,andS.Yan.Networkinnetwork.\narXiv:1312.4400,2013.[26]T.-Y.Lin,M.Maire,S.Belongie,J.Hays,P.Perona,D.Ramanan,\nP.Doll\nar,andC.L.Zitnick.MicrosoftCOCO:Commonobjectsin\ncontext.In\nECCV.2014.\n[27]J.Long,E.Shelhamer,andT.Darrell.Fullyconvolutionalnetworks\nforsemanticsegmentation.In\nCVPR,2015.\n[28]G.Mont\nufar,R.Pascanu,K.Cho,andY.Bengio.Onthenumberof\nlinearregionsofdeepneuralnetworks.In\nNIPS,2014.\n[29]V.NairandG.E.Hinton.Rectiedlinearunitsimproverestricted\nboltzmannmachines.In\nICML,2010.\n[30]F.PerronninandC.Dance.Fisherkernelsonvisualvocabulariesfor\nimagecategorization.In\nCVPR,2007.\n[31]T.Raiko,H.Valpola,andY.LeCun.Deeplearningmadeeasierby\nlineartransformationsinperceptrons.In\nAISTATS\n,2012.\n[32]S.Ren,K.He,R.Girshick,andJ.Sun.FasterR-CNN:Towards\nreal-timeobjectdetectionwithregionproposalnetworks.In\nNIPS,2015.[33]B.D.Ripley.\nPatternrecognitionandneuralnetworks\n.Cambridge\nuniversitypress,1996.\n[34]A.Romero,N.Ballas,S.E.Kahou,A.Chassang,C.Gatta,and\nY.Bengio.Fitnets:Hintsforthindeepnets.In\nICLR,2015.\n[35]O.Russakovsky,J.Deng,H.Su,J.Krause,S.Satheesh,S.Ma,\nZ.Huang,A.Karpathy,A.Khosla,M.Bernstein,etal.Imagenet\n\nlargescalevisualrecognitionchallenge.\narXiv:1409.0575,2014.\n[36]A.M.Saxe,J.L.McClelland,andS.Ganguli.Exactsolutionsto\nthenonlineardynamicsoflearningindeeplinearneuralnetworks.\n\narXiv:1312.6120,2013.\n[37]N.N.Schraudolph.Acceleratedgradientdescentbyfactor-centering\ndecomposition.Technicalreport,1998.\n[38]N.N.Schraudolph.Centeringneuralnetworkgradientfactors.In\nNeuralNetworks:TricksoftheTrade\n,pages207226.Springer,\n1998.[39]P.Sermanet,D.Eigen,X.Zhang,M.Mathieu,R.Fergus,andY.Le-\nCun.Overfeat:Integratedrecognition,localizationanddetection\n\nusingconvolutionalnetworks.In\nICLR,2014.\n[40]K.SimonyanandA.Zisserman.Verydeepconvolutionalnetworks\nforlarge-scaleimagerecognition.In\nICLR,2015.\n[41]R.K.Srivastava,K.Greff,andJ.Schmidhuber.Highwaynetworks.\narXiv:1505.00387,2015.\n[42]R.K.Srivastava,K.Greff,andJ.Schmidhuber.Trainingverydeep\nnetworks.\n1507.06228,2015.\n[43]C.Szegedy,W.Liu,Y.Jia,P.Sermanet,S.Reed,D.Anguelov,D.Er-\nhan,V.Vanhoucke,andA.Rabinovich.Goingdeeperwithconvolu-\n\ntions.In\nCVPR,2015.\n[44]R.Szeliski.Fastsurfaceinterpolationusinghierarchicalbasisfunc-\ntions.TPAMI\n,1990.\n[45]R.Szeliski.Locallyadaptedhierarchicalbasispreconditioning.In\nSIGGRAPH,2006.\n[46]T.Vatanen,T.Raiko,H.Valpola,andY.LeCun.Pushingstochas-\nticgradienttowardssecond-ordermethodsbackpropagationlearn-\n\ningwithtransformationsinnonlinearities.In\nNeuralInformation\nProcessing\n,2013.\n[47]A.VedaldiandB.Fulkerson.VLFeat:Anopenandportablelibrary\nofcomputervisionalgorithms,2008.\n[48]W.VenablesandB.Ripley.Modernappliedstatisticswiths-plus.\n1999.[49]M.D.ZeilerandR.Fergus.Visualizingandunderstandingconvolu-\ntionalneuralnetworks.In\nECCV,2014.\n9778\n'