b'Multi-columnDeepNeuralNetworksforImage\nDanCiresan\nUeliMeier\nJ\n\nurgenSchmidhuber\nTechnicalReportNo.IDSIA-04-12\nFebruary2012\nIDSIA/USI-SUPSI\nDalleMolleInstituteforIntelligence\nGalleria2,6928Manno,Switzerland\nIDSIAisajointinstituteofbothUniversityofLugano(USI)andUniversityofAppliedSciencesofSouthernSwitzerland(SUPSI),andwasfounded\nin1988bytheDalleMolleFoundationwhichpromotedqualityoflife.\narXiv:1202.2745v1  [cs.CV]  13 Feb 2012'b"TechnicalReportNo.IDSIA-04-12\n1\nMulti-columnDeepNeuralNetworksforImage\n\nDanCiresan\nUeliMeier\nJ\n\nurgenSchmidhuber\nFebruary2012\nAbstract\nTraditionalmethodsofcomputervisionandmachinelearningcannotmatchhumanperformanceon\ntaskssuchastherecognitionofhandwrittendigitsortrafsigns.Ourbiologicallyplausibledeeparti-\nneuralnetworkarchitecturescan.Small(oftenminimal)receptiveofconvolutionalwinner-\ntake-allneuronsyieldlargenetworkdepth,resultinginroughlyasmanysparselyconnectedneurallayers\nasfoundinmammalsbetweenretinaandvisualcortex.Onlywinnerneuronsaretrained.Severaldeep\nneuralcolumnsbecomeexpertsoninputspreprocessedindifferentways;theirpredictionsareaveraged.\nGraphicscardsallowforfasttraining.OntheverycompetitiveMNISThandwritingbenchmark,our\nmethodisthetoachievenear-humanperformance.Onatrafsignrecognitionbenchmarkitout-\nperformshumansbyafactoroftwo.Wealsoimprovethestate-of-the-artonaplethoraofcommonimage\nbenchmarks.\n1Introduction\nRecentpublicationssuggestthatunsupervisedpre-trainingofdeep,hierarchicalneuralnetworksimproves\nsupervisedpattern[\n2\n,\n10\n].Herewetrainsuchnetsbysimpleonlineback-propagation,setting\nnew,greatlyimprovedrecordsonMNIST[\n20\n],Latinletters[\n13\n],Chinesecharacters[\n23\n],trafsigns[\n36\n],\nNORB(jittered,cluttered)[\n21\n]andCIFAR10[\n18\n]benchmarks.\nWefocusondeepconvolutionalneuralnetworks(DNN),introducedby[\n11\n],improvedby[\n20\n],\nandby[\n1\n,\n35\n,\n7\n].Lately,DNNprovedtheirmettleondatasetsrangingfromhandwrittendigits\n(MNIST)[\n5\n,\n7\n],handwrittencharacters[\n6\n]to3Dtoys(NORB)andfaces[\n37\n].DNNsfullyunfoldtheir\npotentialwhentheyarebiganddeep[\n7\n].Buttrainingthemrequiresweeks,months,evenyearsonCPUs.\nHighdatatransferlatencypreventsmulti-threadingandmulti-CPUcodefromsavingthesituation.In\nrecentyears,however,fastparallelneuralnetcodeforgraphicscards(GPUs)hasovercomethisproblem.\nCarefullydesignedGPUcodeforimagecanbeuptotwoordersofmagnitudefasterthan\nitsCPUcounterpart[\n38\n,\n37\n].Hence,totrainhugeDNNinhoursordays,weimplementthemonGPU,\nbuildingupontheworkof[\n5\n,\n7\n].Thetrainingalgorithmisfullyonline,i.e.weightupdatesoccuraftereach\nerrorback-propagationstep.WewillshowthatproperlytrainedbiganddeepDNNscanoutperformall\npreviousmethods,anddemonstratethatunsupervisedinitialization/pretrainingisnotnecessary(although\nwedon'tdenythatitmighthelpsometimes,especiallyforsmalldatasets).Wealsoshowhowcombining\nseveralDNNcolumnsintoaMulti-columnDNN(MCDNN)furtherdecreasestheerrorrateby30-40%.\n"b"TechnicalReportNo.IDSIA-04-12\n2\n2Architecture\nTheinitiallyrandomweightsoftheDNNareiterativelytrainedtominimizetheerroronaset\noflabeledtrainingimages;generalizationperformanceisthentestedonaseparatesetoftestimages.Our\narchitecturedoesthisbycombiningseveraltechniquesinanovelway:\n(1)\nUnliketheshallowNNusedinmany1990sapplications,oursaredeep,inspiredbytheNeocogni-\ntron[\n11\n],withmany(6-10)layersofnon-linearneuronsstackedontopofeachother,comparabletothe\nnumberoflayersfoundbetweenretinaandvisualcortexofmacaquemonkeys[\n3\n].\n(2)\nItwasshown[\n14\n]thatsuchmulti-layeredDNNarehardtotrainbystandardgradientdescent\n[\n39\n,\n19\n,\n30\n],themethodofchoicefromamathematical/algorithmicpointofview.Today'scomputers,\nhowever,arefastenoughforthis,morethan60000timesfasterthanthoseoftheearly90s\n1\n.Carefully\ndesignedcodeformassivelyparallelgraphicsprocessingunits(GPUsnormallyusedforvideogames)\nallowsforgaininganadditionalspeedupfactorof50-100overserialcodeforstandardcomputers.Given\nenoughlabeleddata,ournetworksdonotneedadditionalheuristicssuchasunsupervisedpre-training\n[\n31\n,\n26\n,\n2\n,\n10\n]orcarefullyprewiredsynapses[\n29\n,\n34\n].\n(3)\nTheDNNofthispaper(Fig.\n1\na)have2-dimensionallayersofwinner-take-allneurons[\n17\n,\n41\n]with\noverlappingreceptivewhoseweightsareshared[\n20\n,\n1\n,\n35\n,\n7\n].Givensomeinputpattern,asimple\nmaxpoolingtechnique[\n29\n]determineswinningneuronsbypartitioninglayersintoquadraticregionsof\nlocalinhibition,selectingthemostactiveneuronofeachregion.Thewinnersofsomelayerrepresenta\nsmaller,down-sampledlayerwithlowerresolution,feedingthenextlayerinthehierarchy.Theapproach\nisinspiredbyHubelandWiesel'sseminalworkonthecat'sprimaryvisualcortex[\n40\n],whichidenti-\norientation-selective\nsimplecells\nwithoverlappinglocalreceptiveand\ncomplexcells\nperforming\ndown-sampling-likeoperations[\n15\n].\n(4)\nNotethatatsomepointdown-samplingautomaticallyleadstothe1-dimensionallayer.From\nthenon,onlytrivial1-dimensionalwinner-take-allregionsarepossible,thatis,thetoppartofthehierarchy\nbecomesastandardmulti-layerperceptron(MLP)[\n39\n,\n19\n,\n30\n].Receptiveandwinner-take-allregions\nofourDNNoftenare(near-)minimal,e.g.,only2x2or3x3neurons.Thisresultsin(near-)maximaldepth\noflayerswithnon-trivial(2-dimensional)winner-take-allregions.Infact,insistingonminimal2x2\nautomaticallytheentiredeeparchitecture,apartfromthenumberofdifferentconvolutionalkernels\nperlayer[\n20\n,\n1\n,\n35\n,\n7\n]andthedepthoftheplainMLPontop.\n(5)\nOnlywinnerneuronsaretrained,thatis,otherneuronscannotforgetwhattheylearntsofar,although\ntheymaybeaffectedbyweightchangesinmoreperipherallayers.Theresultingdecreaseofsynaptic\nchangespertimeintervalcorrespondstobiologicallyplausiblereductionofenergyconsumption.Our\ntrainingalgorithmisfullyonline,i.e.weightupdatesoccuraftereachgradientcomputationstep.\n(6)\nInspiredbymicrocolumnsofneuronsinthecerebralcortex,wecombineseveralDNNcolumns\ntoformaMulti-columnDNN(MCDNN).Givensomeinputpattern,thepredictionsofallcolumnsare\ndemocraticallyaveraged.Beforetraining,theweights(synapses)ofallcolumnsarerandomlyinitialized.\nVariouscolumnscanbetrainedonthesameinputs,oroninputspreprocessedindifferentways.Thelatter\nhelpstoreducebotherrorrateandnumberofcolumnsrequiredtoreachagivenaccuracy.TheMCDNN\narchitectureanditstrainingandtestingproceduresareillustratedinFigure\n1\n.\n3Experiments\nInthefollowingwegiveadetaileddescriptionofalltheexperimentsweperformed.Weevaluateour\narchitectureonvariouscommonlyusedobjectrecognitionbenchmarksandimprovethestate-of-the-art\nonallofthem.ThedescriptionoftheDNNarchitectureusedforthevariousexperimentsisgiveninthe\nfollowingway:2x48x48-100C5-MP2-100C5-MP2-100C4-MP2-300N-100N-6Nrepresentsanetwith2\ninputimagesofsize48x48,aconvolutionallayerwith100mapsand5x5amax-poolinglayerover\n1\n1991486DX-33MHz,2011i7-990X3.46GHz\n"b'TechnicalReportNo.IDSIA-04-12\n3\n(a)\n(b)\n(c)\nFigure1:(a)DNNarchitecture.(b)MCDNNarchitecture.Theinputimagecanbepreprocessedby\nP\n0\n\nP\nn\n\n1\nblocks.Anarbitrarynumberofcolumnscanbetrainedoninputspreprocessedindifferent\nways.ThepredictionsareobtainedbyaveragingindividualpredictionsofeachDNN.(c)Traininga\nDNN.Thedatasetispreprocessedbeforetraining,then,atthebeginningofeveryepoch,theimagesare\ndistorted(Dblock).Seetextformoreexplanations.\nnonoverlappingregionsofsize2x2,aconvolutionallayerwith100mapsand4x4lters,amax-pooling\nlayerovernonoverlappingregionsofsize2x2,afullyconnectedlayerwith300hiddenunits,afully\nconnectedlayerwith100hiddenunitsandafullyconnectedoutputlayerwith6neurons(oneperclass).\nWeuseascaledhyperbolictangentactivationfunctionforconvolutionalandfullyconnectedlayers,a\nlinearactivationfunctionformax-poolinglayersandasoftmaxactivationfunctionfortheoutputlayer.All\nDNNaretrainedusingon-linegradientdescentwithanannealedlearningrate.Duringtraining,imagesare\ncontinuallytranslated,scaledandrotated(evenelasticallydistortedincaseofcharacters),whereasonlythe\noriginalimagesareusedforvalidation.Trainingendsoncethevalidationerroriszeroorwhenthelearning\nratereachesitspredeterminedminimum.Initialweightsaredrawnfromauniformrandomdistributionin\ntherange\n[\n\n0\n:\n05\n;\n0\n:\n05]\n.\n3.1MNIST\nTheoriginalMNISTdigits[\n20\n]arenormalizedsuchthatthewidthorheightoftheboundingboxequals\n20pixels.Aspectratiosforvariousdigitsvarystronglyandwethereforecreatesixadditionaldatasetsby\n'b'TechnicalReportNo.IDSIA-04-12\n4\nnormalizingdigitwidthto10,12,14,16,18,20pixels.Thisislikeseeingthedatafromdifferentangles.\nWetrainveDNNcolumnspernormalization,resultinginatotalof35columnsfortheentireMCDNN.\nAll1x29x29-20C4-MP2-40C5-MP3-150N-10NDNNaretrainedforaround800epochswithanannealed\nlearningrate(i.e.initializedwith0.001multipliedbyafactorof0.993/epochuntilitreaches0.00003).\nTrainingaDNNtakesalmost14hoursandafter500trainingepochslittleadditionalimprovementisob-\nserved.Duringtrainingthedigitsarerandomlydistortedbeforeeachepoch(seeFig.\n2\naforrepresentative\ncharactersandtheirdistortedversions[\n7\n]).TheinternalstateofasingleDNNisdepictedinFigure\n2\nc,\nwhereaparticulardigitisforwardpropagatedthroughatrainednetworkandallactivationstogetherwith\nthenetworkweightsareplotted.\n(a)\n(b)\n(c)\nFigure2:(a)Handwrittendigitsfromthetrainingset(toprow)andtheirdistortedversionsaftereachepoch\n(secondtorow).(b)The23errorsoftheMCDNN,withcorrectlabel(upright)andandsecond\nbestpredictions(downleftandright).(c)DNNarchitectureforMNIST.Outputlayernotdrawntoscale;\nweightsoffullyconnectedlayersnotdisplayed.\n'b"TechnicalReportNo.IDSIA-04-12\n5\nResultsofallindividualnetsandvariousMCDNNaresummarizedinTable\n1\n.MCDNNof5nets\ntrainedwiththesamepreprocessorachievebetterresultsthantheirconstituentDNNs,exceptfororiginal\nimages(Tab.\n1\n).TheMCDNNhasaverylow0.23%errorrate,improvingstateoftheartbyatleast34%\n[\n5\n,\n7\n,\n27\n](Tab.\n2\n).Thisisthetimeanmethodcomesclosetothe\n\n0.2%errorrateofhumans\nonthistask[\n22\n].Manyofthewronglydigitseithercontainbrokenorstrangestrokes,orhave\nwronglabels.The23errors(Fig.\n2\nb)areassociatedwith20correctsecondguesses.\nWealsotrainedasingleDNNonall7datasetssimultaneouslywhichyieldedworseresult(0.52%)than\nbothMCDNNandtheirindividualDNN.ThisshowsthattheimprovementscomefromtheMCDNNand\nnotfromusingmorepreprocesseddata.\nTable1:Testerrorrate[%]ofthe35NNstrainedonMNIST.Wxx-widthofthecharacterisnormalized\ntoxxpixels\nTrial\nW10W12W14W16W18W20ORIGINAL\n1\n0.490.390.400.400.390.360.52\n2\n0.480.450.450.390.500.410.44\n3\n0.590.510.410.410.380.430.40\n4\n0.550.440.420.430.390.500.53\n5\n0.510.390.480.400.360.290.46\navg.\n0.52\n\n0.050.44\n\n0.050.43\n\n0.030.40\n\n0.020.40\n\n0.060.39\n\n0.080.47\n\n0.05\n35-netaverageerror:0.44\n\n0.06\n5columns\n0.370.260.320.330.310.260.46\nMCDNN\n35-netMCDNN:\n0.23%\nTable2:ResultsonMNISTdataset.\nMethod\nPaperErrorrate[%]\nCNN\n[\n35\n]0.40\nCNN\n[\n28\n]0.39\nMLP\n[\n5\n]0.35\nCNNcommittee\n[\n6\n]0.27\nMCDNN\nthis\n0.23\nHowaretheMCDNNerrorsaffectedbythenumberofpreprocessors?Wetrain5DNNsonall7\ndatasets.AMCDNN'\ny\nout-of-7'(\ny\nfrom1to7)averages\n5\ny\nnetstrainedon\ny\ndatasets.Table\n3\nshowsthat\nmorepreprocessingresultsinlowerMCDNNerror.\nWealsotrain5DNNforeachoddnormalization,i.e.W11,W13,W15,W17andW19.The60-net\nMCDNNperforms(0.24%)similarlytothe35-netMCDNN,indicatingthatadditionalpreprocessingdoes\nnotfurtherimproverecognition.\nWeconcludethatMCDNNoutperformDNNtrainedonthesamedata,andthatdifferentpreprocessors\nfurtherdecreasetheerrorrate.\n3.2NISTSD19\nThe35-columnsMCDNNarchitectureandpreprocessingusedforMNISTarealsoappliedtoLatinchar-\nactersfromNISTSD19[\n13\n].ForalltasksourMCDNNachievesrecognitionrates1.5-5timesbetter\n"b'TechnicalReportNo.IDSIA-04-12\n6\nTable3:Averagetesterrorrate[%]ofMCDNNtrainedon\ny\npreprocesseddatasets.\ny\n#MCDNN\nAverageError[%]\n17\n0.33\n\n0.07\n221\n0.27\n\n0.02\n335\n0.27\n\n0.02\n435\n0.26\n\n0.02\n521\n0.25\n\n0.01\n67\n0.24\n\n0.01\n71\n0.23\nthananypublishedresult(Tab.\n4\n).Intotalthereare82000charactersinthetestset,buttherearemany\nmoreeasytoclassifydigits(58000)thanhardtoclassifyletters(24000).Thisexplainstheloweroverall\nerrorrateofthe62-classproblemcomparedtothe52-classlettersproblem.Fromallerrorsofthe62-class\nproblem3%ofthe58000digitsareand33%ofthe24000lettersareLetters\nareingeneralmorediftoclassify,butthereisalsoahigheramountofconfusionbetweensimilar\nlower-andupper-caseletterssuchasi,Iando,Oforexample.Indeed,errorratesforthecaseinsensitive\ntaskdropfrom21%to7.37%.Iftheconfusedupper-andlower-caseclassesaremerged,resultingin37\ndifferentclasses,theerrorrateisonlyslightlybigger(7.99%).Upper-caselettersarefareasiertoclassify\n(1.83%errorrate)thanlowercaseletters(7.47%)duetothesmallerwriterdependentin-classvariability.\nForadetailedanalysisofalltheerrorsandconfusionsbetweendifferentclasses,theconfusionmatrixis\nmostinformative(SupplementarymaterialFig.S1).\nTable4:AverageerrorratesofMCDNNforallexperiments,plusresultsfromtheliterature.*case\ninsensitive\nData\nMCDNN\nPublishedresults\n(task)\nerror[%]\nError[%]andpaper\nall(62)\n11.63\ndigits(10)\n0.77\n3.71[\n12\n]1.88[\n25\n]\nletters(52)\n21.01\n30.91[\n16\n]\nletters*(26)\n7.37\n13.00[\n4\n]13.66[\n16\n]\nmerged(37)\n7.99\nuppercase(26)\n1.83\n10.00[\n4\n]6.44[\n9\n]\nlowercase(26)\n7.47\n16.00[\n4\n]13.27[\n16\n]\n3.3Chinesecharacters\nComparedtoLatincharacterrecognition,isolatedChinesecharacterrecognitionisamuchharderproblem,\nmainlybecauseofthemuchlargercategoryset,butalsobecauseofwidevariabilityofwritingstyles,and\ntheconfusionbetweensimilarcharacters.WeuseadatasetfromtheInstituteofAutomationofChinese\nAcademyofSciences(CASIA[\n23\n]),whichcontains300samplesforeachof3755characters(inGB1\nset).Thisresultedinadatasetwithmorethan1Millioncharacters(3GBofdata)whichposedamajor\ncomputationalchallengeeventooursystem.WithoutourfastGPUimplementationthenetsonthistask\nwouldtrainformorethanoneyear.Onlytheforwardpropagationofthetrainingsettakes27hona\nnormalCPU,andtrainingasingleepochwouldconsequentlyhavelastedseveraldays.OnourfastGPU\nimplementationontheotherhand,trainingasingleepochtakes3.4h,whichmakesitfeasibletotrainanet\nwithinafewdaysinsteadofmanymonths.\nWetrainfollowingDNN,1x48x48-100C3-MP2-200C2-MP2-300C2-MP2-400C2-MP2-500N-3755N,\nonofaswellasononlinecharacters.Fortheofcharacterrecognitiontask,weresizeallcharacters\n'b"TechnicalReportNo.IDSIA-04-12\n7\nto40x40pixelsandplacetheminthecenterofa48x48image.Thecontrastofeachimageisnormalized\nindependently.Assuggestedbytheorganizers,the240writersfromthedatabaseCASIA-HWDB1.1\nareusedfortrainingandtheremaining60writersareusedfortesting.Thetotalnumbersoftrainingand\ntestcharactersare938679and234228,respectively.\nFortheonlinedataset,wedraweachcharacterfromitslistofcoordinates,resizetheresultingimages\nto40x40pixelsandplacetheminthecenterofa48x48image.Additionally,wesmooth-outtheresulting\nimageswithaGaussianblurovera3x3pixelneighborhoodanduniformstandarddeviationof0.75.\nAssuggestedbytheorganizers,thecharactersof240writersfromdatabaseCASIA-OLHWDB1.1areused\nfortrainingtheandthecharactersoftheremaining60writersareusedfortesting.Theresulting\nnumbersoftrainingandtestcharactersare939564and234800,respectively.\nAllmethodspreviouslyappliedtothisdatasetperformsomefeatureextractionfollowedbyadimen-\nsionalityreduction,whereasourmethoddirectlyworksonrawpixelintensitiesandlearnsthefeature\nextractionanddimensionalityreductioninasupervisedway.Ontheoftaskweobtainanerrorrateof\n6.5%\ncomparedto10.01%ofthebestmethod[\n23\n].Eventhoughmuchinformationislostwhendrawinga\ncharacterfromit'scoordinatesequence,weobtainarecognitionrateof\n5.61%\nontheonlinetaskcompared\nto7.61%ofthebestmethod[\n23\n].\nWeconcludethatonthisveryhardproblem,withmanyclasses(3755)andrelativelyfew\nsamplesperclass(240),ourfullysupervisedDNNbeatsthecurrentstate-of-the-artmethodsbyalarge\nmargin.\n3.4Tsigns\nRecognizingtrafsignsisessentialfortheautomotiveindustry'seffortsintheofdriver'sassistance,\nandformanyothertrafapplications.WeusetheGTSRBtrafsigndataset[\n36\n].\nTheoriginalcolorimagescontainonetrafsigneach,withaborderof10%aroundthesign.They\nvaryinsizefrom\n15\n\n15\nto\n250\n\n250\npixelsandarenotnecessarilysquare.Theactualtrafsignisnot\nalwayscenteredwithintheimage;itsboundingboxispartoftheannotations.Thetrainingsetconsistsof\n26640images;thetestsetof12569images.Wecropallimagesandprocessonlywithintheboundingbox.\nOurDNNimplementationrequiresalltrainingimagestobeofequalsize.Aftervisualinspectionofthe\nimagesizedistributionweresizeallimagesto\n48\n\n48\npixels.Asaconsequence,scalingfactorsalongboth\naxesaredifferentfortrafsignswithrectangularboundingboxes.Resizingforcesthemtohavesquare\nboundingboxes.\nOurMCDNNistheonlymethodtooutperformhumans,whoproducedtwiceasmanyerrors.\nSincetrafsignsgreatlyvaryinilluminationandcontrast,standardimagepreprocessingmethodsare\nusedtoenhance/normalizethem(Fig.\n3\naandsupplementarymaterial).ForeachdatasetveDNNare\ntrained(architecture:3x48x48-100C7-MP2-150C4-150MP2-250C4-250MP2-300N-43N),resultingina\nMCDNNwith25columns,achievinganerrorrateof\n0.54%\nonthetestset.Figure\n3\nbdepictsallerrors,\nplusgroundtruthandandsecondpredictions.Over80%ofthe68errorsareassociatedwithcorrect\nsecondpredictions.ErroneouslypredictedclassprobabilitiestendtobeverylowheretheMCDNNis\nquiteunsureaboutitsIngeneral,however,itisveryofitspredictedclass\nprobabilitiesareclosetooneorzero.Rejectingonly1%percentofallimagesbelow0.51)\nresultsinanevenlowererrorrateof0.24%.Toreachanerrorrateof0.01%(asingle\nonly6.67%oftheimageshavetoberejectedbelow0.94).Ourmethodoutperformsthesecond\nbestalgorithmbyafactorof3.Ittakes37hourstotraintheMCDNNwith25columnsonfourGPUs.The\ntrainedMCDNNcancheck87imagespersecondononeGPU(and2175images/s/DNN).\n3.5CIFAR10\nCIFAR10isasetofnaturalcolorimagesof32x32pixels[\n18\n].Itcontains10classes,eachwith5000\ntrainingsamplesand1000testsamples.Imagesvarygreatlywithineachclass.Theyarenotnecessarily\n"b'TechnicalReportNo.IDSIA-04-12\n8\n(a)\n(b)\nFigure3:(a)Preprocessedimages,fromtoptobottom:original,Imadjust,Histeq,Adapthisteq,Conorm.\n(b)The68errorsoftheMCDNN,withcorrectlabel(left)andandsecondbestpredictions(middleand\nright).\nTable5:Errorrates,averagesandstandarddeviationsfor10runsofa10layerDNNontheCIFAR10test\nset.Thenetsintherowaretrainedonpreprocessedimages(seetrafsignpreprocessing),whereas\nthoseinthesecondrowaretrainedonoriginalimages.\npreprocessing\nerrorsfor8runs[%]mean[%]\nyes\n16.4719.2019.7220.31\n18\n:\n93\n\n1\n:\n69\nno\n15.6315.8516.1316.05\n15\n:\n91\n\n0\n:\n22\n8-netaverageerror:17.42\n\n1.96%\n8-netMCDNNerror:\n11.21%\npreviousstateoftheart:18.50%-[\n8\n];19.51%-[\n7\n]\ncentered,maycontainonlypartsoftheobject,andshowdifferentbackgrounds.Subjectsmayvaryinsize\nbyanorderofmagnitude(i.e.,someimagesshowonlytheheadofabird,othersanentirebirdfroma\ndistance).Colorsandtexturesofobjects/animalsalsovarygreatly.\nOurDNNinputlayershavethreemaps,oneforeachcolorchannel(RGB).Weusea10-layerarchitec-\nturewithverysmallkernels:3x32x32-300C3-MP2-300C2-MP2-300C3-MP2-300C2-MP2-300N-100N-\n10N.JustlikeforMNIST,theinitiallearningrate0.001decaysbyafactorof0.993aftereveryepoch.\nTransformingCIFARcolorimagestograyscalereducesinputlayercomplexitybutincreaseserrorrates.\nHencewesticktotheoriginalcolorimages.AsforMNIST,augmentingthetrainingsetwithrandomly\n(byatmost5%)translatedimagesgreatlydecreasestheerrorfrom28%to20%(theNN-inherentlocal\ntranslationinvariancebyitselfisnotsufByadditionalscaling(upto\n\n15%),rotation(upto\n\n5\n\n),\nandupto\n\n15%translation,theindividualneterrorsdecreasebyanother3%(Tab.\n5\n).Theabovesmall\nmaximalboundspreventlossoftoomuchinformationleakedbeyondthe\n32\n\n32\npixelsrectangle.\n'b'TechnicalReportNo.IDSIA-04-12\n9\nFigure4:ConfusionmatrixfortheCIFAR10MCDNN:correctlabelsonverticalaxis;detectedlabelson\nhorizontalaxis.Squareareasareproportionaltoerrornumbers,shownbothasrelativepercentagesofthe\ntotalerrornumber,andinabsolutevalue.Left-imagesofallbirdsasplanes.Right-imagesof\nallplanesasbirds.Confusionsub-matrixforanimalclasseshasagraybackround.\nWerepeattheexperimentwithdifferentrandominitializationsandcomputemeanandstandarddevia-\ntionoftheerror,whichisrathersmallfororiginalimages,showingthatourDNNarerobust.OurMCDNN\nobtainsaverylowerrorrateof11.21%,greatlyrisingthebarforthisbenchmark.\nTheconfusionmatrix(Figure\n4\n)showsthattheMCDNNalmostperfectlyseparatesanimalsfromarti-\nfacts,exceptforplanesandbirds,whichseemsnatural,althoughhumanseasilydistinguishalmostallthe\nincorrectlyimages,evenifmanyareclutteredorcontainonlypartsoftheobjects/animals(see\nfalsepositiveandfalsenegativeimagesinFigure\n4\n).Therearemanyconfusionsbetweendifferentani-\nmals;thefrogclasscollectsmostfalsepositivesfromotheranimalclasses,withveryfewfalsenegatives.\nAsexpected,catsarehardtotellfromdogs,collectivelycausing15.25%oftheerrors.\nTheMCDNNwith8columns(fourtrainedonoriginaldataandonetrainedforeachpreprocessingused\nalsofortrafsigns)reachesalow11.21%errorrate,farbetterthananyotheralgorithm.\n3.6NORB\nWetestaMCDNNwithfourcolumnsonNORB(jittered-cluttered)[\n21\n],acollectionofstereoimagesof\n3Dmodels(Figure\n5\n).Theobjectsarecentrallyplacedonrandomlychosenbackgrounds,andthereisalso\nclutteringfromaperipherallyplacedsecondobject.Thisdatabaseisdesignedforexperimentingwith3D\nobjectrecognitionfromshape.Itcontainsimagesof50toysbelongingto5genericcategories:four-legged\nanimals,humanairplanes,trucks,andcars.Theobjectswereimagedbytwocamerasunder6\nlightingconditions,9elevations(30to70degreesevery5degrees),and18azimuths(0to340every20\ndegrees).Thetrainingsethas10foldsof29160imageseachforatotalof291600images;thetestingset\nconsistsoftwofoldstotalizing58320images.\n'b'TechnicalReportNo.IDSIA-04-12\n10\nFigure5:TwentyNORBstereoimages(leftimage-up,rightimage-down).\nTable6:Errorrates,averagesandstandarddeviationsover4runsofa9layerDNNontheNORBtestset.\ntraining\nerrorsfor4runs[%]mean[%]\nsetsize\n\n4.494.714.824.85\n4\n:\n72\n\n0\n:\n16\n2folds\n4-netMCDNNerror:3.57%\nall\n3.323.183.733.36\n3\n:\n40\n\n0\n:\n23\n10folds\n4-netMCDNNerror:\n2.70%\npreviousstateoftheart:5.00%-[\n8\n];5.60%-[\n32\n]\nNopreprocessingisusedforthisdataset.Wescaledownimagesfromtheoriginal108x108to48x48\npixels.Thissizeisbigenoughtopreservethedetailspresentinimagesandsmallenoughtoallowfast\ntraining.Weperformtworoundsofexperiments,usingonlythetwofolds(tocomparewithprevious\nresultsthatdonotusetheentiretrainingdata)andusingalltrainingdata.\nWetestedseveraldistortionparameterswithsmallnetsandfoundthatmaximumrotationof\n15\n\n,maxi-\nmumtranslationof15%andmaximumscalingof15%aregoodchoices,henceweusethemforallNORB\nexperiments.\nTocomparetopreviousresults,wetrainonlyonthe2-foldsofthedata.Thenetarchitecture\nisdeep,buthasfewmapsperlayer:2x48x48-50C5-MP2-50C5-MP2-50C4-MP2-300N-100N-6N.The\nlearningratesetupis:etastart0.001;etafactor0.95;etastop0.000003.Duetosmallnetsize,trainingis\nfastat156s/epochfor114epochs.Testingonesamplerequires0.5ms.Evenwhenweuselessdatatotrain,\ntheMCDNNgreatlyimprovesthestateoftheartfrom5%to3.57%(Table\n6\n).\nOurmethodisfastenoughtoprocesstheentiretrainingsetthough.Weusethesamearchitecturebut\ndoublethenumberofmapswhentrainingwithall10folds:2x48x48-100C5-MP2-100C5-MP2-100C4-\nMP2-300N-100N-6N.Thelearningratesetupremainsthesame.Trainingtimeincreasesto34min/epoch\nbecausethenetisbigger,andweusevetimesmoredata.Testingonesampletakes1.3ms.Allofthis\npaysoff,resultinginaverylow2.70%errorrate,furtherimprovingthestateoftheart.\nAlthoughNORBhasonlysixclasses,trainingandtestinstancessometimesdiffergreatly,making\nhard.Morethan50%oftheerrorsareduetoconfusionsbetweencarsandtrucks.Considering\nsecondpredictions,too,theerrorratedropsfrom2.70%to0.42%,showingthat84%oftheerrorsare\nassociatedwithacorrectsecondprediction.\n4Conclusion\nThisisthetimehuman-competitiveresultsarereportedonwidelyusedcomputervisionbenchmarks.\nOnmanyotherimagedatasetsourMCDNNimprovesthestate-of-the-artby30-80%(Tab.\n7\n).\nWedrasticallyimproverecognitionratesonMNIST,NISTSD19,Chinesecharacters,trafsigns,CI-\n'b'TechnicalReportNo.IDSIA-04-12\n11\nFAR10andNORB.Ourmethodisfullysupervisedanddoesnotuseanyadditionalunlabeleddatasource.\nSingleDNNalreadyaresuftoobtainnewstate-of-the-artresults;combiningthemintoMCDNNs\nyieldsfurtherdramaticperformanceboosts.\nTable7:Resultsandrelativeimprovementsondifferentdatasets.\nDataset\nBestresultMCDNNRelative\nofothers[%][%]improv.[%]\nMNIST\n0.390.2341\nNISTSD19\nseeTable\n4\nseeTable\n4\n30-80\nHWDB1.0on.\n7.615.6126\nHWDB1.0off.\n10.016.535\nCIFAR10\n18.5011.2139\ntrafsigns\n1.690.5472\nNORB\n5.002.7046\nAcknowledgment\nThisworkwaspartiallysupportedbyaFP7-ICT-2009-6EUGrantunderProjectCode270247:ANeuro-\ndynamicFrameworkforCognitiveRobotics:SceneRepresentations,BehavioralSequences,andLearning.\nReferences\n[1]\nS.Behnke.\nHierarchicalNeuralNetworksforImageInterpretation\n,volume2766of\nLectureNotesin\nComputerScience\n.Springer,2003.\n1\n,\n2\n[2]\nY.Bengio,P.Lamblin,D.Popovici,andH.Larochelle.Greedylayer-wisetrainingofdeepnetworks.\nIn\nNeuralInformationProcessingSystems\n,2007.\n1\n,\n2\n[3]\nN.P.Bichot,A.F.Rossi,andR.Desimone.Parallelandserialneuralmechanismsforvisualsearch\ninmacaqueareaV4.\nScience\n,308:529534,2005.\n2\n[4]\nP.R.Cavalin,A.deSouzaBrittoJr.,F.Bortolozzi,R.Sabourin,andL.E.S.deOliveira.Animplicit\nsegmentation-basedmethodforrecognitionofhandwrittenstringsofcharacters.In\nSAC\n,pages836\n840,2006.\n6\n[5]\nD.C.Ciresan,U.Meier,L.M.Gambardella,andJ.Schmidhuber.Deep,big,simpleneuralnetsfor\nhandwrittendigitrecognition.\nNeuralComputation\n,22(12):32073220,2010.\n1\n,\n5\n[6]\nD.C.Ciresan,U.Meier,L.M.Gambardella,andJ.Schmidhuber.Convolutionalneuralnetworkcom-\nmitteesforhandwrittencharacterIn\nInternationalConferenceonDocumentAnalysis\nandRecognition\n,pages12501254,2011.\n1\n,\n5\n[7]\nD.C.Ciresan,U.Meier,J.Masci,L.M.Gambardella,andJ.Schmidhuber.Flexible,highperfor-\nmanceconvolutionalneuralnetworksforimageIn\nInternationalJointConferenceon\nIntelligence\n,pages12371242,2011.\n1\n,\n2\n,\n4\n,\n5\n,\n8\n[8]\nA.CoatesandA.Y.Ng.Theimportanceofencodingversustrainingwithsparsecodingandvector\nquantization.In\nInternationalConferenceonMachineLearning\n,2011.\n8\n,\n10\n[9]\nE.M.DosSantos,L.S.Oliveira,R.Sabourin,andP.Maupin.Ovintheselectionof\nensembles:acomparativestudybetweenpsoandga.In\nConferenceonGeneticandEvolutionary\nComputation\n,pages14231424.ACM,2008.\n6\n'b"TechnicalReportNo.IDSIA-04-12\n12\n[10]\nA.C.P.-A.M.P.V.DumitruErhan,YoshuaBengioandS.Bengio.Whydoesunsupervisedpre-\ntraininghelpdeeplearning?\nJournalofMachineLearningResearch\n,11:625660,2010.\n1\n,\n2\n[11]\nK.Fukushima.Neocognitron:Aself-organizingneuralnetworkforamechanismofpatternrecogni-\ntionunaffectedbyshiftinposition.\nBiologicalCybernetics\n,36(4):193202,1980.\n1\n,\n2\n[12]\nE.Granger,P.Henniges,andR.Sabourin.SupervisedLearningofFuzzyARTMAPNeuralNetworks\nThroughParticleSwarmOptimization.\nPatternRecognition\n,1:2760,2007.\n6\n[13]\nP.J.Grother.NISTspecialdatabase19-Handprintedformsandcharactersdatabase.Technical\nreport,NationalInstituteofStandardsandThechnology(NIST),1995.\n1\n,\n5\n[14]\nS.Hochreiter,Y.Bengio,P.Frasconi,andJ.Schmidhuber.Gradientwinrecurrentnets:the\ndifoflearninglong-termdependencies.InS.C.KremerandJ.F.Kolen,editors,\nAFieldGuide\ntoDynamicalRecurrentNeuralNetworks\n.IEEEPress,2001.\n2\n[15]\nD.H.HubelandT.Wiesel.Receptivebinocularinteraction,andfunctionalarchitectureinthe\ncat'svisualcortex.\nJournalofPhysiology(London)\n,160:106154,1962.\n2\n[16]\nA.L.KoerichandP.R.Kalva.Unconstrainedhandwrittencharacterrecognitionusingmetaclassesof\ncharacters.In\nIntl.Conf.onImageProcessing\n,pages542545,2005.\n6\n[17]\nT.Kohonen.\nSelf-OrganizationandAssociativeMemory\n.Springer,secondedition,1988.\n2\n[18]\nA.Krizhevsky.Learningmultiplelayersoffeaturesfromtinyimages.Master'sthesis,Computer\nScienceDepartment,UniversityofToronto,2009.\n1\n,\n7\n[19]\nY.LeCun.Uneproc\n\nedured'apprentissagepourr\n\neseauaseuilasymmetrique(alearningschemefor\nasymmetricthresholdnetworks).In\nProceedingsofCognitiva85\n,pages599604,Paris,France,\n1985.\n2\n[20]\nY.LeCun,L.Bottou,Y.Bengio,andP.Haffner.Gradient-basedlearningappliedtodocumentrecog-\nnition.\nProceedingsoftheIEEE\n,86(11):22782324,November1998.\n1\n,\n2\n,\n3\n[21]\nY.LeCun,F.-J.Huang,andL.Bottou.Learningmethodsforgenericobjectrecognitionwithinvari-\nancetoposeandlighting.In\nComputerVisionandPatternRecognition\n,2004.\n1\n,\n9\n[22]\nY.LeCun,L.D.Jackel,L.Bottou,C.Cortes,J.S.Denker,H.Drucker,I.Guyon,U.A.Muller,\nE.Sackinger,P.Simard,andV.Vapnik.LearningalgorithmsforAcomparisonon\nhandwrittendigitrecognition.InJ.H.Oh,C.Kwon,andS.Cho,editors,\nNeuralNetworks:The\nStatisticalMechanicsPerspective\n,pages261276.World1995.\n5\n[23]\nC.-L.Liu,F.Yin,D.-H.Wang,andQ.-F.Wang.ChineseHandwritingRecognitionContest.In\nChineseConferenceonPatternRecognition\n,2010.\n1\n,\n6\n,\n7\n[24]\nMATLAB.\nversion7.10.0(R2010a)\n.TheMathWorksInc.,Natick,Massachusetts,2010.\n16\n[25]\nJ.Milgram,M.Cheriet,andR.Sabourin.Estimatingaccuratemulti-classprobabilitieswithsupport\nvectormachines.In\nInt.JointConf.onNeuralNetworks\n,pages19061911,2005.\n6\n[26]\nM.Ranzato,Y.-L.B.FuJieHuang,andY.LeCun.Unsupervisedlearningofinvariantfeaturehier-\narchieswithapplicationstoobjectrecognition.In\nProc.ofComputerVisionandPatternRecognition\nConference\n,2007.\n2\n[27]\nM.Ranzato,F.Huang,Y.Boureau,andY.LeCun.Unsupervisedlearningofinvariantfeaturehier-\narchieswithapplicationstoobjectrecognition.In\nProc.ComputerVisionandPatternRecognition\nConference(CVPR'07)\n.IEEEPress,2007.\n5\n[28]\nM.A.Ranzato,C.Poultney,S.Chopra,andY.Lecun.Eflearningofsparserepresentations\nwithanenergy-basedmodel.In\nAdvancesinNeuralInformationProcessingSystems(NIPS2006)\n,\n2006.\n5\n[29]\nM.RiesenhuberandT.Poggio.Hierarchicalmodelsofobjectrecognitionincortex.\nNat.Neurosci.\n,\n2(11):10191025,1999.\n2\n"b"TechnicalReportNo.IDSIA-04-12\n13\n[30]\nD.E.Rumelhart,G.E.Hinton,andR.J.Williams.Learninginternalrepresentationsbyerrorprop-\nagation.In\nParalleldistributedprocessing:explorationsinthemicrostructureofcognition,vol.1:\nfoundations\n,pages318362.MITPress,Cambridge,MA,USA,1986.\n2\n[31]\nR.SalakhutdinovandG.Hinton.Learninganonlinearembeddingbypreservingclassneighborhood\nstructure.In\nProc.oftheInternationalConferenceonIntelligenceandStatistics\n,volume11,\n2007.\n2\n[32]\nD.Scherer,A.M\n\nuller,andS.Behnke.Evaluationofpoolingoperationsinconvolutionalarchitectures\nforobjectrecognition.In\nInternationalConferenceonNeuralNetworks\n,2010.\n10\n[33]\nP.SermanetandY.LeCun.Trafsignrecognitionwithmulti-scaleconvolutionalnetworks.In\nProceedingsofInternationalJointConferenceonNeuralNetworks(IJCNN'11)\n,2011.\n17\n[34]\nT.Serre,L.Wolf,S.M.Bileschi,M.Riesenhuber,andT.Poggio.Robustobjectrecognitionwith\ncortex-likemechanisms.\nIEEETrans.PatternAnal.Mach.Intell.\n,29(3):411426,2007.\n2\n[35]\nP.Y.Simard,D.Steinkraus,andJ.C.Platt.Bestpracticesforconvolutionalneuralnetworksap-\npliedtovisualdocumentanalysis.In\nSeventhInternationalConferenceonDocumentAnalysisand\nRecognition\n,pages958963,2003.\n1\n,\n2\n,\n5\n[36]\nJ.Stallkamp,M.Schlipsing,J.Salmen,andC.Igel.TheGermanTrafSignRecognitionBench-\nmark:Amulti-classcompetition.In\nInternationalJointConferenceonNeuralNet-\nworks\n,2011.\n1\n,\n7\n[37]\nD.Strigl,K.K,andS.Podlipnig.Performanceandscalabilityofgpu-basedconvolutionalneural\nnetworks.\nParallel,Distributed,andNetwork-BasedProcessing,EuromicroConferenceon\n,0:317\n324,2010.\n1\n[38]\nR.UetzandS.Behnke.Large-scaleobjectrecognitionwithCUDA-acceleratedhierarchicalneural\nnetworks.In\nIEEEInternationalConferenceonIntelligentComputingandIntelligentSystems(ICIS)\n,\n2009.\n1\n[39]\nP.J.Werbos.\nBeyondRegression:NewToolsforPredictionandAnalysisintheBehavioralSciences.\nPhDthesis,HarvardUniversity,1974.\n2\n[40]\nD.H.WieselandT.N.Hubel.Receptiveofsingleneuronesinthecat'sstriatecortex.\nJ.\nPhysiol.\n,148:574591,1959.\n2\n[41]\nD.J.WillshawandC.vonderMalsburg.Howpatternedneuralconnectionscanbesetupbyself-\norganization.\nProc.R.Soc.LondonB\n,194:431445,1976.\n2\n"b"TechnicalReportNo.IDSIA-04-12\n14\n5SupplementaryMaterial\n5.1Experimentdetails\n5.1.1NISTSD19\nTheconfusionmatrixofthe62characterstask(Fig.\n6\n)showsthatmostoftheerrorsareduetoconfusions\nbetweendigitsandlettersandbetweenlower-andupper-caseletters.\nFigure6:ConfusionmatrixoftheNISTSD19MCDNNtrainedonthe62-classtask:correctlabelson\nverticalaxis;detectedlabelsonhorizontalaxis.Squareareasareproportionaltoerrornumbers,shownas\nrelativepercentagesofthetotalerrornumber.Forconvenience,classlabelsarewrittenbeneaththeerrors.\nErrorsbelow1%ofthetotalerrornumberarenotdetailed.\nNotverysurprisingly,theconfusionmatrixforthedigittask(Fig.\n7\n)showsthatconfusionsbetween\nfoursandninesarethemostcommonerrorsource.\nForthe52lettertask(casesensitive)theconfusionmatrix(Fig.\n8\n)showsthattheMCDNNhasmainly\nproblemswithupper-andlower-caseconfusionsofthesameletter.Otherhard-to-distinguishclassesare:\n'q'and'g','l'and'i'.\n"b'TechnicalReportNo.IDSIA-04-12\n15\nFigure7:ConfusionmatrixfortheNISTSD19MCDNNtrainedonthedigittask:correctlabelsonvertical\naxis;detectedlabelsonhorizontalaxis.Squareareasareproportionaltoerrornumbers,shownasrelative\npercentagesofthetotalerrornumber.Classlabelsareshownbeneaththeerrors.Errorsbelow1%ofthe\ntotalerrornumberareshownasdotswithoutanydetails.\nFigure8:ConfusionmatrixfortheNISTSD19MCDNNtrainedonall52letters:correctlabelsonvertical\naxis;detectedlabelsonhorizontalaxis.Squareareasareproportionaltoerrornumbers,shownasrelative\npercentagesofthetotalerrornumber.Classlabelsareshownbeneaththeerrors.Errorsbelow1%ofthe\ntotalerrornumberareshownasdotswithoutanydetails.\n'b"TechnicalReportNo.IDSIA-04-12\n16\nFortheupper-caselettertasktheconfusionmatrix(Figure\n9\n)showsthattheMCDNNhasproblems\nwithlettersofsimilarshape,i.e.'D',and'O','V'and'U'etc.Thetotalerrorof1.82%isverylowthough.\nFigure9:ConfusionmatrixfortheNISTSD19MCDNNtrainedonuppercaseletters:correctlabelson\nverticalaxis;detectedlabelsonhorizontalaxis.Squareareasareproportionaltoerrornumbers,shownas\nrelativepercentagesofthetotalerrornumber.Classlabelsareshownbeneaththeerrors.Errorsbelow1%\nofthetotalerrornumberareshownasdotswithoutanydetails.\nForthelower-caselettertasktheconfusionmatrix(Fig.\n10\n)showsthatlikewithupper-caseletters,the\nMCDNNhasproblemswithlettersofsimilarshapes,i.e.'g',and'q','v'and'u'etc.Butthetotalerroris\nmuchhigher(7.47%)thanfortheupper-caseletterstask.\nForthemerged-caselettertask(37classes)theconfusionmatrix(Figure\n11\n)showsthattheMCDNN\nhasmostlyproblemswithlettersofsimilarshapes,i.e.'l',and'i'.Allupper-lower-caseconfusionsof\nidenticallettersfromthe52classtaskvanish,theerrorshrinksbyafactorofalmostthreedownto7.99%.\nTheexperimentsondifferentsubsetsofthe62charactertaskclearlyshowthatitisveryhardtodistin-\nguishbetweensmallandcapitalletters.Also,digits0and1arehardtoseparatefromlettersOandI.Many\noftheseproblemscouldbealleviatedbyincorporatingcontextwherepossible.\n5.1.2Tsigns\nHighcontrastvariationamongtheimagescallsfornormalization.Wetestthefollowingstandardcontrast\nnormalizations:\n\nImageAdjustment(Imadjust)\nincreasesimagecontrastbymappingpixelintensitiestonewvalues\nsuchthat1%ofthedataissaturatedatlowandhighintensities[\n24\n].\n\nHistogramEqualization(Histeq)\nenhancescontrastbytransformingpixelintensitiessuchthatthe\noutputimagehistogramisroughlyuniform[\n24\n].\n\nAdaptiveHistogramEqualization(Adapthisteq)\noperates(unlikeHisteq)ontilesratherthanthe\nentireimage,wetiledtheimagein8nonoverlappingregionsof6x6pixels.Eachtile'scontrastis\nenhancedsuchthatitshistogrambecomesroughlyuniform[\n24\n].\n"b'TechnicalReportNo.IDSIA-04-12\n17\nFigure10:ConfusionmatrixfortheNISTSD19MCDNNtrainedonlowercaseletters:correctlabelson\nverticalaxis;detectedlabelsonhorizontalaxis.Squareareasareproportionaltoerrornumbers,shownas\nrelativepercentagesofthetotalerrornumber.Classlabelsareshownbeneaththeerrors.Errorsbelow1%\nofthetotalerrornumberareshownasdotswithoutanydetails.\n\nContrastNormalization(Conorm)\nenhancesedges,theinputimagebyadifferenceof\nGaussians,usingasizeof5x5pixels[\n33\n].\nNotethattheabovenormalizations,exceptConorm,areperformedinacolorspacewithimageintensity\nasoneofitscomponents.ForthispurposewetransformtheimagefromRGB-toLab-space,thenperform\nnormalization,thentransformtheresultbacktoRGB-space.Theeffectofthefourdifferentnormalizations\nissummarizedinFigure\n12\n,wherehistogramsofpixelintensitiestogetherwithoriginalandnormalized\nimagesareshown.\nTheDNNhavethreemapsfortheinputlayer,oneforeachcolorchannel(RGB).Therestofthenet\narchitectureisdetailedinTable\n8\n.Weusea10-layerarchitecturewithverysmallmax-poolingkernels.\nTable8:10layerDNNarchitectureusedforrecognizingtrafsigns.\nLayer\nType\n#maps&neurons\nkernel\n0\ninput\n3mapsof48x48neurons\n1\nconvolutional\n100mapsof42x42neurons\n7x7\n2\nmaxpooling\n100mapsof21x21neurons\n2x2\n3\nconvolutional\n150mapsof18x18neurons\n4x4\n4\nmaxpooling\n150mapsof9x9neurons\n2x2\n5\nconvolutional\n250mapsof6x6neurons\n4x4\n6\nmaxpooling\n250mapsof3x3neurons\n2x2\n9\nfullyconnected\n300neurons\n1x1\n10\nfullyconnected\n43neurons\n1x1\n'b'TechnicalReportNo.IDSIA-04-12\n18\nFigure11:ConfusionmatrixfortheNISTSD19MCDNNtrainedonmergedletters(37classes):correct\nlabelsonverticalaxis;detectedlabelsonhorizontalaxis.Squareareasareproportionaltoerrornumbers,\nshownasrelativepercentagesofthetotalerrornumber.Classlabelsareshownbeneaththeerrors.Errors\nbelow1%ofthetotalerrornumberareshownasdotswithoutanydetails.\nFigure12:Preprocessing.\n'b'TechnicalReportNo.IDSIA-04-12\n19\n5.1.3CIFAR10\nTable9:10layerDNNarchitectureusedforCIFAR10.\nLayer\nType\n#maps&neurons\nkernel\n0\ninput\n3mapsof32x32neurons\n1\nconvolutional\n300mapsof30x30neurons\n3x3\n2\nmaxpooling\n300mapsof15x15neurons\n2x2\n3\nconvolutional\n300mapsof14x14neurons\n2x2\n4\nmaxpooling\n300mapsof7x7neurons\n2x2\n5\nconvolutional\n300mapsof6x6neurons\n2x2\n6\nmaxpooling\n300mapsof3x3neurons\n2x2\n7\nconvolutional\n300mapsof2x2neurons\n2x2\n8\nmaxpooling\n300mapsof1x1neurons\n2x2\n9\nfullyconnected\n300neurons\n1x1\n10\nfullyconnected\n100neurons\n1x1\n11\nfullyconnected\n10neurons\n1x1\n5.1.4NORB\nFigure13:ConfusionmatrixfortheNORB:correctlabelsonverticalaxis;detectedlabelsonhorizontal\naxis.\n'