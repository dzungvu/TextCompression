b"ConvolutionalNeuralNetworksforSentence\nYoonKim\nNewYorkUniversity\nyhk255@nyu.edu\nAbstract\nWereportonaseriesofexperimentswith\nconvolutionalneuralnetworks(CNN)\ntrainedontopofpre-trainedwordvec-\ntorsforsentence-leveltasks.\nWeshowthatasimpleCNNwithlit-\ntlehyperparametertuningandstaticvec-\ntorsachievesexcellentresultsonmulti-\nplebenchmarks.Learning\nvectorsthroughoffersfurther\ngainsinperformance.Weadditionally\nproposeasimpletothear-\nchitecturetoallowfortheuseofboth\nandstaticvectors.TheCNN\nmodelsdiscussedhereinimproveuponthe\nstateofthearton4outof7tasks,which\nincludesentimentanalysisandquestion\n\n1Introduction\nDeeplearningmodelshaveachievedremarkable\nresultsincomputervision(Krizhevskyetal.,\n2012)andspeechrecognition(Gravesetal.,2013)\ninrecentyears.Withinnaturallanguageprocess-\ning,muchoftheworkwithdeeplearningmeth-\nodshasinvolvedlearningwordvectorrepresenta-\ntionsthroughneurallanguagemodels(Bengioet\nal.,2003;Yihetal.,2011;Mikolovetal.,2013)\nandperformingcompositionoverthelearnedword\nvectorsfor(Collobertetal.,2011).\nWordvectors,whereinwordsareprojectedfroma\nsparse,1-of-\nV\nencoding(here\nV\nisthevocabulary\nsize)ontoalowerdimensionalvectorspaceviaa\nhiddenlayer,areessentiallyfeatureextractorsthat\nencodesemanticfeaturesofwordsintheirdimen-\nsions.Insuchdenserepresentations,semantically\nclosewordsarelikewisecloseineuclideanor\ncosinedistanceinthelowerdimensionalvector\nspace.\nConvolutionalneuralnetworks(CNN)utilize\nlayerswithconvolvingthatareappliedto\nlocalfeatures(LeCunetal.,1998).Originally\ninventedforcomputervision,CNNmodelshave\nsubsequentlybeenshowntobeeffectiveforNLP\nandhaveachievedexcellentresultsinsemantic\nparsing(Yihetal.,2014),searchqueryretrieval\n(Shenetal.,2014),sentencemodeling(Kalch-\nbrenneretal.,2014),andothertraditionalNLP\ntasks(Collobertetal.,2011).\nInthepresentwork,wetrainasimpleCNNwith\nonelayerofconvolutionontopofwordvectors\nobtainedfromanunsupervisedneurallanguage\nmodel.ThesevectorsweretrainedbyMikolovet\nal.(2013)on100billionwordsofGoogleNews,\nandarepubliclyavailable.\n1\nWeinitiallykeepthe\nwordvectorsstaticandlearnonlytheotherparam-\netersofthemodel.Despitelittletuningofhyper-\nparameters,thissimplemodelachievesexcellent\nresultsonmultiplebenchmarks,suggestingthat\nthepre-trainedvectorsare`universal'featureex-\ntractorsthatcanbeutilizedforvarious\ntiontasks.Learningtask-svectorsthrough\nresultsinfurtherimprovements.We\ndescribeasimpletothearchi-\ntecturetoallowfortheuseofbothpre-trainedand\nvectorsbyhavingmultiplechannels.\nOurworkisphilosophicallysimilartoRazavian\netal.(2014)whichshowedthatforimageclas-\nfeatureextractorsobtainedfromapre-\ntraineddeeplearningmodelperformwellonava-\nrietyoftasksincludingtasksthatareverydif-\nferentfromtheoriginaltaskforwhichthefeature\nextractorsweretrained.\n2Model\nThemodelarchitecture,shownin1,isa\nslightvariantoftheCNNarchitectureofCollobert\netal.(2011).Let\nx\ni\n2\nR\nk\nbethe\nk\n-dimensional\nwordvectorcorrespondingtothe\ni\n-thwordinthe\nsentence.Asentenceoflength\nn\n(paddedwhere\n1\nhttps://code.google.com/p/word2vec/\narXiv:1408.5882v2  [cs.CL]  3 Sep 2014"b"Figure1:Modelarchitecturewithtwochannelsforanexamplesentence.\nnecessary)isrepresentedas\nx\n1:\nn\n=\nx\n1\n\nx\n2\n\n:::\n\nx\nn\n;\n(1)\nwhere\n\nistheconcatenationoperator.Ingen-\neral,let\nx\ni\n:\ni\n+\nj\nrefertotheconcatenationofwords\nx\ni\n;\nx\ni\n+1\n;:::;\nx\ni\n+\nj\n.Aconvolutionoperationin-\nvolvesa\n\nw\n2\nR\nhk\n,whichisappliedtoa\nwindowof\nh\nwordstoproduceanewfeature.For\nexample,afeature\nc\ni\nisgeneratedfromawindow\nofwords\nx\ni\n:\ni\n+\nh\n\n1\nby\nc\ni\n=\nf\n(\nw\n\nx\ni\n:\ni\n+\nh\n\n1\n+\nb\n)\n:\n(2)\nHere\nb\n2\nR\nisabiastermand\nf\nisanon-linear\nfunctionsuchasthehyperbolictangent.This\nisappliedtoeachpossiblewindowofwordsinthe\nsentence\nf\nx\n1:\nh\n;\nx\n2:\nh\n+1\n;:::;\nx\nn\n\nh\n+1:\nn\ng\ntoproduce\na\nfeaturemap\nc\n=[\nc\n1\n;c\n2\n;:::;c\nn\n\nh\n+1\n]\n;\n(3)\nwith\nc\n2\nR\nn\n\nh\n+1\n.Wethenapplyamax-over-\ntimepoolingoperation(Collobertetal.,2011)\noverthefeaturemapandtakethemaximumvalue\n^\nc\n=max\nf\nc\ng\nasthefeaturecorrespondingtothis\nparticular.Theideaistocapturethemostim-\nportantfeatureonewiththehighestvaluefor\neachfeaturemap.Thispoolingschemenaturally\ndealswithvariablesentencelengths.\nWehavedescribedtheprocessbywhich\none\nfeatureisextractedfrom\none\n.Themodel\nusesmultiple(withvaryingwindowsizes)\ntoobtainmultiplefeatures.Thesefeaturesform\nthepenultimatelayerandarepassedtoafullycon-\nnectedsoftmaxlayerwhoseoutputistheprobabil-\nitydistributionoverlabels.\nInoneofthemodelvariants,weexperiment\nwithhavingtwo`channels'ofwordvectorsone\nthatiskeptstaticthroughouttrainingandonethat\nisviabackpropagation(section3.2).\n2\nInthemultichannelarchitecture,illustratedin\nure1,eachisappliedtobothchannelsand\ntheresultsareaddedtocalculate\nc\ni\ninequation\n(2).Themodelisotherwiseequivalenttothesin-\nglechannelarchitecture.\n2.1Regularization\nForregularizationweemploydropoutonthe\npenultimatelayerwithaconstrainton\nl\n2\n-normsof\ntheweightvectors(Hintonetal.,2012).Dropout\npreventsco-adaptationofhiddenunitsbyran-\ndomlydroppingouti.e.,settingtozeroapro-\nportion\np\nofthehiddenunitsduringfoward-\nbackpropagation.Thatis,giventhepenultimate\nlayer\nz\n=[^\nc\n1\n;:::;\n^\nc\nm\n]\n(notethatherewehave\nm\ninsteadofusing\ny\n=\nw\n\nz\n+\nb\n(4)\nforoutputunit\ny\ninforwardpropagation,dropout\nuses\ny\n=\nw\n\n(\nz\n\nr\n)+\nb;\n(5)\nwhere\n\nistheelement-wisemultiplicationopera-\ntorand\nr\n2\nR\nm\nisa`masking'vectorofBernoulli\nrandomvariableswithprobability\np\nofbeing1.\nGradientsarebackpropagatedonlythroughthe\nunmaskedunits.Attesttime,thelearnedweight\nvectorsarescaledby\np\nsuchthat\n^\nw\n=\np\nw\n,and\n^\nw\nisused(withoutdropout)toscoreunseensen-\ntences.Weadditionallyconstrain\nl\n2\n-normsofthe\nweightvectorsbyrescaling\nw\ntohave\njj\nw\njj\n2\n=\ns\nwhenever\njj\nw\njj\n2\n>s\nafteragradientdescentstep.\n2\nWeemploylanguagefromcomputervisionwhereacolor\nimagehasred,green,andbluechannels.\n"b"Data\nc\nl\nN\nj\nV\nj\nj\nV\npre\nj\nTest\nMR\n2\n20\n10662\n18765\n16448\nCV\nSST-1\n5\n18\n11855\n17836\n16262\n2210\nSST-2\n2\n19\n9613\n16185\n14838\n1821\nSubj\n2\n23\n10000\n21323\n17913\nCV\nTREC\n6\n10\n5952\n9592\n9125\n500\nCR\n2\n19\n3775\n5340\n5046\nCV\nMPQA\n2\n3\n10606\n6246\n6083\nCV\nTable1:Summarystatisticsforthedatasetsaftertokeniza-\ntion.\nc\n:Numberoftargetclasses.\nl\n:Averagesentencelength.\nN\n:Datasetsize.\nj\nV\nj\n:Vocabularysize.\nj\nV\npre\nj\n:Numberof\nwordspresentinthesetofpre-trainedwordvectors.\nTest\n:\nTestsetsize(CVmeanstherewasnostandardtrain/testsplit\nandthus10-foldCVwasused).\n3DatasetsandExperimentalSetup\nWetestourmodelonvariousbenchmarks.Sum-\nmarystatisticsofthedatasetsareintable1.\n\nMR\n:Moviereviewswithonesentenceperre-\nview.Classiinvolvesdetectingposi-\ntive/negativereviews(PangandLee,2005).\n3\n\nSST-1\n:StanfordSentimentTreebankan\nextensionofMRbutwithtrain/dev/testsplits\nprovidedandlabels(verypos-\nitive,positive,neutral,negative,verynega-\ntive),re-labeledbySocheretal.(2013).\n4\n\nSST-2\n:SameasSST-1butwithneutralre-\nviewsremovedandbinarylabels.\n\nSubj\n:Subjectivitydatasetwherethetaskis\ntoclassifyasentenceasbeingsubjectiveor\nobjective(PangandLee,2004).\n\nTREC\n:TRECquestiondatasettaskin-\nvolvesclassifyingaquestioninto6question\ntypes(whetherthequestionisaboutperson,\nlocation,numericinformation,etc.)(Liand\nRoth,2002).\n5\n\nCR\n:Customerreviewsofvariousproducts\n(cameras,MP3setc.).Taskistopredictpos-\nitive/negativereviews(HuandLiu,2004).\n6\n3\nhttps://www.cs.cornell.edu/people/pabo/movie-review-data/\n4\nhttp://nlp.stanford.edu/sentiment/Dataisactuallyprovided\natthephrase-levelandhencewetrainthemodelonboth\nphrasesandsentencesbutonlyscoreonsentencesattest\ntime,asinSocheretal.(2013),Kalchbrenneretal.(2014),\nandLeandMikolov(2014).Thusthetrainingsetisanorder\nofmagnitudelargerthanlistedintable1.\n5\nhttp://cogcomp.cs.illinois.edu/Data/QA/QC/\n6\nhttp://www.cs.uic.edu/\n\nliub/FBS/sentiment-analysis.html\n\nMPQA\n:Opinionpolaritydetectionsubtask\noftheMPQAdataset(Wiebeetal.,2005).\n7\n3.1HyperparametersandTraining\nForalldatasetsweuse:linearunits,\nwindows(\nh\n)of3,4,5with100featuremapseach,\ndropoutrate(\np\n)of0.5,\nl\n2\nconstraint(\ns\n)of3,and\nmini-batchsizeof50.Thesevalueswerechosen\nviaagridsearchontheSST-2devset.\nWedonototherwiseperformanydataset-\ntuningotherthanearlystoppingondev\nsets.Fordatasetswithoutastandarddevsetwe\nrandomlyselect10%ofthetrainingdataasthe\ndevset.Trainingisdonethroughstochasticgra-\ndientdescentovershufmini-batcheswiththe\nAdadeltaupdaterule(Zeiler,2012).\n3.2Pre-trainedWordVectors\nInitializingwordvectorswiththoseobtainedfrom\nanunsupervisedneurallanguagemodelisapopu-\nlarmethodtoimproveperformanceintheabsence\nofalargesupervisedtrainingset(Collobertetal.,\n2011;Socheretal.,2011;Iyyeretal.,2014).We\nusethepubliclyavailable\nword2vec\nvectorsthat\nweretrainedon100billionwordsfromGoogle\nNews.Thevectorshavedimensionalityof300and\nweretrainedusingthecontinuousbag-of-words\narchitecture(Mikolovetal.,2013).Wordsnot\npresentinthesetofpre-trainedwordsareinitial-\nizedrandomly.\n3.3ModelVariations\nWeexperimentwithseveralvariantsofthemodel.\n\nCNN-rand\n:Ourbaselinemodelwhereall\nwordsarerandomlyinitializedandthenmod-\nduringtraining.\n\nCNN-static\n:Amodelwithpre-trained\nvectorsfrom\nword2vec\n.Allwords\nincludingtheunknownonesthatareran-\ndomlyinitializedarekeptstaticandonly\ntheotherparametersofthemodelarelearned.\n\nCNN-non-static\n:Sameasabovebutthepre-\ntrainedvectorsareforeachtask.\n\nCNN-multichannel\n:Amodelwithtwosets\nofwordvectors.Eachsetofvectorsistreated\nasa`channel'andeachisapplied\n7\nhttp://www.cs.pitt.edu/mpqa/\n"b"Model\nMR\nSST-1\nSST-2\nSubj\nTREC\nCR\nMPQA\nCNN-rand\n76\n:\n1\n45\n:\n0\n82\n:\n7\n89\n:\n6\n91\n:\n2\n79\n:\n8\n83\n:\n4\nCNN-static\n81\n:\n0\n45\n:\n5\n86\n:\n8\n93\n:\n0\n92\n:\n8\n84\n:\n7\n89\n:\n6\nCNN-non-static\n81\n:\n5\n48\n:\n0\n87\n:\n2\n93\n:\n4\n93\n:\n6\n84\n:\n3\n89\n:\n5\nCNN-multichannel\n81\n:\n1\n47\n:\n4\n88\n:\n1\n93\n:\n2\n92\n:\n2\n85\n:\n0\n89\n:\n4\nRAE(Socheretal.,2011)\n77\n:\n7\n43\n:\n2\n82\n:\n4\n\n\n\n86\n:\n4\nMV-RNN(Socheretal.,2012)\n79\n:\n0\n44\n:\n4\n82\n:\n9\n\n\n\n\nRNTN(Socheretal.,2013)\n\n45\n:\n7\n85\n:\n4\n\n\n\n\nDCNN(Kalchbrenneretal.,2014)\n\n48\n:\n5\n86\n:\n8\n\n93\n:\n0\n\n\nParagraph-Vec(LeandMikolov,2014)\n\n48\n:\n7\n87\n:\n8\n\n\n\n\nCCAE(HermannandBlunsom,2013)\n77\n:\n8\n\n\n\n\n\n87\n:\n2\nSent-Parser(Dongetal.,2014)\n79\n:\n5\n\n\n\n\n\n86\n:\n3\nNBSVM(WangandManning,2012)\n79\n:\n4\n\n\n93\n:\n2\n\n81\n:\n8\n86\n:\n3\nMNB(WangandManning,2012)\n79\n:\n0\n\n\n93\n:\n6\n\n80\n:\n0\n86\n:\n3\nG-Dropout(WangandManning,2013)\n79\n:\n0\n\n\n93\n:\n4\n\n82\n:\n1\n86\n:\n1\nF-Dropout(WangandManning,2013)\n79\n:\n1\n\n\n93\n:\n6\n\n81\n:\n9\n86\n:\n3\nTree-CRF(Nakagawaetal.,2010)\n77\n:\n3\n\n\n\n\n81\n:\n4\n86\n:\n1\nCRF-PR(YangandCardie,2014)\n\n\n\n\n\n82\n:\n7\n\nSVM\nS\n(Silvaetal.,2011)\n\n\n\n\n95\n:\n0\n\n\nTable2:ResultsofourCNNmodelsagainstothermethods.\nRAE\n:RecursiveAutoencoderswithpre-trainedwordvectorsfrom\nWikipedia(Socheretal.,2011).\nMV-RNN\n:Matrix-VectorRecursiveNeuralNetworkwithparsetrees(Socheretal.,2012).\nRNTN\n:RecursiveNeuralTensorNetworkwithtensor-basedfeaturefunctionandparsetrees(Socheretal.,2013).\nDCNN\n:\nDynamicConvolutionalNeuralNetworkwithk-maxpooling(Kalchbrenneretal.,2014).\nParagraph-Vec\n:Logisticregres-\nsionontopofparagraphvectors(LeandMikolov,2014).\nCCAE\n:CombinatorialCategoryAutoencoderswithcombinatorial\ncategorygrammaroperators(HermannandBlunsom,2013).\nSent-Parser\n:Sentimentparser(Dongetal.,\n2014).\nNBSVM,MNB\n:NaiveBayesSVMandMultinomialNaiveBayeswithuni-bigramsfromWangandManning(2012).\nG-Dropout,F-Dropout\n:GaussianDropoutandFastDropoutfromWangandManning(2013).\nTree-CRF\n:Dependencytree\nwithConditionalRandomFields(Nakagawaetal.,2010).\nCRF-PR\n:ConditionalRandomFieldswithPosteriorRegularization\n(YangandCardie,2014).\nSVM\nS\n:SVMwithuni-bi-trigrams,whword,headword,POS,parser,hypernyms,and60hand-coded\nrulesasfeaturesfromSilvaetal.(2011).\ntobothchannels,butgradientsareback-\npropagatedonlythroughoneofthechan-\nnels.Hencethemodelisableto\nonesetofvectorswhilekeepingtheother\nstatic.Bothchannelsareinitializedwith\nword2vec\n.\nInordertodisentangletheeffectoftheabove\nvariationsversusotherrandomfactors,weelim-\ninateothersourcesofrandomnessCV-foldas-\nsignment,initializationofunknownwordvec-\ntors,initializationofCNNparametersbykeep-\ningthemuniformwithineachdataset.\n4ResultsandDiscussion\nResultsofourmodelsagainstothermethodsare\nlistedintable2.Ourbaselinemodelwithallran-\ndomlyinitializedwords(CNN-rand)doesnotper-\nformwellonitsown.Whilewehadexpectedper-\nformancegainsthroughtheuseofpre-trainedvec-\ntors,weweresurprisedatthemagnitudeofthe\ngains.Evenasimplemodelwithstaticvectors\n(CNN-static)performsremarkablywell,giving\ncompetitiveresultsagainstthemoresophisticated\ndeeplearningmodelsthatutilizecomplexpool-\ningschemes(Kalchbrenneretal.,2014)orrequire\nparsetreestobecomputedbeforehand(Socher\netal.,2013).Theseresultssuggestthatthepre-\ntrainedvectorsaregood,`universal'featureex-\ntractorsandcanbeutilizedacrossdatasets.Fine-\ntuningthepre-trainedvectorsforeachtaskgives\nstillfurtherimprovements(CNN-non-static).\n4.1Multichannelvs.SingleChannelModels\nWehadinitiallyhopedthatthemultichannelar-\nchitecturewouldpreventov(byensuring\nthatthelearnedvectorsdonotdeviatetoofar\nfromtheoriginalvalues)andthusworkbetterthan\nthesinglechannelmodel,especiallyonsmaller\ndatasets.Theresults,however,aremixed,andfur-\ntherworkonregularizingtheprocess\niswarranted.Forinstance,insteadofusingan\nadditionalchannelforthenon-staticportion,one\ncouldmaintainasinglechannelbutemployextra\ndimensionsthatareallowedtobeduring\ntraining.\n"b"MostSimilarWordsfor\nStaticChannel\nNon-staticChannel\nbad\ngood\nterrible\nterrible\nhorrible\nhorrible\nlousy\nlousy\nstupid\ngood\ngreat\nnice\nbad\ndecent\n\nsolid\ndecent\n\nn't\nos\nnot\nca\nnever\nireland\nnothing\nwo\nneither\n!\n2,500\n2,500\nentire\nlush\njez\nbeautiful\nchanger\n\n,\ndecasia\nbut\nabysmally\ndragon\ndemise\na\nvaliant\nand\nTable3:Top4neighboringwordsbasedoncosine\nsimilarityforvectorsinthestaticchannel(left)and\ntunedvectorsinthenon-staticchannel(right)fromthemul-\ntichannelmodelontheSST-2datasetaftertraining.\n4.2Staticvs.Non-staticRepresentations\nAsisthecasewiththesinglechannelnon-static\nmodel,themultichannelmodelisableto\nthenon-staticchanneltomakeitmoreto\nthetask-at-hand.Forexample,\ngood\nismostsim-\nilarto\nbad\nin\nword2vec\n,presumablybecause\ntheyare(almost)syntacticallyequivalent.Butfor\nvectorsinthenon-staticchannelthatwere\ntunedontheSST-2dataset,thisisnolongerthe\ncase(table3).Similarly,\ngood\nisarguablycloser\nto\nnice\nthanitisto\ngreat\nforexpressingsentiment,\nandthisisindeedinthelearnedvectors.\nFor(randomlyinitialized)tokensnotintheset\nofpre-trainedvectors,allowsthemto\nlearnmoremeaningfulrepresentations:thenet-\nworklearnsthatexclamationmarksareassoci-\natedwitheffusiveexpressionsandthatcommas\nareconjunctive(table3).\n4.3FurtherObservations\nWereportonsomefurtherexperimentsandobser-\nvations:\n\nKalchbrenneretal.(2014)reportmuch\nworseresultswithaCNNthathasessentially\nthesamearchitectureasoursinglechannel\nmodel.Forexample,theirMax-TDNN(Time\nDelayNeuralNetwork)withrandomlyini-\ntializedwordsobtains\n37\n:\n4%\nontheSST-1\ndataset,comparedto\n45\n:\n0%\nforourmodel.\nWeattributesuchdiscrepancytoourCNN\nhavingmuchmorecapacity(multiple\nwidthsandfeaturemaps).\n\nDropoutprovedtobesuchagoodregularizer\nthatitwastousealargerthannecessary\nnetworkandsimplyletdropoutregularizeit.\nDropoutconsistentlyadded2%4%relative\nperformance.\n\nWhenrandomlyinitializingwordsnotin\nword2vec\n,weobtainedslightimprove-\nmentsbysamplingeachdimensionfrom\nU\n[\n\na;a\n]\nwhere\na\nwaschosensuchthatthe\nrandomlyinitializedvectorshavethesame\nvarianceasthepre-trainedones.Itwouldbe\ninterestingtoseeifemployingmoresophis-\nticatedmethodstomirrorthedistributionof\npre-trainedvectorsintheinitializationpro-\ncessgivesfurtherimprovements.\n\nWeexperimentedwithanothersetof\npubliclyavailablewordvectorstrainedby\nCollobertetal.(2011)onWikipedia,\n8\nand\nfoundthat\nword2vec\ngavefarsuperiorper-\nformance.Itisnotclearwhetherthisisdue\ntoMikolovetal.(2013)'sarchitectureorthe\n100billionwordGoogleNewsdataset.\n\nAdadelta(Zeiler,2012)gavesimilarresults\ntoAdagrad(Duchietal.,2011)butrequired\nfewerepochs.\n5Conclusion\nInthepresentworkwehavedescribedaseriesof\nexperimentswithconvolutionalneuralnetworks\nbuiltontopof\nword2vec\n.Despitelittletuning\nofhyperparameters,asimpleCNNwithonelayer\nofconvolutionperformsremarkablywell.Ourre-\nsultsaddtothewell-establishedevidencethatun-\nsupervisedpre-trainingofwordvectorsisanim-\nportantingredientindeeplearningforNLP.\nAcknowledgments\nWewouldliketothankYannLeCunandthe\nanonymousreviewersfortheirhelpfulfeedback\nandsuggestions.\n8\nhttp://ronan.collobert.com/senna/\n"b'References\nY.Bengio,R.Ducharme,P.Vincent.2003.Neu-\nralProbabilitisticLanguageModel.\nJournalofMa-\nchineLearningResearch\n3:11371155.\nR.Collobert,J.Weston,L.Bottou,M.Karlen,K.\nKavukcuglu,P.Kuksa.2011.NaturalLanguage\nProcessing(Almost)fromScratch.\nJournalofMa-\nchineLearningResearch12:24932537\n.\nJ.Duchi,E.Hazan,Y.Singer.2011Adaptivesubgra-\ndientmethodsforonlinelearningandstochasticop-\ntimization.\nJournalofMachineLearningResearch,\n12:21212159.\nL.Dong,F.Wei,S.Liu,M.Zhou,K.Xu.2014.A\nStatisticalParsingFrameworkforSentimentClassi-\n\nCoRR,abs/1401.6330.\nA.Graves,A.Mohamed,G.Hinton.2013.Speech\nrecognitionwithdeeprecurrentneuralnetworks.\nIn\nProceedingsofICASSP2013.\nG.Hinton,N.Srivastava,A.Krizhevsky,I.Sutskever,\nR.Salakhutdinov.2012.Improvingneuralnet-\nworksbypreventingco-adaptationoffeaturedetec-\ntors.\nCoRR,abs/1207.0580\n.\nK.Hermann,P.Blunsom.2013.TheRoleofSyntaxin\nVectorSpaceModelsofCompositionalSemantics.\nInProceedingsofACL2013\n.\nM.Hu,B.Liu.2004.MiningandSummarizingCus-\ntomerReviews.\nInProceedingsofACMSIGKDD\n2004.\nM.Iyyer,P.Enns,J.Boyd-Graber,P.Resnik2014.\nPoliticalIdeologyDetectionUsingRecursiveNeural\nNetworks.\nInProceedingsofACL2014.\nN.Kalchbrenner,E.Grefenstette,P.Blunsom.2014.A\nConvolutionalNeuralNetworkforModellingSen-\ntences.\nInProceedingsofACL2014.\nA.Krizhevsky,I.Sutskever,G.Hinton.2012.Ima-\ngeNetwithDeepConvolutionalNeu-\nralNetworks.\nInProceedingsofNIPS2012.\nQ.Le,T.Mikolov.2014.DistributedRepresenations\nofSentencesandDocuments.\nInProceedingsof\nICML2014.\nY.LeCun,L.Bottou,Y.Bengio,P.Haffner.1998.\nGradient-basedlearningappliedtodocumentrecog-\nnition.\nInProceedingsoftheIEEE\n,86(11):2278\n2324,November.\nX.Li,D.Roth.2002.LearningQuestion\nInProceedingsofACL2002.\nT.Mikolov,I.Sutskever,K.Chen,G.Corrado,J.Dean.\n2013.DistributedRepresentationsofWordsand\nPhrasesandtheirCompositionality.\nInProceedings\nofNIPS2013.\nT.Nakagawa,K.Inui,S.Kurohashi.2010.De-\npendencytree-basedsentimentusing\nCRFswithhiddenvariables.\nInProceedingsofACL\n2010.\nB.Pang,L.Lee.2004.Asentimentaleducation:\nSentimentanalysisusingsubjectivitysummarization\nbasedonminimumcuts.\nInProceedingsofACL\n2004.\nB.Pang,L.Lee.2005.Seeingstars:Exploitingclass\nrelationshipsforsentimentcategorizationwithre-\nspecttoratingscales.\nInProceedingsofACL2005.\nA.S.Razavian,H.Azizpour,J.Sullivan,S.Carlsson\n2014.CNNFeaturesoff-the-shelf:anAstounding\nBaseline.\nCoRR,abs/1403.6382\n.\nY.Shen,X.He,J.Gao,L.Deng,G.Mesnil.2014.\nLearningSemanticRepresentationsUsingConvolu-\ntionalNeuralNetworksforWebSearch.\nInProceed-\ningsofWWW2014\n.\nJ.Silva,L.Coheur,A.Mendes,A.Wichert.2011.\nFromsymbolictosub-symbolicinformationinques-\ntion\nIntelligenceReview\n,\n35(2):137154.\nR.Socher,J.Pennington,E.Huang,A.Ng,C.Man-\nning.2011.Semi-SupervisedRecursiveAutoen-\ncodersforPredictingSentimentDistributions.\nIn\nProceedingsofEMNLP2011.\nR.Socher,B.Huval,C.Manning,A.Ng.2012.Se-\nmanticCompositionalitythroughRecursiveMatrix-\nVectorSpaces.\nInProceedingsofEMNLP2012.\nR.Socher,A.Perelygin,J.Wu,J.Chuang,C.Manning,\nA.Ng,C.Potts.2013.RecursiveDeepModelsfor\nSemanticCompositionalityOveraSentimentTree-\nbank.\nInProceedingsofEMNLP2013.\nJ.Wiebe,T.Wilson,C.Cardie.2005.AnnotatingEx-\npressionsofOpinionsandEmotionsinLanguage.\nLanguageResourcesandEvaluation\n,39(2-3):165\n210.\nS.Wang,C.Manning.2012.BaselinesandBigrams:\nSimple,GoodSentimentandTopic\nInProceedingsofACL2012.\nS.Wang,C.Manning.2013.FastDropoutTraining.\nInProceedingsofICML2013\n.\nB.Yang,C.Cardie.2014.Context-awareLearning\nforSentence-levelSentimentAnalysiswithPoste-\nriorRegularization.\nInProceedingsofACL2014\n.\nW.Yih,K.Toutanova,J.Platt,C.Meek.2011.Learn-\ningDiscriminativeProjectionsforTextSimilarity\nMeasures.\nProceedingsoftheFifteenthConfer-\nenceonComputationalNaturalLanguageLearning,\n247256\n.\nW.Yih,X.He,C.Meek.2014.SemanticParsingfor\nSingle-RelationQuestionAnswering.\nInProceed-\ningsofACL2014\n.\nM.Zeiler.2012.Adadelta:Anadaptivelearningrate\nmethod.\nCoRR,abs/1212.5701\n.\n'