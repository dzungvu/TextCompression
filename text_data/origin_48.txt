b"IEEETRANSACTIONSONNEURALNETWORKS,VOL.10,NO.5,SEPTEMBER1999\n1055SupportVectorMachinesfor\nHistogram-BasedImage\nOlivierChapelle,PatrickHaffner,andVladimirN.Vapnik\nAbstract\nTraditionalapproachesgeneralize\npoorlyonimagetasks,becauseofthehigh\n\ndimensionalityofthefeaturespace.Thispapershowsthat\n\nsupportvectormachines(SVM's)cangeneralizewellon\n\nimageproblemswheretheonlyfeaturesare\n\nhighdimensionalhistograms.Heavy-tailedRBFkernelsof\ntheform\nK(x;y)=ejxyjwitha1andb2areevaluatedontheofimagesextractedfrom\ntheCorelstockphotocollectionandshowntofaroutperform\n\ntraditionalpolynomialorGaussianradialbasisfunction(RBF)\n\nkernels.Moreover,weobservedthatasimpleremappingofthe\n\ninputxi!xaiimprovestheperformanceoflinearSVM'sto\nsuchanextendthatitmakesthem,forthisproblem,avalid\n\nalternativetoRBFkernels.\nIndexTerms\nCorel,imageimagehistogram,\nradialbasisfunctions,supportvectormachines.\nI.I\nNTRODUCTIONLARGEcollectionsofimagesarebecomingavailableto\nthepublic,fromphotocollectionstoWebpagesoreven\nvideodatabases.Toindexorretrievethemisachallengewhich\n\nisthefocusofmanyresearchprojects(forinstanceIBM's\n\nQBIC[1]).Alargepartofthisresearchworkisdevotedto\nsuitablerepresentationsfortheimages,andretrieval\ngenerallyinvolvescomparisonsofimages.Inthispaper,we\n\nchoosetousecolorhistogramsasanimagerepresentation\n\nbecauseofthereasonableperformancethatcanbeobtained\ninspiteoftheirextremesimplicity[2].Usingthishistogram\nrepresentation,ourinitialgoalistoperformgenericobject\n\nwithawinnertakesallapproach:theone\n\ncategoryofobjectthatisthemostlikelytobepresentina\ngivenimage.\nFromtreestoneuralnetworks,therearemany\npossiblechoicesforwhattouse.Thesupportvector\n\nmachine(SVM)approachisconsideredagoodcandidate\nbecauseofitshighgeneralizationperformancewithoutthe\nneedtoadd\napriori\nknowledge,evenwhenthedimensionof\ntheinputspaceisveryhigh.\nIntuitively,givenasetofpointswhichbelongstoeither\noneoftwoclasses,alinearSVMthehyperplaneleaving\nthelargestpossiblefractionofpointsofthesameclassonthe\n\nsameside,whilemaximizingthedistanceofeitherclassfrom\n\nthehyperplane.Accordingto[3],thishyperplaneminimizes\ntheriskofmisclassifyingexamplesofthetestset.\nManuscriptreceivedJanuary21,1999;revisedApril30,1999.\nTheauthorsarewiththeSpeechandImageProcessingServicesResearch\nLaboratory,AT&TLabs-Research,RedBank,NJ07701USA.\nPublisherItemS1045-9227(99)07269-0.\nThispaperfollowsanexperimentalapproach,anditsor-\nganizationunfoldsasincreasinglybetterresultsareobtained\nthroughoftheSVMarchitecture.SectionII\nprovidesabriefintroductiontoSVM's.SectionIIIdescribes\n\ntheimagerecognitionproblemonCorelphotoimages.Section\n\nIVcomparesSVMandKNN-basedrecognitiontechniques\nwhichareinspiredbypreviouswork.Fromtheseresults,\nSectionVexploresnoveltechniques,byeitherselectingthe\n\nSVMkernel,orremappingtheinput,thatprovidehighimage\n\nrecognitionperformancewithlowcomputationalrequirements.\nII.S\nUPPORT\nVECTOR\nMACHINESA.OptimalSeparatingHyperplanes\nWegiveinthissectionaverybriefintroductiontoSVM's.\nLetbeasetoftrainingexamples,eachexample\nbeingthedimensionoftheinputspace,belongs\ntoaclasslabeledby\n.Theaimistoa\nhyperplanewhichdividesthesetofexamplessuchthatall\n\nthepointswiththesamelabelareonthesamesideofthe\n\nhyperplane.Thisamountsto\nandsothat\n(1)Ifthereexistsahyperplanesatisfying(1),thesetissaid\ntobe\nlinearlyseparable\n.Inthiscase,itisalwayspossibleto\nrescaleandsothat\ni.e.,sothatthedistancebetweentheclosestpointtothe\n\nhyperplaneis\n.Then,(1)becomes\n(2)Amongtheseparatinghyperplanes,theoneforwhichthe\ndistancetotheclosestpointismaximaliscalled\noptimalseparatinghyperplane\n(OSH).Sincethedistancetotheclosest\npointis\n,theOSHamountstominimizing\nunderconstraints(2).\nThequantity\niscalledthemargin,andthustheOSH\nistheseparatinghyperplanewhichmaximizesthemargin.The\nmargincanbeseenasameasureofthegeneralizationability:\nthelargerthemargin,thebetterthegeneralizationisexpected\n\ntobe[4],[5].\nSinceisconvex,minimizingitunderlinearconstraints\n(2)canbeachievedwithLagrangemultipliers.Ifwedenote\n10459227/9910.001999IEEE\n"b"1056IEEETRANSACTIONSONNEURALNETWORKS,VOL.10,NO.5,SEPTEMBER1999\nbythenonnegativeLagrangemultipli-\nersassociatedwithconstraints(2),ouroptimizationproblem\n\namountstomaximizing\n(3)withandunderconstraint\n.Thiscan\nbeachievedbytheuseofstandardquadraticprogramming\n\nmethods[6].\nOncethevector\nsolutionofthemaxi-\nmizationproblem(3)hasbeenfound,theOSH\nhasthefollowingexpansion:\n(4)Thesupportvectors\narethepointsforwhich\nsatisfy(2)withequality.\nConsideringtheexpansion(4)of\n,thehyperplanedeci-\nsionfunctioncanthusbewrittenas\n(5)B.LinearlyNonseparableCase\nWhenthedataisnotlinearlyseparable,weintroduceslack\nvariableswith[7]suchthat\n(6)toallowthepossibilityofexamplesthatviolate(2).The\npurposeofthevariables\nistoallowpoints,\nwhichhavetheircorresponding\n.Therefore\nisan\nupperboundonthenumberoftrainingerrors.Thegeneralized\nOSHisthenregardedasthesolutionofthefollowingproblem:\nminimize(7)subjecttoconstraints(6)and\n.Thetermis\nminimizedtocontrolthelearningcapacityasintheseparable\ncase;thepurposeofthesecondtermistocontrolthenumberof\npoints.Theparameter\nischosenbytheuser,a\nlarger\ncorrespondingtoassigningahigherpenaltytoerrors.\nSVMtrainingrequiresto\nin(7),thepenaltyterm\nforWhendealingwithimages,mostof\nthetime,thedimensionoftheinputspaceislarge(\n1000)comparedtothesizeofthetrainingset,sothatthetraining\n\ndataisgenerallylinearlyseparable.Consequently,thevalue\nofhasinthiscaselittleimpactonperformance.\nC.NonlinearSupportVectorMachines\nTheinputdataismappedintoahigh-dimensional\nfeature\nspacethroughsomenonlinearmappingchosen\napriori\n[8].In\nthisfeaturespace,theOSHisconstructed.\nIfwereplace\nbyitsmappinginthefeaturespace\n,(3)becomes\nIfwehave\n,thenonly\nisneededinthetrainingalgorithmandthemapping\nisnever\nexplicitlyused.Conversely,givenasymmetricpositivekernel\n,Mercer'stheorem[3]indicatesusthatthereexistsa\nmappingsuchthat\nOnceakernel\nsatisfyingMercer'sconditionhasbeen\nchosen,thetrainingalgorithmconsistsofminimizing\n(8)andthedecisionfunctionbecomes\n(9)D.MulticlassLearning\nSVM'saredesignedforbinaryWhendealing\nwithseveralclasses,asinobjectrecognitionandimage\n\noneneedsanappropriatemulticlassmethod.\n\nDifferentpossibilitiesincludethefollowing.\nModifythedesignoftheSVM,asin[9],inorder\ntoincorporatethemulticlasslearningdirectlyinthe\n\nquadraticsolvingalgorithm.\nCombineseveralbinaryOneagainstone\n[10]appliespairwisecomparisonsbetweenclasses,while\n\nOneagainsttheothers[11]comparesagivenclasswith\n\nalltheothersputtogether.\nAccordingtoacomparisonstudy[9],theaccuraciesofthese\nmethodsarealmostthesame.Asaconsequence,wechosethe\n\nonewiththelowestcomplexity,whichisoneagainstthe\nothers.Intheoneagainsttheothersalgorithm,\nhyperplanesare\nconstructed,where\nisthenumberofclasses.Eachhyperplane\nseparatesoneclassfromtheotherclasses.Inthisway,weget\ndecisionfunctions\noftheform(5).Theclassof\nanewpoint\nisgivenby\n,i.e.,theclasswith\nthelargestdecisionfunction.\nWemadetheassumptionthateverypointhasasinglelabel.\nNevertheless,inimageanimagemaybelong\ntoseveralclassesasitscontentisnotunique.Itwouldbe\n\npossibletomakemulticlasslearningmorerobust,andextend\n\nittohandlemultilabelproblemsbyusingerror\ncorrectingcodes[12].Thismorecomplexapproachhasnot\nbeenexperimentedinthispaper.\nIII.T\nHEDATAAND\nITSREPRESENTATION\nAmongthemanypossiblefeaturesthatcanbeextracted\nfromanimage,werestrictourselvestooneswhichareglobal\nandlow-level(thesegmentationoftheimageintoregions,\n\nobjectsorrelationsisnotinthescopeofthepresentpaper).\n"b"CHAPELLEetal.\n:SVM'SFORHISTOGRAM-BASEDIMAGECLASSIFICATION\n1057Thesimplestwaytorepresentanimageistoconsiderits\nbitmaprepresentation.Assumingthesizesoftheimagesin\n\nthedatabaseareto\n(fortheheightand\nforthewidth),thentheinputdatafortheSVMarevectors\nofsize\nforgrey-levelimagesand3\nforcolor\nimages.Eachcomponentofthevectorisassociatedtoapixel\n\nintheimage.Somemajordrawbacksofthisrepresentation\n\nareitslargesizeanditslackofinvariancewithrespect\ntotranslations.Forthesereasons,ourchoicewasthe\nhistogramrepresentationwhichisdescribedpresently.\nA.ColorHistograms\nInspiteofthefactthatthecolorhistogramtechniqueisa\nverysimpleandlow-levelmethod,ithasshowngoodresultsin\npractice[2]especiallyforimageindexingandretrievaltasks,\nwherefeatureextractionhastobeassimpleandasfastas\n\npossible.Spatialfeaturesarelost,meaningthatspatialrelations\nbetweenpartsofanimagecannotbeused.Thisalsoensures\nfulltranslationandrotationinvariance.\nAcolorisrepresentedbyathreedimensionalvectorcorre-\nspondingtoapositioninacolorspace.Thisleavesustoselect\nthecolorspaceandthequantizationstepsinthiscolorspace.\nAsacolorspace,wechosethehue-saturation-value(HSV)\n\nspace,whichisinbijectionwiththeredgreenblue(RGB)\n\nspace.ThereasonforthechoiceofHSVisthatitiswidely\nusedintheliterature.\nHSVisattractiveintheory.Itisconsideredmoresuitable\nsinceitseparatesthecolorcomponents(HS)fromthelu-\n\nminancecomponent(V)andislesssensitivetoillumination\nchanges.NotealsothatdistancesintheHSVspacecorrespond\ntoperceptualdifferencesincolorinamoreconsistentway\n\nthanintheRGBspace.\nHowever,thisdoesnotseemtomatterinpractice.Allthe\nexperimentsreportedinthepaperusetheHSVspace.For\nthesakeofcomparison,wehaveselectedafewexperiments\n\nandusedtheRGBspaceinsteadoftheHSVspace,while\n\nkeepingtheotherconditionsidentical:theimpactofthechoice\nofthecolorspaceonperformancewasfoundtobeminimal\ncomparedtotheimpactsoftheotherexperimentalconditions\n\n(choiceofthekernel,remappingoftheinput).Anexplanation\n\nforthisfactisthat,afterquantizationintobins,noinformation\naboutthecolorspaceisusedbythe\nThenumberofbinspercolorcomponenthasbeen\nto16,andthedimensionofeachhistogramis\n.Someexperimentswithasmallernumberofbinshavebeen\nundertaken,butthebestresultshavebeenreachedwith16\nbins.Wehavenottriedtoincreasethisnumber,becauseit\n\niscomputationallytoointensive.Itispreferabletocompute\n\nthehistogramfromthehighestspatialresolutionavailable.\nSubsamplingtheimagetoomuchresultsinlosses\ninperformance.Thismaybeexplainedbythefactthatby\n\nsubsampling,thehistogramlosesitssharppeaks,aspixel\n\ncolorsturnintoaverages(aliasing).\nB.SelectingClassesofImagesintheCorel\nStockPhotoCollection\nTheCorelstockphotocollectionconsistsofasetof\nphotographsdividedintoabout200categories,eachonewith\n100images.Forourexperiments,theoriginal200categories\nhavebeenreducedusingtwodifferentlabelingapproaches.\n\nIntheone,named\nCorel14\n,wechosetokeepthecat-\negoriesbyCorel.Forthesakeofcomparison,we\nchosethesamesubsetofcategoriesas[13],whichare:\nairshows,bears,elephants,tigers,Arabianhorses,polar\n\nbears,Africanspecialtyanimals,cheetahs-leopards-jaguars,\n\nbaldeagles,mountains,deserts,sunrises-sunsets,night\nscenes.Itisimportanttonotethatwehad\nno\nonthe\nchoicesmadein\nCorel14\n:theclasseswereselectedby[13]\nandtheexamplesillustratingaclassarethe100imageswe\n\nfoundinaCorelcategory.In[13],someimageswhichwere\nvisuallydeemedinconsistentwiththerestoftheircategory\nwereremoved.Intheresultsreportedinthispaper,weuseall\n\n100imagesineachcategoryandkeptmanyobviousoutliers:\n\nseeforinstance,inFig.2,thepolarbearalertsignwhichis\nconsideredtobeanimageofapolarbear.With14categories,\nthisresultsinadatabaseof1400images.NotethatsomeCorel\n\ncategoriescomefromthesamebatchofphotographs:asystem\n\ntrainedtoclassifythemmayonlyhavetoclassifycolorand\nexposureidiosyncracies.\nInanattempttoavoidthesepotentialproblemsandto\nmovetowardamoregenericwealso\n\nasecondlabelingapproach,\nCorel7\n,inwhichwedesignedour\nownsevencategories:\nairplanes,birds,boats,buildings,\npeople,vehicles\n.Thenumberofimagesineachcategoryvaries\nfrom300to625foratotalof2670samples.\nForeachcategoryimageswerehand-pickedfromseveral\noriginalCorelcategories.Forexample,the\nairplanescategoryincludesimagesof\nairshows,aviationphotography,jets\nandWW-IIplanes\n.Therepresentationofwhatisanairplane\nisthenmoregeneral.TableIshowstheoriginoftheimages\nforeachcategory.\nIV.S\nELECTINGTHE\nKERNELA.Introduction\nThedesignoftheSVMarchitectureisverysimple\nandmainlyrequiresthechoiceofthekernel(theonlyother\nparameteris\n).Nevertheless,ithastobechosencarefully\nsinceaninappropriatekernelcanleadtopoorperformance.\n\nTherearecurrentlynotechniquesavailabletolearntheform\n\nofthekernel;asaconsequence,thekernelsinvestigated\nwereborrowedfromthepatternrecognitionliterature.The\nkernelproductsbetweeninputvectors\nandareresultsinawhichhasapolynomialdecision\nfunction.givesaGaussianradialbasisfunction\n(RBF)IntheGaussianRBFcase,thenumberof\n\ncenters(numberofsupportvectors),thecentersthemselves\n\n(thesupportvectors),theweights\nandthethreshold\nareallproducedautomaticallybytheSVMtrainingandgive\n\nexcellentresultscomparedtoRBF'strainedwithnon-SVM's\n\nmethods[14].\n"b"1058IEEETRANSACTIONSONNEURALNETWORKS,VOL.10,NO.5,SEPTEMBER1999\nFig.1.Corel14:eachrowincludesimagesfromthefollowingsevencategories:airshows,bears,Arabianhorses,nightscenes,elephants,baldeagle\ns,cheetahs-leopards-jaguars.Encouragedbythepositiveresultsobtainedwith\n,welookedatgeneralizedformsofRBFkernels\nwherecanbechosentobeanydistanceintheinput\nspace.Inthecaseofimagesasinput,the\nnormseemsto\nbequitemeaningful.Butashistogramsarediscretedensities,\nmoresuitablecomparisonfunctionsexist,especiallythe\nfunction,whichhasbeenusedextensivelyforhistogram\ncomparisons[15].Weusehereasymmetrizedapproximation\nofItisnotknownifthekernelMercer'scondition.\n1Anotherobviousalternativeisthe\ndistance,whichgives\naLaplacianRBF\n1ItisstillpossibleapplytheSVMtrainingproceduretokernelsthatdonot\nsatisfyMercer'scondition.Whatisnolongerguaranteedisthattheoptimal\nhyperplanemaximizessomemargininahiddenspace.\n"b"CHAPELLEetal.\n:SVM'SFORHISTOGRAM-BASEDIMAGECLASSIFICATION\n1059Fig.2.Corel14:eachrowincludesimagesfromthefollowingsevencategories:Tigers,Africanspecialtyanimals,mountains,deserts,sun-\nrises-sunsets,polarbears.\nB.Experiments\nTheseriesofexperimentsaredesignedtoroughly\nassesstheperformanceoftheaforementionedinputrepresen-\ntationsandSVMkernelsonourtwoCoreltasks.The1400\n\nexamplesof\nCorel14\nweredividedinto924trainingexamples\nand476testexamples.The2670examplesof\nCorel7\nweresplitevenlybetween1375trainingandtestexamples.The\nSVMerrorpenaltyparameter\nwassetto100,whichcanbe\nconsideredinmostcasesaslarge.However,inthisseries\n\nofexperiments,thisparametersettingwasfoundtoenforce\nfullseparabilityforalltypesofkernelsexceptthelinearone.\nInthecasesoftheRBFkernels,the\nvalueswereselected\nheuristically.Morerigorousprocedureswillbedescribedin\nthesecondseriesofexperiments.\nTableIIshowsverysimilarresultsforboththeRBG\nandHSVhistogramrepresentations,andalso,withHSV\n\nhistograms,similarbehaviorsbetween\nCorel14\nandCorel7\n.Theleapinperformancedoesnothappen,asnormally\nexpectedbyusingRBFkernelsbutwiththeproperchoiceof\n\nmetricwithintheRBFkernel.Laplacianor\nRBFkernels\nreducetheGaussianRBFerrorratefromaround30%down\n\nto1520%.\nThisimprovedperformanceisnotonlyduetothechoiceof\ntheappropriatemetric,butalsotothegoodgeneralizationof\n"b"1060IEEETRANSACTIONSONNEURALNETWORKS,VOL.10,NO.5,SEPTEMBER1999\nTABLEI\nHAND-LABELEDCATEGORIES\nUSEDWITHTHE\nCORELD\nATABASE\nTABLEII\nERRORRATES\nUSINGTHE\nFOLLOWINGKERNELS:LINEAR,POLYNOMIALOF\nDEGREE2,G\nAUSSIANRBF,L\nAPLACIANRBFAND2RBFTABLEIII\nERRORRATESWITH\nKNNSVM's.Todemonstratethis,weconductedsomeexperiments\nofimagehistogramwithaK-nearestneighbors\n\n(KNN)algorithmwiththedistances\nand.gavethebestresults.TableIIIpresentstheresults.Asexpected,the\ndistanceisbettersuited;the\n-basedSVMisstillroughly\ntwiceasgoodasthe\n-basedKNN.\nWealsodidsomeexperimentsusingthepixelimageasinput\ntoSVMwith96\n64images.Exceptinthelinear\ncase,theconvergenceofthesupportvectorsearchprocesswas\n\nproblematic,oftenahyperplanewhereeverysampleis\n\nasupportvector.Theerrorrateneverdroppedbelow45%.\nThesamedatabasehasbeenusedby[13]withadecision\ntreeandtheerrorratewasabout50%,similartothe\n\n47.7%errorrateobtainedwiththetraditionalcombinationof\n\nanHSVhistogramandaKNNThe14.7%errorrate\nobtainedwiththeLaplacianor\nRBFrepresentsanearly\nfour-foldreduction.\nOnepartialexplanationforthesuperiorperformanceof\norLaplacianRBFkernelscomesfromthenatureof\nthehistogramrepresentation.Letusstartwithanexample:in\nmanyimages,thelargestcoordinateinthehistogramvector\n\ncorrespondstotheblueofthesky.Asmallshiftinthecolor\n\nofthesky,whichdoesnotaffectthenatureoftheobjectto\nberecognized(forinstanceplaneorbird)resultsintoalarge\ndistance.Supposea\n-pixelbininthehistogramaccountsforasingle\nuniformcolorregionintheimage(withhistogram\n).Asmall\nchangeofcolorinthisregioncanmovethe\npixelstoa\nneighboringbin,resultinginaslightlydifferenthistogram\n.Ifweassumethatthisneighboringbinwasemptyinthe\noriginalhistogram\n,thekernelvaluesare\nThekernelhasalinearexponentialdecayintheLaplacian\nandcases,whileithasaquadraticexponentialdecayin\ntheGaussiancase.\nV.K\nERNELDESIGNVERSUSINPUTREMAPPINGTheexperimentsperformedintheprevioussectionshow\nthatnon-GaussianRBFkernelswithexponentialdecayrates\n\nthatarelessthanquadraticcanleadtoremarkableSVM\nperformancesonimagehistograms.Thissection\nexplorestwowaystoreducethedecayrateofRBFkernels.It\n\nshowsthatoneofthemamountstoasimpleremappingofthe\n\ninput,inwhichcasetheuseofthekerneltrickisnotalways\nnecessary.A.Non-GaussianRBFKernels\nWeintroducekernelsoftheform\nwithThedecayratearoundzeroisgivenby\n.InthecaseofGaussianRBFkernels,\n:decreasingthevalueof\nwouldprovideforaslowerdecay.\nAdata-generatinginterpretationofRBF'sisthattheycorre-\nspondtoamixtureoflocaldensities(generallyGaussian):in\nthiscase,loweringthevalueof\namountstousingheavy-tailed\ndistributions.Suchdistributionshavebeenobservedinspeech\n\nrecognitionandimprovedperformanceshavebeenobtained\n\nbymovingfrom\n(Gaussian)to\n(Laplacian)or\neven(Sublinear)[16].Notethatifweassumethat\nhistogramsareoftendistributedaroundzero(onlyafewbins\n\nhavenonzerovalues),decreasingthevalueof\nshouldhave\nroughlythesameimpactaslowering\n.22AnevenmoregeneraltypeofKernelis\nK(x;y)=ed(x;y)withda;b;c\n(x;y)=ixaiyaibc:Decreasingthevalueof\ncdoesnotimproveperformanceasmuchas\ndecreasingaandb,andincreasesthenumberofsupportvectors.\n"b"CHAPELLEetal.\n:SVM'SFORHISTOGRAM-BASEDIMAGECLASSIFICATION\n1061Fig.3.Corel7:eachrowincludesimagesfromthefollowingcategories:airplanes,birds,boats,buildings,people,cars.\nThechoiceof\nhasnoimpactonMercer'sconditionasit\namountstoachangeofinputvariables.\nMercer'sconditionifandonlyif\n([4]page434).\nB.NonlinearRemappingoftheInput\nTheexponentiationofeachcomponentoftheinputvector\nbydoesnothavetobeinterpretedintermsofkernel\nproducts.Onecanseeitasthesimplestpossiblenonlinear\n\nremappingoftheinputthatdoesnotaffectthedimension.\nThefollowinggivesusreasonstobelievethat\n-exponentiationmayimproverobustnesswithrespectto\nchangesinscale.Imaginethatthehistogramcomponent\niscausedbythepresenceofcolor\ncolinsomeobject.Ifwe\nincreasethesizeoftheobjectbysomescalingfactor\n,the\nnumberofpixelsismultipliedby\n,and\nismultiplied\nbythesamefactor.The\n-exponentiationcouldlowerthis\nquadraticscalingeffecttoamorereasonable\n,with\n.Aninterestingcaseis\n,whichtransformsallthe\ncomponentswhicharenotzerotoone(weassumethat\n).C.ExperimentalSetup\nToavoidacombinatorialexplosionofkernel/remapping\ncombinations,itisimportanttorestrictthenumberofkernels\nwetry.WechosethreetypesofRBFkernels:Gaussian\n,Laplacian\nandSublinear\n.Asa\nbasisforcomparison,wealsokeptthelinearSVM's.\n"b"1062IEEETRANSACTIONSONNEURALNETWORKS,VOL.10,NO.5,SEPTEMBER1999\nForthereasonsstatedinSectionIII.A,theonlyimage\nrepresentationweconsiderhereisthe16\n1616HSV\nhistogram.Oursecondseriesofexperimentsattemptstoarigor-\nousproceduretochoose\nand.Becauseweareonlytesting\nlinearandRBFkernels,wecanreducethesetwochoicesto\n\none,amultiplicativerenormalizationoftheinputdata.\nInthecaseofRBFkernels,weobservedexperimentally\nthatfullseparabilitywasalwaysdesirableonboth\nCorel7\nandCorel14\n.Asaconsequence,\nhastobechosenlargeenough\ncomparedtothediameterofthespherecontaining\ntheinputdata(Thedistancebetween\nandisequalto\n,whichisalways\nsmallerthan2.0).However,RBFkernelsstilldonotspecify\n\nwhatvaluetochoosefor\n.Withproperrenormalizationof\ntheinputdata,wecanset\naswithInthelinearcase,thediameterofthedatadependsonthe\nwayitisnormalized.Thechoiceof\nisequivalenttothe\nchoiceofamultiplicativefactor\nfortheinputdata.\nIf,in(6),wereplace\nwithandwith,(7)\nbecomes(10)Similarexperimentalconditionsareappliedtoboth\nCorel7\nandCorel14\n.Eachcategoryisdividedintothreesets,each\ncontainingonethirdoftheimages,usedastraining,validation\nandtestsets.Foreachvalueoftheinputrenormalization,\n\nsupportvectorsareobtainedfromthetrainingsetandtested\n\nonthevalidationset.The\nrenormalizationforwhichwe\nobtainthebestresultisthenusedtoobtainasetofsupport\nvectorsfromboth,thetrainingandthevalidationsets.Each\n\nCorelimagecontains\nusablepixels:the4096\nhistogramvectorcomponentsrangefrom0to\nandsumupto\n.Theywererenormalizedwith\n101010.Usually,theoptimalvaluesare10\nor1.Nonoptimal\nvaluesincreasetheerrorratebyvaluesrangingfrom0.5%to\n\n5%.Thisverysparsesamplingratewasfoundtobesuf\nforallkernelsexceptGaussianRBF's.Inthelattercase,we\nchose100.0025,0.01,0.04,0.16,1,10\n.Theperformanceresultismeasuredonthetestset.\nToobtainmoretestsamples,weappliedthisprocedurethree\ntimes,eachtimewithadifferenttestset:thenumberoftesting\nsamplesisthetotalnumberofdata(1400for\nCorel14\nand2670\nforCorel7\n).On\nCorel14\n,eachofthethreetrainingsessions\nused933examplesandrequiredbetween52and528support\nvectorstoseparateoneclassfromtheothers.On\nCorel7\n,each\nofthethreetrainingsessionsused1780examplesandrequired\n\nbetween254and1008supportvectorstoseparateoneclass\n\nfromtheothers.Thealgorithmsandthesoftwareusedtotrain\ntheSVM'sweredesignedbyOsuna[17],[18].\nD.ComputationRequirements\nWealsomeasuredthecomputationrequiredtoclassifyone\nimagehistogram.Sincethenumberofcyclesrequiredto\nTABLEIV\nAVERAGEERRORRATESON\nCOREL14.EACHCOLUMNCORRESPONDSTOA\nDIFFERENTKERNEL.THEFIRSTLINEREPORTSTHE\nAVERAGENUMBEROF\nSUPPORT\nVECTORS\nREQUIREDFORTHE\nFULLRECOGNIZER(i.e.,14O\nNEAGAINSTTHE\nOTHERSSVMC\nLASSIFIERS).T\nHENEXTLINESREPORTTHE\nERRORRATES\nUSINGNONLINEARINPUTREMAPPINGS(EXPONENTIATIONBY\na)TABLEV\nAVERAGEERRORRATESON\nCOREL7performanoperationdependsonthemachine,wecountthe\nthreemaintypesofoperationsweinourSVM\nfltbasicpointoperationsuchasthemultiply-add\northecomputationoftheabsolutevalueofthedifference\nbetweentwovectorcomponents.Thisisthecentralopera-\ntionofthekerneldotproduct.Thisoperationcanbeavoided\n\nifbothcomponentsarezero,butweassumethatverifying\n\nthisconditionusuallytakesmoretimethantheoperation\nitself.Thecomputationof\ncanbereducedtoa\nmultiply-addas\nandcanbecomputedinadvance.\nsqrtsquareroot\nexpexponentialExceptinthesublinearRBFcase,thenumberof\nfltisthedominatingfactor.Inthelinearcase,thedecisionfunction\n\n(5)allowsthesupportvectorstobelinearlycombined:there\nisonlyone\nfltperclassandcomponent.IntheRBFcase,\nthereisone\nfltperclass,componentandsupportvector.\nBecauseofthenormalizationby7\n4096,thenumberthat\nappearsonthetableequalsthenumberofsupportvectors.\nFluctuationsofthisnumberaremostlycausedbychangesin\ntheinputnormalization\n.InthesublinearRBFcase,thenumberof\nsqrtisdom-\ninating.sqrtisintheoryrequiredforeachcomponentof\nthekernelproduct:thisisthenumberwereport.Itisa\npessimisticupperboundsincecomputationscanbeavoided\n\nforcomponentswithvaluezero.\nE.Observations\nTheanalysisoftheTablesIVVIshowsthefollowing\ncharacteristicsthatapplyconsistentlytoboth\nCorel14\nandCorel7\n:"b"CHAPELLEetal.\n:SVM'SFORHISTOGRAM-BASEDIMAGECLASSIFICATION\n1063TABLEVI\nCOMPUTATIONAL\nREQUIREMENTSFOR\nCOREL7,REPORTEDASTHE\nNUMBEROF\nOPERATIONSFORTHE\nRECOGNITIONOF\nONEEXAMPLE,DIVIDEDBY\n74096Asanticipated,decreasing\nhasroughlythesameimpact\nasdecreasing\n.(comparecolumn\ntoline\n,onbothTablesIVandV).\nForboth,\nCorel14\nandCorel7\n,thebestperformanceis\nachievedwith\nand.ForhistogramGaussianRBFkernelsare\nhardlybetterthanlinearSVM'sandrequirearoundNSV\n(numberofsupportvectors)timesmorecomputationsat\n\nrecognitiontime.\nSublinearRBFkernelsarenobetterthanLaplacianRBF\nkernels(providedthat\n)andaretoocomputationally\nintensive:atime-consumingsquarerootisrequiredfor\n\nnonzerocomponentsofeverysupportvector.\nForthepracticaluseofRBFkernels,memoryrequire-\nmentsmayalsobeanissue.Afullpointrep-\nresentationof5000supportvectors,eachwith4096\n\ncomponents,requires80Megabytesofmemory.\nReducing\nto0.25makeslinearSVM'saveryattractive\nsolutionformanyapplications:itserrorrateisonly30%\nhigherthanthebestRBF-basedSVM,whileitscompu-\n\ntationalandmemoryrequirementsareseveralordersof\n\nmagnitudesmallerthanforthemostefRBF-based\nSVM.Experimentswith\nyieldsurprisinglygoodresults,\nandshowthatwhatisimportantaboutahistogrambin\nisnotitsvalue,butwhetheritcontainsanypixelatall.\nNotethatinthiscase,Gaussian,Laplacian,andsublinear\n\nRBF'sareexactlyequivalent.\nTheinputspacehas4096dimensions:thisishighenough\ntoenforcefullseparabilityinthelinearcase.However,\nwhenoptimizingfor\nwiththevalidationset,asolution\nwithtrainingwaspreferred(around1%\n\nerroronthecaseof\nCorel14\nand5%errorinthecaseof\nCorel7\n).TableVIIpresentstheclass-confusionmatrixcorresponding\ntotheuseoftheLaplaciankernelon\nCorel7\nwithand(thesevaluesyieldthebestresultsforboth\nCorel7\nandCorel14\n).Themostcommonconfusionshappenbetween\nbirds\nandairplanes,whichisconsistent.\nVI.S\nUMMARY\nInthispaper,wehaveshownthatitispossibletopush\ntheperformanceobtainedonimagehistograms\n\ntosurprisinglyhighlevelswitherrorratesaslowas11%\n\nfortheof14Corelcategoriesand16%fora\nmoregenericsetofobjects.Thisisachievedwithoutanyother\nknowledgeaboutthetaskthanthefactthattheinputissome\n\nsortofcolorhistogramordiscretedensity.\nTABLEVII\nCLASS-CONFUSIONMATRIXFOR\na=0:25ANDb=1:0.FOREXAMPLE,ROW(1)I\nNDICATES\nTHATONTHE\n386I\nMAGESOFTHE\nAIRPLANESCATEGORY\n,341\nHAVE\nBEENCORRECTLY\nCLASSIFIED,22H\nAVE\nBEENCLASSIFIEDIN\nBIRDS,SEVENIN\nBOATS\n,FOURIN\nBUILDINGS,AND12INVEHICLESThisextremelygoodperformanceisduetothesuperior\ngeneralizationabilityofSVM'sinhigh-dimensionalspaces\ntotheuseofheavy-tailedRBF'saskernelsandtononlin-\neartransformationsappliedtothehistogrambinvalues.We\n\nstudiedhowthechoiceofthe\ndistanceusedinaRBF\nkernelaffectsperformanceonhistogramand\nfoundLaplacianRBFkernelstobesuperiortothestandard\nGaussianRBFkernels.Asanonlineartransformationofthe\nbinvalues,weused\n-exponentiationwith\nrangingfrom1\ndownto0.InthecaseofRBFkernels,theloweringof\nandhavesimilareffects,andtheircombinedyieldsthe\nbestperformance.\nTheloweringof\nimprovestheperformanceoflinear\nSVM'stosuchanextentthatitmakesthemavalidalternative\ntoRBFkernels,givingcomparableperformanceforafraction\nofthecomputationalandmemoryrequirements.Thissuggests\nanewstrategyfortheuseofSVM'swhenthedimension\noftheinputspaceisextremelyhigh.Ratherthanintroducing\nkernelsintendedatmakingthisdimensionevenhigher,which\n\nmaynotbeuseful,itisrecommendedtotrynonlinear\ntransformationsoftheinputcomponentsincombinationwith\nlinearSVM's.Thecomputationsmaybeordersofmagnitude\nfasterandtheperformancescomparable.\nThisworkcanbeextendedinseveralways.Higher-level\nspatialfeaturescanbeaddedtothehistogramfeatures.Al-\nlowingforthedetectionofmultipleobjectsinasingleimage\nwouldmakethistechniqueusablefor\nimageretrieval:animagewouldbedescribedbythelistof\nobjectsitcontains.Histogramsareusedtocharacterizeother\ntypesofdatathanimages,andcanbeused,forinstance,\nforfrauddetectionapplications.Itwouldbeinterestingto\ninvestigateifthesametypeofkernelbringsthesamegains\ninperformance.\nREFERENCES[1]W.Niblack,R.Barber,W.Equitz,M.Flickner,D.Glasman,D.\nPetkovic,andP.Yanker,Theqbicproject:Queryingimagebycontent\n"b"1064IEEETRANSACTIONSONNEURALNETWORKS,VOL.10,NO.5,SEPTEMBER1999\nusingcolor,texture,andshape,\nSPIE,vol.1908,pp.173187,Feb.\n1993.[2]M.SwainandD.Ballard,Indexingviacolorhistograms,\nInt.J.\nComput.Vision\n,vol.7,pp.1132,1991.\n[3]V.Vapnik,\nTheNatureofStatisticalLearningTheory\n.NewYork:\nSpringer-Verlag,1995.\n[4]V.Vapnik,\nStatisticalLearningTheory\n.NewYork:Wiley,1998.\n[5]P.BartlettandJ.Shawe-Taylor,Generalizationperformanceofsupport\nvectormachinesandotherpatternin\nAdvancesinKer-\nnelMethodsSupportVectorLearning\n.Cambridge,MA:MITPress,\n1998.[6]M.BazaraaandC.M.Shetty,\nNonlinearProgramming\nNewYork:\nWiley,1979.\n[7]C.CortesandV.Vapnik,Supportvectornetworks,\nMachineLearning\n,vol.20,pp.125,1995.\n[8]B.E.Boser,I.M.Guyon,andV.N.Vapnik,Atrainingalgorithmfor\noptimalmarginin\nProc.5thACMWkshp.Comput.Learning\nTheory,Pittsburgh,PA,July1992,pp.144152.\n[9]J.WestonandC.Watkins,Multiclasssupportvectormachines,Univ.\nLondon,U.K.,Tech.Rep.CSD-TR-98-04,1998.\n[10]M.PontilandA.Verri,Supportvectormachinesfor3-dobject\nrecognition,in\nPatternAnal.MachineIntell.\n,vol.20,June1998.\n[11]V.Blanz,B.Sch\nolkopf,H.B\nulthoff,C.Burges,V.Vapnik,andT.\nVetter,Comparisonofview-basedobjectrecognitionalgorithmsusing\nrealistic3dmodels,in\nNeuralNetworksICANN'96\n,Berlin,\nGermany,1996,pp.251256.\n[12]R.SchapireandY.Singer,Improvedboostingalgorithmsusing\npredictions,in\nProc.1998Wkshp.Comput.Learning\nTheory,1998.\n[13]C.Carson,S.Belongie,H.Greenspan,andJ.Malik,Color-and\ntexture-basedimagessegmentationusingemanditsapplicationtoimage\nqueryingandsubmittedto\nPatternAnal.MachineIntell.\n,1998.[14]B.Sch\nolkopf,K.Sung,C.Burges,F.Girosi,P.Niyogi,T.Poggio,and\nV.Vapnik,ComparingsupportvectormachineswithGaussiankernels\n\ntoradialbasisfunctionMassachusettsInst.Technol.,A.I.\nMemo1599,1996.\n[15]B.SchieleandJ.L.Crowley,Objectrecognitionusingmultidimen-\nsionalreceptivehistograms,in\nECCV'96,4thEuropeanConf.\nComput.Vision,\nvol.I,1996,pp.610619.\n[16]S.BasuandC.A.Micchelli,Parametricdensityestimationforthe\nofacousticfeaturevectorsinspeechrecognition,in\nNonlinearModeling:AdvancedBlack-BoxTechniques\n,J.A.K.Suykens\nandJ.Vandewalle,Eds.Boston,MA:Kluwer,1998.\n[17]E.Osuna,R.Freund,andF.Girosi,Trainingsupportvectormachines:\nAnapplicationtofacedetection,in\nIEEECVPR'97\n,PuertoRico,June\n1719,1997.\n[18]E.Osuna,R.Freund,andF.Girosi,Improvedtrainingalgorithmfor\nsupportvectormachines,in\nIEEENNSP'97\n,AmeliaIsland,FL,Sept.\n2426,1997.\nOlivierChapelle\nreceivedtheB.Sc.degreeincom-\nputersciencefromtheEcoleNormaleSup\nerieuredeLyon,France,in1998.Heiscurrentlystudying\n\ntheM.Sc.degreeincomputervisionattheEcole\nNormaleSuperieuredeCachan,France.\nHehasbeenavisitingscholarintheMOVI\ncomputervisionteamatInria,Grenoble,France,in\n1997andatAT&TResearchLabs,RedBank,NJ,\ninthesummerof1998.Forthelastthreemonths,he\nhasbeenworkingatAT&TResearchLaboratories\nwithV.Vapnikintheofmachinelearning.\nHisresearchinterestsincludelearningtheory,computervision,andsupport\n\nvectormachines.\nPatrickHaffner\nreceivedthebachelor'sdegree\nfromEcolePolytechnique,Paris,France,in\n1987andfromEcoleNationaleSup\nerieuredes\nTelecommunications(ENST),Paris,France,in\n1989.HereceivedthePh.D.degreeinspeech\nandsignalprocessingfromENSTin1994.\nIn1988and1990,heworkedwithA.Waibel\nonthedesignoftheTDNNandtheMS-TDNN\narchitecturesatATR,Japan,andCarnegieMellon\nUniversity,Pittsburgh,PA.From1989to1995,as\naResearchScientistforCNET/France-T\nelecomin\nLannion,France,hedevelopedconnectionistlearningalgorithmsfortelephone\nspeechrecognition.In1995,hejoinedAT&TBellLaboratoriesandworked\nontheapplicationofOpticalCharacterRecognitionandtransducerstothe\nprocessingofdocuments.In1997,hejoinedtheImageProcessing\n\nServicesResearchDepartmentatAT&TLabs-Research.Hisresearchinterests\nincludestatisticalandconnectionistmodelsforsequencerecognition,machine\nlearning,speechandimagerecognition,andinformationtheory.\nVladimirN.Vapnik\n,foraphotographandbiography,seethisissue,p.999.\n"