b'IMAGE DENOISING \nVIA LOSSY COMPRESSION AND WAVELET THRESHOLDING S. Grace Chang Bin Yu2 Martin Vetterli13 Department of Electrical Engineering \nand Computer Sciences University of California, Berkeley, CA 94720, USA 2Department of Statistics University of California, Berkeley, CA 94720, USA \n3DCpartement dElectricitC Ecole Polytechnique FCdCrale de Lausanne, CH-1015 Lausanne, Switzerland grchangQeecs.berkeley.edu, binyuQstat.berkeley.edu, vetterliQde.epfl.ch ABSTRACT Some past work has proposed to use lossy compression to remove noise, \nbased on the rationale that a reasonable \ncom- pression method retains the dominant signal features more than the randomness of the noise. Building on \nthis theme, we explain why compression \n(via coefficient quantization) is appropriate for filtering \nnoise from signal \nby making the connection that quantization of transform coefficients ap- proximates the operation of wavelet thresholding for de- noising. That is, denoising is mainly due to the zero-zone and that \nthe full precision \nof the thresholded coefficients is of secondary importance. The method of quantization is facilitated by a criterion similar \nto Rissanens minimum \nde- scription length \nprinciple. An important \nissue is \nthe thresh- old value \nof the zero-zone (and of wavelet thresholding). For \na natural image, it has been observed that its subband \nco- efficients can be well modeled by a Laplacian distribution. With this assumption, \nwe derive a threshold which is easy \nto compute and is intuitive. Experiments \nshow that the proposed threshold \nperforms close to optimal thresholding. 1. INTRODUCTION Suppose an image has been corrupted by additive noise, and the goal is \nto remove the noise. The idea of using a lossy compression algorithm to denoise the signal has been proposed in several works [4, \n71. Continuing on this theme, \none main \npurpose of this paper \nis to explain why lossy com- \npression can be \nappropriate for noise filtering. More \nspecif- ically, we wish to show that quantization (a common step in compression) of transform coefficients achieves denoising \nby posing \nquantization as an approximation to an effective denoising method called wavelet thresholding \n[a]. The theoretical formalization of thresholding in \nthe con- text of removing noise via \nthresholding wavelet coefficients \nwas pioneered by Donoho and Johnstone [a]. Both the \nsoft- (shrink or kill) and the hard- (keep or kill) thresholding methods compare the input \nto a given threshold and set it to \nzero if its magnitude is less than the threshold. The idea is that coefficients insignificant \nrelative to the thresh- old are likely due to noise, whereas significant coefficients \nare important signal structures. Thresholding essentially creates a region around zero where the coefficients are con- sidered negligible. Outside of this region, the thresholded coefficients are kept to full precision. \nAnalogously, in a typical transform domain \nlossy com- \npression method, negligible coefficients \nare set to zero, cre- ating what is called a zero-zone or dead-zone, and co- efficients outside of this zone are quantized. Our hypothesis is that an appropriate quantization scheme (and hence com- pression) achieves denoising \nbecause it is an approximation to the thresholding operation (see Figure 1). Furthermore, the effectiveness of denoising is mainly \ndue to the \nzero-zone, and the full precision \nof the thresholded coefficients is of sec- 0-8186-8183-7/97 $10.00 @ 1997 IEEE I I Figure 1. The thresholding function \ncan be approx- imated by quantization with a zero-zone. ondary importance. \nThus, a comparable level of denoising performance can be achieved by quantizing the coefficients with a zero-zone and a few number of quantization levels outside of the zero-zone. The manner of quantization will be facilitated by a criterion similar to Rissanens minimum \ndescription length (MDL) [5]. One of the most important and frequently asked \nques- tions in using wavelet thresholding is What is the thresh- old?, or in the compression scenario, \nhow to choose the zero-zone? While Donoho and Johnstone [a] have pro- posed several \nthresholds such as the \nuniversal (ad-), SURE, and hybrid thresholds, \nand have demonstrated their \nasymptotic optimality \nconditions, these thresholds \ndo not work well in practice. \nThis is particularly true for images, and it \nis also \nrather counter-intuitive in \nsignal processing \napplications to have the threshold values dependent on the sample size n. There are also many works of threshold- ing/shrinkage based on standard statistics techniques (e.g. \nBayesian, cross-validation), \nbut most of them are not suit- able for images and some are rather \ncomputationally in- tensive. Here \nwe propose a threshold \nfor soft-thresholding images which is simple \nand straighforward to compute. A large class of natural images has decaying spectrums, which means that the subband energy also follows \na cer- tain decay across \nscales. Within each subband, the coeffi- cients can be well modeled by a generalized Laplacian dis- tribution [3]. Assuming a Laplacian \ndistribution, the pro- posed threshold is an approximation to the \noptimal thresh- \nold which minimizes \nthe expected squared error among \nsoft- threshold estimators. A different threshold is \ncomputed for each subband to adapt to the changing subband character- istics. 2. WAVELET THRESHOLDING AND Definitions and Notations For simpler \nnotations, vari- ables are referenced by a single index even though we are working in the 2-D domain. Suppose the signal x = {st, i = 1,. . . , n} has been cor- rupted and \none observes \ny = {yt = x, + ut, i = 1,. . . \n, n} where ut is iid N(0, c) and independent of xc. Let Y = Wy denote the vector of wavelet coefficients \nof y, where W is THRESHOLD SELECTION 604 'b'the orthogonal wavelet transform operator, and \nsimilarly with X and V. The readers are referred to [3] for details on wavelet subband decomposition. Since the transform is orthogonal, V, is also iid Gaussian N(0, r2). The idea of wavelet thresholding suggests \nfilter- ing noise from \ny by thresholding its wavelet coefficients \n(ex- cept the coarsest scale \ncoefficients . The soft-thresholding function, defined as ox(t) = sgn ? t)(ltl - A)+, where A is the threshold, is used here because \nit generally yields \nvisu- ally more pleasing images over \nhard-thresholding, defined as t)x(t) = t. lt>x. The denoised estimate j, is then the \ninverse transform of qx(Y), B = W-\'rp Y). estimate of X to be in the class of soft-threshold estimates, X = qx(Y), the goal is to derive a threshold \nX which mini- \nmizes the averaged squared error, \n1/N ci(Xz - Xt)\'- For a large class of natural images, it has been observed that the coefficients in each subband of its wavelet trans- form (with the exception of the lowest scale) can \nbe well described by a generalized Laplacian distribution [3]. For this work, we assume the Laplacian pdf \nfor simplicity. Consider now only coefficients from one subband. Let X - LAP(&) = $6 e-&lx1. For a large number of coefficients, 1/N x,(Xc - X,)2 N E(2 - X)\'. Thus, we proceed to minimize E(x - X)\'. It can be shown that the Laplacian pdf is a scaled mixture of normals, and that \nthe denoising problem can be reformulated with \nthe fol- lowing priors: YIX N N(X,g2), Xlu: N N(O,a;), and u: N EXP(a) = a e-ao:, U: 2 0. Breaking the problem into three \npriors gives implemen- \ntation advantages because \nit requires numerical \nintegration of only one variable. That is, Derivation of Threshold Suppose t \n6 at we restrict the E(2 - X)" = I" J_, J__(?.(Vi - w2 p( Y IX\')p(X 10;) p( U:) dY dX da2 where "- \' Without loss of generality, assume = 1. The optimal threshold for each a is X*(a) = argminx?og(a,X). The \ncurve of A*(&) is plotted against 1/fi on the x-axis in \nFigure 2 (a). This curve is compared with i(a) .= 6 which is seen to approximate closely X*(a), and yet IS sim- ple and does not need to be calculated through numerical minimization, Their corresponding expected \nsquared errors are shown in Figure 2 (b), and the \ndifference is within 1%. The choice of the threshold A(,) = fi also makes \nintu- itive sense. For \nX N LAP(G), its standard of deviation is Std X) = l/fi, and is inversely proportional to much larger than 1 (recall Std(X\\. Thus, \nwhen that c = l), the signal is much stronger than the noise, thus the. threshold \nis chosen to be small to preserve most of the signal and remove some \nof the noise; vice versa, when Std(X) is small relative \nto 1, the noise dominates and the \nthreshold is large to remove the noise which \nhas Figure 2. (a) Comparing the optimal threshold A*(cy) (solid -) and the threshold i(a) (dotted ...) against l/& on the x-axis. (b) Their corresponding MSEs, with (-) for X*(cy) and (...) for x(a). overwhelmed the signal. It is also interesting to note that if us were treated as deterministic (that is, X - N(0,u:) and gz is to be estimated), then the \nthreshold X(a,) = l/uz also approximates the corresponding optimal threshold \nvery well. Now for a general \nvalue of U, the above discussion \nholds by replacing A, CY, and rl? by X/u, U\'&, and br/u, respectively, and our proposed threshold \nis A similar threshold to (1) is found independently in \n[6] for using the same priors with \nthe hard-thresholding function. \nSoft-thresholding is used here because \nit is more suitable for images. Furthermore, with these priors, \nthe expected aquared error of optimal soft-thresholding is smaller than that of optimal hard-thresholding. There are two parameters to be estimated: the noise vari- ance U\' and the hyperparameter, a, of the Laplacian pdf. The noise variance \nis estimated by the robust median \nesti- mator in the highest subband, & = Median(lY,1)/.6745, Y, E subband HHI, also used in [2]. Since the signal and noise are assumed to be independent, Var(Y) \n= l/a + U\', thus 6 = SampleVariance(Y) - 5\'. In the rare case that C2 > SampleVariance(Y), the threshold is set to the maxi- mum value \nof that subband, X = max IYI; that is, all coef- ficients are set \nto 0. The above method for estimating & and A(&) are done for each subband independently to adapt to the different characteristics. 3. QUANTIZATION WITH MDL CRITERION In the original thresholding scheme, \nthe thresholded coeffi- cients are then inverse transformed back to yield the esti- mate 2. In this work, to show that quantization approxi- mates thresholding, there is an additional step of quantizing the thresholded coefficients before inverse \ntransform. Consider again only one particular subband \nof the wavelet i(&) have been calculated. After thresholding, \nthere are the questions of how many quantization levels and what type of transform, and that the parameters 8, & and the threshold 605 'b'reconstruction values (e.g. \nuniform, centroid). We propose to use an MDL-like criterion to facilitate this decision. For a given set of observations, the MDL criterion \nis use- ful for \nchoosing a reasonable statistical model \nwhich yields \nthe shortest description length [5]. It does this by choosing \nthe model which minimizes \nthe total code-length of a two- part encoding consisting \nof the data (based on the \nchosen model) and of the model parameters. The idea is that the chosen model should establish \na compromise between fit- ting the data well and having low complexity (i.e. having a simple representation or a reasonable number of parame- ters). More specifically, given \nthe set of noisy transform coeffi- cients Y = X + V, the framework is \nto code Y given the model X. The MDL principle chooses \nX which minimizes \nthe two-part code-length Ltotal(Y,X) = L(YIX) + L(X), where L(YIX) is the code-length for \nY based on X, and L(X) is the code-length for \nX. If it is possible to associate a probability distribution \np(.), then one can use the ideal- ized code-length - log, p. In this case where \nthe noise V, is iid N(0, m), the first term becomes The second term in (2) is a \nconstant and irrelevant in the minimization, and thus only the first term is considered. \nIn Saitos simultaneous compression \nand denoising method [7] of combining MDL with thresholding, \nthe hard- thresholding function \nwas used and the term L(X) was taken to be (3/2)k log, n, where k is the number of nonzero coefficients: log, n bits to indicate the location of each nonzero coefficient (assuming an uniform indexing) \nand (1/2) log, n bits to represent its value. Although compres- sion has been achieved in the sense that a fewer number of nonzero coefficients are kept, \nit still does \nnot address the issue that in a practical setting, the \ncoefficients are usually quantized. Thus, our criterion is developed \nfrom a cod- ing point of view, and the minimization of LtoJal(Y, X) is restricted to X belonging to the set of quantized signals, whose construction will become clear in the following text. After the zero-zone has been determined (by the thresh- old), there are k nonzero coefficients and n - IC zero coef- ficients to be quantized \nand coded. For a \ngiven threshold, the value k is fixed and so is the bitrate for coding the lo- cations of zero coefficients (e.g. a naive \nway is to use log, n bits to index each of the n - k nonzero coefficients or, more realistically, to use runlength encoding). Thus, this term is again neglected in the minimization. For the nonzero coefficients, on each positive \nand nega- tive side, \nm equal-width bins are partitioned between 0 and the maximum absolute \ncoefficient value, \nmax IYI. The re- construction values are taken to be the centroid of each bin, assuming a Laplacian distribution, \nLAP(a), with the pa- rameter estimate & as described in Section 2. Let 7 = &, the closed-form expression \nfor the centroid value of a bin on the positive side with \nboundaries t* and t.+l is Jt:+l ZIJ(~)C~~ 1 tie-7 - tisle --rt.+1 st;+, P(Z)dZ 7 rl = =-+ e-?tt - e-Ytz+l  The negative side \nis similar. \nThe indices of ti are i E (0, fl, ..., +m} and the indices of ri are i E {hi, ..., rtm} for the positive and negative sides. To code the quantized value, first one needs \nto transmit the range max IY I and the value of m. These are fixed over- \nhead and will be ignored in the minimization. Then entropy encoding is typically used \nto transmit the bin index of each coefficient. A good estimate of the bitrate \nfor the k nonzero coefficients is \nthe entropy H(m) = -E, lc,log2(lc,/k), where IC,, i E {fl, ..., hm}, is the number of coefficients with reconstruction value r,. Let Q[z, m] denote the operation of quantizing the input \nx with 2m + 1 levels (including the zero-zone) as described above, and let Xy = Q[qx(K),m] denote the estimate of X, obtained by soft-thresholding followed by \nquantization. We choose the estimate \n2Q with the associated m which minimizes the criterion II 1 2(1n2)a2 MDLQ = - z(x - 2:) + H(m) (3) t=1 The space-domain estimate is taken to be the inverse trans- form of XQ. Note that the quantized estimate naturally could do worse than the unquantized threshold estimate. \nHowever, it is not our goal to achieve better than the unquantized estimate, but rather to \nestablish a connection between \ncom- pression (via quantization) \nand thresholding to show that lossy compression can be a good method for noise removal. \nThis thresholding-quantization scheme is applied to each subband independently. First \nthe noise variance \na2 is esti- mated. Then for each subband, the parameter \nti and the threshold x(&) are calculated, and 3) is minimized \nto find LL is not thresholded but is quantized using (3) with the uniform distribution. the desired quantized coefficients. 4 he coarsest component \n4. EXPERIMENTAL RESULTS The images goldhill and lena , with various noise \nstrength r 5 10,15,20, are used as test data. The cho- sen wavelet is Daubechies least asymmetric \ncompactly- supported wavelet with 8 vanishing moments [1]; and 4 levels of decomposition is \nused. Table 1 compares the SNRs of the soft-thresholding denoised results using the oracle threshold, lor-, and i(&) (adaptive in \neach sub- \nband). For each subband, the oracle threshold \nis found by Xorc = argminx ~,(qx(yZ) - X,)2 assuming X, is known. The SNRs resulting from using i(&) are very close to those of Xorc, indicating that the Laplacian pdf is \na good assump- tion and that the estimation of the parameters \nis appropri- ate. Visually, the two sets of images are also very \nsimilar (see Figure 3 (b) and \n(c)). The last column in Table \n1 shows the SNR of the quan- tized signal using \ni(&) as the zero-zone threshold. The quantized goldhill image with \n= 15 is shown \nin Figure 3(d), where the quantization noise is \nquite visible. As ex- pected, the quantized signal uses \nless bits, but at the ex- pense of some degradation. On the average, the quantized signal loses \nabout 1-1.5 dB over the unquantized thresh- \nolded signal, although it \nstill has a much higher SNR than responsible for filtering \nthe noise. One thing \nto note is that the zeroth-order entropy \nestimate, H(m), for the bitrate of the nonzero coefficients is \na rather loose estimate. With more sophisticated coding, the same bitrate could yield \na higher number of quantization level m, thus resulting in a better SNR. Table 2 shows the values of m chosen for each subband of the goldhill image, U = 15, averaged over 5 runs. The results show that the MDLQ criterion allocates more \nlevels in the coarser, more \nimportant levels, as would be the case in a practical subband coding situation. the noisy image. \nThis suggests that \nmainly the zero-zone is 606 'b'I SNR 11 noisy I Xorc 1 X I A, Q[.,m] 1 Orientation Scale coarse 7iH 0 2.0 3.6 \n6.2 Table 1. SNRs (in dB) of (1) the noisy image, (2) oracle soft-thresholding, (3) soft-thresholding with thresholds 1, and (4) quantized signal with zero- zone thresholds \n1. Averaged over 5 runs. 2.6 4.06.0 18.6 LH-2.8 3166.4 12.2 LL 39.0 5. DISCUSSION AND CONCLUSION We have addressed two main issues regarding image denois- \ning in the paper. We demonstrated the connection between lossy compression and wavelet thresholding to explain why compression is suitable for denoising. Specifically, it is \nthe zero-zone in coefficient quantization that is the main agent \nin removing the noise. A suitable threshold for images has also been proposed \nfor wavelet thresholding and for the quantization zero-zone. Results suggest that the proposed method may be appropriate for subband coding, in \nthe de- cision of bit allocation and the \nmanner of quantization. For future work, it would be interesting to jointly compute the best aero-zone threshold and quantization bins rather than compute them separately. REFERENCES [l] I. Daubechies, Ten Lectures on \nWavelets, vol. 61 of CBMS-NSF Regional Conference Series in Applied Mathematics, SIAM, Philadelphia, 1992. 121 D.L. Donoho \nand I.M. Johnstone. Ideal sDatial adaD- L1 tation via wavelet \nshrinkage, Bzornetrika; vol 81, pp. 425-455, 1994. \n[3] S. Mallat, A theory for multiresolution signal de- \ncomposition: The wavelet representation, IEEE Pat. Anal. Mach. Intell., vol. 11, no. 7, pp. 674-693, July 1989. [4] B.K. Natarajan, Filtering random noise from \ndeter- ministic signals via \ndata compression, IEEE Trans. on Signal Processing, \nvol. 43, no. 11, pp. 2595-2605, November 1995. [5] J. Rissanen, Stochastic Complexity in Statistical In- quiry, World Scientific, 1989. [6] F. Ruggeri and B. Vidakovic, A Bayesian decision \ntheoretic approach \nto wavelet thresholding, preprint, Duke University, \nDurham, NC. (ftp://ftp.isds.duke. edu/pub/Users/brani/papers/Decision.ps) . (4 [7] N. Saito, Simultaneous noise suppression \nand signal compression using a library of orthonormal bases and the minimum description length criterion, in \nWavelets thresholding. (c) Threshold with A(&). (d). Our in Geophysics, Eds. E. Foufoula-Georgiou and P. Ku- mar, pp. \n299-324, Academic Press, 1994. Figure 3. (a) Noisy \nimage, D = 15. (b) Oracle soft- method of thresholding followed by quantization. 607 '