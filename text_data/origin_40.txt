b'MatConvNet\nConvolutionalNeuralNetworksforMATLAB\nAndreaVedaldiKarelLenc\ni\narXiv:1412.4564v3  [cs.CV]  5 May 2016'b'ii\nAbstract\nMatConvNet\nisanimplementationofConvolutionalNeuralNetworks(CNNs)\nforMATLAB.Thetoolboxisdesignedwithanemphasisonsimplicityandy.\nItexposesthebuildingblocksofCNNsaseasy-to-useMATLABfunctions,providing\nroutinesforcomputinglinearconvolutionswithbanks,featurepooling,andmany\nmore.Inthismanner,\nMatConvNet\nallowsfastprototypingofnewCNNarchitec-\ntures;atthesametime,itsupportstcomputationonCPUandGPUallowing\ntotraincomplexmodelsonlargedatasetssuchasImageNetILSVRC.Thisdocument\nprovidesanoverviewofCNNsandhowtheyareimplementedin\nMatConvNet\nand\ngivesthetechnicaldetailsofeachcomputationalblockinthetoolbox.\n'b'Contents\n1IntroductiontoMatConvNet\n1\n1.1Gettingstarted..................................\n2\n1.2\nMatConvNet\nataglance...........................\n4\n1.3Documentationandexamples..........................\n5\n1.4Speed.......................................\n6\n1.5Acknowledgments.................................\n7\n2NeuralNetworkComputations\n9\n2.1Overview......................................\n9\n2.2Networkstructures................................\n10\n2.2.1Sequences.................................\n10\n2.2.2Directedacyclicgraphs..........................\n11\n2.3Computingderivativeswithbackpropagation..................\n12\n2.3.1Derivativesoftensorfunctions......................\n12\n2.3.2Derivativesoffunctioncompositions..................\n13\n2.3.3Backpropagationnetworks........................\n14\n2.3.4BackpropagationinDAGs........................\n15\n2.3.5DAGbackpropagationnetworks.....................\n18\n3Wrappersandpre-trainedmodels\n21\n3.1Wrappers.....................................\n21\n3.1.1SimpleNN.................................\n21\n3.1.2DagNN..................................\n21\n3.2Pre-trainedmodels................................\n22\n3.3Learningmodels..................................\n23\n3.4Runninglargescaleexperiments.........................\n23\n4Computationalblocks\n25\n4.1Convolution....................................\n25\n4.2Convolutiontranspose(deconvolution).....................\n27\n4.3Spatialpooling..................................\n29\n4.4Activationfunctions...............................\n29\n4.5Spatialbilinearresampling............................\n30\n4.6Normalization...................................\n30\n4.6.1Localresponsenormalization(LRN)..................\n30\niii\n'b'iv\nCONTENTS\n4.6.2Batchnormalization...........................\n30\n4.6.3Spatialnormalization...........................\n31\n4.6.4Softmax..................................\n31\n4.7Categoricallosses.................................\n31\n4.7.1losses............................\n32\n4.7.2Attributelosses..............................\n33\n4.8Comparisons....................................\n34\n4.8.1\np\n-distance.................................\n34\n5Geometry\n37\n5.1Preliminaries...................................\n37\n5.2Simple...................................\n38\n5.2.1Poolingin..............................\n38\n5.3Convolutiontranspose..............................\n40\n5.4Transposingreceptive...........................\n41\n5.5Composingreceptive............................\n42\n5.6Overlayingreceptive............................\n42\n6Implementationdetails\n43\n6.1Convolution....................................\n43\n6.2Convolutiontranspose..............................\n44\n6.3Spatialpooling..................................\n45\n6.4Activationfunctions...............................\n45\n6.4.1ReLU...................................\n45\n6.4.2Sigmoid..................................\n46\n6.5Spatialbilinearresampling............................\n46\n6.6Normalization...................................\n46\n6.6.1Localresponsenormalization(LRN)..................\n46\n6.6.2Batchnormalization...........................\n47\n6.6.3Spatialnormalization...........................\n48\n6.6.4Softmax..................................\n48\n6.7Categoricallosses.................................\n49\n6.7.1losses............................\n49\n6.7.2Attributelosses..............................\n49\n6.8Comparisons....................................\n50\n6.8.1\np\n-distance.................................\n50\nBibliography\n51\n'b'Chapter1\nIntroductiontoMatConvNet\nMatConvNet\nisaMATLABtoolboximplementing\nConvolutionalNeuralNetworks\n(CNN)\nforcomputervisionapplications.Sincethebreakthroughworkof[\n7\n],CNNshavehada\nmajorimpactincomputervision,andimageunderstandinginparticular,essentiallyreplacing\ntraditionalimagerepresentationssuchastheonesimplementedinourownVLFeat[\n11\n]open\nsourcelibrary.\nWhilemostCNNsareobtainedbycomposingsimplelinearandnon-linearop-\nerationssuchasconvolutionandtheirimplementationisfarfromtrivial.The\nreasonisthatCNNsneedtobelearnedfromvastamountsofdata,oftenmillionsofimages,\nrequiringveryientimplementations.AsmostCNNlibraries,\nMatConvNet\nachieves\nthisbyusingavarietyofoptimizationsand,c,bysupportingcomputationsonGPUs.\nNumerousothermachinelearning,deeplearning,andCNNopensourcelibrariesexist.\nTocitesomeofthemostpopularones:CudaConvNet,\n1\nTorch,\n2\nTheano,\n3\nand\n4\n.Many\noftheselibrariesarewellsupported,withdozensofactivecontributorsandlargeuserbases.\nTherefore,whycreatingyetanotherlibrary?\nThekeymotivationfordeveloping\nMatConvNet\nwastoprovideanenvironmentpar-\nticularlyfriendlyandtforresearcherstouseintheirinvestigations.\n5\nMatConvNet\nachievesthisbyitsdeepintegrationintheMATLABenvironment,whichisoneofthemost\npopulardevelopmentenvironmentsincomputervisionresearchaswellasinmanyotherareas.\nInparticular,\nMatConvNet\nexposesassimpleMATLABcommandsCNNbuildingblocks\nsuchasconvolution,normalisationandpooling(chapter\n4\n);thesecanthenbecombinedand\nextendedwitheasetocreateCNNarchitectures.Whilemanyofsuchblocksuseoptimised\nCPUandGPUimplementationswritteninC++andCUDA(sectionsection\n1.4\n),MATLAB\nnativesupportforGPUcomputationmeansthatitisoftenpossibletowritenewblocks\ninMATLABdirectlywhilemaintainingcomputational.Comparedtowritingnew\nCNNcomponentsusinglowerlevellanguages,thisisanimportantthatcan\ntlyacceleratetestingnewideas.UsingMATLABalsoprovidesabridgetowards\n1\nhttps://code.google.com/p/cuda-convnet/\n2\nhttp://cilvr.nyu.edu/doku.php?id=code:start\n3\nhttp://deeplearning.net/software/theano/\n4\nhttp://caffe.berkeleyvision.org\n5\nWhilefromauserperspective\nMatConvNet\ncurrentlyreliesonMATLAB,thelibraryisbeingdevel-\nopedwithacleanseparationbetweenMATLABcodeandtheC++andCUDAcore;therefore,inthefuture\nthelibrarymaybeextendedtoallowprocessingconvolutionalnetworksindependentlyofMATLAB.\n1\n'b'2\nCHAPTER1.INTRODUCTIONTOMATCONVNET\notherareas;forinstance,\nMatConvNet\nwasrecentlyusedbytheUniversityofArizonain\nplanetaryscience,assummarisedinthisNVIDIAblogpost.\n6\nMatConvNet\ncanlearnlargeCNNmodelssuchAlexNet[\n7\n]andtheverydeepnet-\nworksof[\n9\n]frommillionsofimages.Pre-trainedversionsofseveralofthesepowerfulmodels\ncanbedownloadedfromthe\nMatConvNet\nhomepage\n7\n.Whilepowerful,\nMatConvNet\nremainssimpletouseandinstall.Theimplementationisfullyself-contained,requiringonly\nMATLABandacompatibleC++compiler(usingtheGPUcoderequiresthefreely-available\nCUDADevKitandasuitableNVIDIAGPU).Asdemonstratedin\n1.1\nandsection\n1.1\n,\nitispossibletodownload,compile,andinstall\nMatConvNet\nusingthreeMATLABcom-\nmands.Severalfully-functionalexamplesdemonstratinghowsmallandlargenetworkscan\nbelearnedareincluded.Importantly,several\nstandardpre-trainednetwork\ncanbeimmedi-\natelydownloadedandusedinapplications.Amanualwithacompletetechnicaldescription\nofthetoolboxismaintainedalongwiththetoolbox.\n8\nThesefeaturesmake\nMatConvNet\nusefulinaneducationalcontexttoo.\n9\nMatConvNet\nisopen-sourcereleasedunderaBSD-likelicense.Itcanbedownloaded\nfrom\nhttp://www.vlfeat.org/matconvnet\naswellasfromGitHub.\n10\n.\n1.1Gettingstarted\nMatConvNet\nissimpletoinstallanduse.\n1.1\nprovidesacompleteexamplethatclas-\nanimageusingalatest-generationdeepconvolutionalneuralnetwork.Theexample\nincludesdownloadingMatConvNet,compilingthepackage,downloadingapre-trainedCNN\nmodel,andevaluatingthelatterononeof\nMATLAB\n\'sstockimages.\nThekeycommandinthisexampleis\nvl_simplenn\n,awrapperthattakesasinputthe\nCNN\nnet\nandthepre-processedimage\nim_\nandproducesasoutputastructure\nres\nofresults.\nThisparticularwrappercanbeusedtomodelnetworksthathaveasimplestructure,namely\na\nchain\nofoperations.Examiningthecodeof\nvl_simplenn\n(\neditvl_simplenn\nin\nMatCon-\nvNet\n)wenotethatthewrappertransformsthedatasequentially,applyinganumberof\nMATLAB\nfunctionsasspbythenetworkThesefunction,discussedin\ndetailinchapter\n4\n,arecalled\\buildingblocks"andconstitutethebackboneof\nMatCon-\nvNet\n.\nWhilemostblocksimplementsimpleoperations,whatmakesthemnontrivialistheir\n(section\n1.4\n)aswellassupportforbackpropagation(section\n2.3\n)toallowlearning\nCNNs.Next,wedemonstratehowtouseoneofsuchbuildingblocksdirectly.Forthesakeof\ntheexample,considerconvolvinganimagewithabankoflinearStartbyreadingan\nimagein\nMATLAB\n,sayusing\nim\n=\nsingle\n(\nimread\n(\n\'\npeppers\n.\npng\n\'\n))\n,obtaininga\nH\n\nW\n\nD\narray\nim\n,where\nD\n=3isthenumberofcolourchannelsintheimage.Thencreateabank\nof\nK\n=16randomofsize3\n\n3using\nf\n=\nrandn\n(3,3,3,16,\n\'\nsingle\n\'\n)\n.Finally,convolvethe\n6\nhttp://devblogs.nvidia.com/parallelforall/deep-learning-image-understanding-planetary-science/\n7\nhttp://www.vlfeat.org/matconvnet/\n8\nhttp://www.vlfeat.org/matconvnet/matconvnet-manual.pdf\n9\nAnexamplelaboratoryexperiencebasedon\nMatConvNet\ncanbedownloadedfrom\nhttp://www.\nrobots.ox.ac.uk/\n~\nvgg/practicals/cnn/index.html\n.\n10\nhttp://www.github.com/matconvnet\n'b"1.1.GETTINGSTARTED\n3\n%\ninstall\nand\ncompile\nMatConvNet\n(\nrun\nonce\n)\nuntar\n([\n'\nhttp\n://\nwww\n.\nvlfeat\n.\norg\n/\nmatconvnet\n/\ndownload\n/\n'\n...\n'\nmatconvnet\n\n1.0\n\nbeta12\n.\ntar\n.\ngz\n'\n]);\ncd\nmatconvnet\n\n1.0\n\nbeta12\nrunmatlab\n/\nvl_compilenn\n%\ndownload\na\npre\n\ntrained\nCNN\nfrom\nthe\nweb\n(\nrun\nonce\n)\nurlwrite\n(...\n'\nhttp\n://\nwww\n.\nvlfeat\n.\norg\n/\nmatconvnet\n/\nmodels\n/\nimagenet\n\nvgg\n\nf\n.\nmat\n'\n,...\n'\nimagenet\n\nvgg\n\nf\n.\nmat\n'\n);\n%\nsetup\nMatConvNet\nrunmatlab\n/\nvl_setupnn\n%\nload\nthe\npre\n\ntrained\nCNN\nnet\n=\nload\n(\n'\nimagenet\n\nvgg\n\nf\n.\nmat\n'\n);\n%\nload\nand\npreprocess\nan\nimage\nim\n=\nimread\n(\n'\npeppers\n.\npng\n'\n);\nim_\n=\nimresize\n(\nsingle\n(\nim\n),\nnet\n.\nmeta\n.\nnormalization\n.\nimageSize\n(1:2));\nim_\n=\nim_\n\nnet\n.\nmeta\n.\nnormalization\n.\naverageImage\n;\n%\nrun\nthe\nCNN\nres\n=\nvl_simplenn\n(\nnet\n,\nim_\n);\n%\nshow\nthe\nation\nresult\nscores\n=\nsqueeze\n(\ngather\n(\nres\n(\nend\n).\nx\n));\n[\nbestScore\n,\nbest\n]=\nmax\n(\nscores\n);\n\n(1);\nclf\n;\nimagesc\n(\nim\n);\ntitle\n(\nsprintf\n(\n'\n%\ns\n(%\nd\n),\nscore\n%.3\nf\n'\n,...\nnet\n.\nclasses\n.\ndescription\nf\nbest\ng\n,\nbest\n,\nbestScore\n));\nFigure1.1:Acompleteexampleincludingdownload,installing,compilingandrunning\nMat-\nConvNet\ntoclassifyoneof\nMATLAB\nstockimagesusingalargeCNNpre-trainedon\nImageNet.\n"b'4\nCHAPTER1.INTRODUCTIONTOMATCONVNET\nimagewiththebyusingthecommand\ny\n=\nvl_nnconv\n(\nx\n,\nf\n,[])\n.Thisresultsinanarray\ny\nwith\nK\nchannels,oneforeachofthe\nK\ninthebank.\nWhileusersareencouragedtomakeuseoftheblocksdirectlytocreatenewarchitectures,\nMATLAB\nprovideswrapperssuchas\nvl_simplenn\nforstandardCNNarchitecturessuchas\nAlexNet[\n7\n]orNetwork-in-Network[\n8\n].Furthermore,thelibraryprovidesnumerousexamples\n(inthe\nexamples\n/\nsubdirectory),includingcodetolearnavarietyofmodelsontheMNIST,\nCIFAR,andImageNetdatasets.Alltheseexamplesusethe\nexamples\n/\ncnn_train\ntraining\ncode,whichisanimplementationofstochasticgradientdescent(section\n3.3\n).Whilethis\ntrainingcodeisperfectlyserviceableandquiteitremainsinthe\nexamples\n/\nsubdirec-\ntoryasitissomewhatproblem-spUsersarewelcometoimplementtheiroptimisers.\n1.2MatConvNetataglance\nMatConvNet\nhasasimpledesignphilosophy.RatherthanwrappingCNNsaroundcomplex\nlayersofsoftware,itexposessimplefunctionstocomputeCNNbuildingblocks,suchaslinear\nconvolutionandReLUoperators,directlyasMATLABcommands.Thesebuildingblocksare\neasytocombineintocompleteCNNsandcanbeusedtoimplementsophisticatedlearning\nalgorithms.Whileseveralreal-worldexamplesofsmallandlargeCNNarchitecturesand\ntrainingroutinesareprovided,itisalwayspossibletogobacktothebasicsandbuildyour\nown,usingtheofMATLABinprototyping.OftennoCcodingisrequiredatall\ntotrynewarchitectures.Assuch,\nMatConvNet\nisanidealplaygroundforresearchin\ncomputervisionandCNNs.\nMatConvNet\ncontainsthefollowingelements:\n\nCNNcomputationalblocks.\nAsetofoptimizedroutinescomputingfundamental\nbuildingblocksofaCNN.Forexample,aconvolutionblockisimplementedby\ny\n=\nvl_nnconv\n(\nx\n,\nf\n,\nb\n)\nwhere\nx\nisanimage,\nf\nabank,and\nb\navectorofbiases(sec-\ntion\n4.1\n).Thederivativesarecomputedas\n[\ndzdx\n,\ndzdf\n,\ndzdb\n]=\nvl_nnconv\n(\nx\n,\nf\n,\nb\n,\ndzdy\n)\nwhere\ndzdy\nisthederivativeoftheCNNoutputw.r.t\ny\n(section\n4.1\n).chapter\n4\nde-\nscribesalltheblocksindetail.\n\nCNNwrappers.\nMatConvNet\nprovidesasimplewrapper,suitablyinvokedby\nvl_simplenn\n,thatimplementsaCNNwithalineartopology(achainofblocks).Italso\nprovidesamuchmorewrappersupportingnetworkswitharbitrarytopologies,\nencapsulatedinthe\ndagnn\n.\nDagNN\nMATLABclass.\n\nExampleapplications.\nMatConvNet\nprovidesseveralexamplesoflearningCNNswith\nstochasticgradientdescentandCPUorGPU,onMNIST,CIFAR10,andImageNet\ndata.\n\nPre-trainedmodels.\nMatConvNet\nprovidesseveralstate-of-the-artpre-trainedCNN\nmodelsthatcanbeusedeithertoclassifyimagesortoproduceimage\nencodingsinthespiritoforDeCAF.\n'b'1.3.DOCUMENTATIONANDEXAMPLES\n5\nFigure1.2:TrainingAlexNetonImageNetILSVRC:dropoutvsbatchnormalisation.\n1.3Documentationandexamples\nTherearethreemainsourcesofinformationabout\nMatConvNet\n.First,thewebsitecon-\ntainsdescriptionsofallthefunctionsandseveralexamplesandtutorials.\n11\nSecond,there\nisaPDFmanualcontainingagreatdealoftechnicaldetailsaboutthetoolbox,including\ndetailedmathematicaldescriptionsofthebuildingblocks.Third,\nMatConvNet\nshipswith\nseveralexamples(section\n1.1\n).\nMostexamplesarefullyself-contained.Forexample,inordertoruntheMNISTexample,\nitstopointMATLABtothe\nMatConvNet\nrootdirectoryandtype\naddpath\n \n-\nexamples\nfollowedby\ncnn_mnist\n.Duetotheproblemsize,theImageNetILSVRCexample\nrequiressomemorepreparation,includingdownloadingandpreprocessingtheimages(using\nthebundledscript\nutils\n/\npreprocess\n\nimagenet\n.\nsh\n).Severaladvancedexamplesareincluded\naswell.Forexample,\n1.2\nillustratesthetop-1andtop-5validationerrorsasamodel\nsimilartoAlexNet[\n7\n]istrainedusingeitherstandarddropoutregularisationortherecent\nbatchnormalisation\ntechniqueof[\n3\n].Thelatterisshowntoconvergeinaboutonethirdof\ntheepochs(passesthroughthetrainingdata)requiredbytheformer.\nThe\nMatConvNet\nwebsitecontainsalsonumerous\npre-trained\nmodels,i.e.largeCNNs\ntrainedonImageNetILSVRCthatcanbedownloadedandusedasastartingpointformany\notherproblems[\n1\n].Theseinclude:AlexNet[\n7\n],VGG-S,VGG-M,VGG-S[\n1\n],andVGG-VD-\n16,andVGG-VD-19[\n10\n].Theexamplecodeof\n1.1\nshowshowonesuchmodelcanbe\nusedinafewlinesofMATLABcode.\n11\nSeealso\nhttp://www.robots.ox.ac.uk/\n~\nvgg/practicals/cnn/index.html\n.\n'b'6\nCHAPTER1.INTRODUCTIONTOMATCONVNET\nmodelbatchsz.\nCPUGPUCuDNN\nAlexNet256\n22.1192.4264.1\nVGG-F256\n21.4211.4289.7\nVGG-M128\n7.8116.5136.6\nVGG-S128\n7.496.2110.1\nVGG-VD-1624\n1.718.420.0\nVGG-VD-1924\n1.515.716.5\nTable1.1:ImageNettrainingspeed(images/s).\n1.4Speed\nisveryimportantforworkingwithCNNs.\nMatConvNet\nsupportsusingNVIDIA\nGPUsasitincludesCUDAimplementationsofallalgorithms(orreliesonMATLABCUDA\nsupport).\nTousetheGPU(providedthatsuitablehardwareisavailableandthetoolboxhasbeen\ncompiledwithGPUsupport),onesimplyconvertstheargumentsto\ngpuArrays\ninMATLAB,\nasin\ny\n=\nvl_nnconv\n(\ngpuArray\n(\nx\n),\ngpuArray\n(\nw\n),[])\n.Inthismanner,switchingbetweenCPU\nandGPUisfullytransparent.Notethat\nMatConvNet\ncanalsomakeuseoftheNVIDIA\nCuDNNlibrarywithtspeedandspaceb\nNextweevaluatetheperformanceof\nMatConvNet\nwhentraininglargearchitectures\nontheImageNetILSVRC2012challengedata[\n2\n].ThetestmachineisaDellserverwith\ntwoIntelXeonCPUE5-2667v2clockedat3.30GHz(eachCPUhaseightcores),256GB\nofRAM,andfourNVIDIATitanBlackGPUs(onlyoneofwhichisusedunlessotherwise\nnoted).Experimentsuse\nMatConvNet\nbeta12,CuDNNv2,andMATLABR2015a.The\ndataispreprocessedtoavoidrescalingimagesontheinMATLABandstoredinaRAM\ndiskforfasteraccess.Thecodeusesthe\nvl_imreadjpeg\ncommandtoreadlargebatchesof\nJPEGimagesfromdiskinanumberofseparatethreads.Thedriver\nexamples\n/\ncnn_imagenet\n.\nm\nisusedinallexperiments.\nWetrainthemodelsdiscussedinsection\n1.3\nonImageNetILSVRC.table\n1.1\nreports\nthetrainingspeedasnumberofimagespersecondprocessedbystochasticgradientdescent.\nAlexNettrainsatabout264images/swithCuDNN,whichisabout40%fasterthanthe\nvanillaGPUimplementation(usingCuBLAS)andmorethan10timesfasterthanusingthe\nCPUs.Furthermore,wenotethat,despiteMATLABoverhead,theimplementationspeedis\ncomparableto(theyreport253images/swithCuDNNandaTitan{aslightlyslower\nGPUthantheTitanBlackusedhere).Notealsothat,asthemodelgrowsinsize,thesizeof\naSGDbatchmustbedecreased(tointheGPUmemory),increasingtheoverheadimpact\nsomewhat.\ntable\n1.2\nreportsthespeedonVGG-VD-16,averylargemodel,usingmultipleGPUs.In\nthiscase,thebatchsizeissetto264images.Thesearefurtherdividedinsub-batchesof22\nimageseachtointheGPUmemory;thelatterarethendistributedamongonetofour\nGPUsonthesamemachine.Whilethereisasubstantialcommunicationoverhead,training\nspeedincreasesfrom20images/sto45.Addressingthisoverheadisoneofthemediumterm\ngoalsofthelibrary.\n'b'1.5.ACKNOWLEDGMENTS\n7\nnumGPUs\n1234\nVGG-VD-16speed\n20.022.2038.1844.8\nTable1.2:MultipleGPUspeed(images/s).\n1.5Acknowledgments\nMatConvNet\nisacommunityproject,andassuchacknowledgementsgotoallcontributors.\nWekindlythankNVIDIAsupportingthisprojectbyprovidinguswithtop-of-the-lineGPUs\nandMathWorksforongoingdiscussiononhowtoimprovethelibrary.\nTheimplementationofseveralCNNcomputationsinthislibraryareinspiredbythe\nlibrary[\n5\n](however,is\nnot\nadependency).Severaloftheexamplenetworkshavebeen\ntrainedbyKarenSimonyanaspartof[\n1\n]and[\n10\n].\n'b''b'Chapter2\nNeuralNetworkComputations\nThischapterprovidesabriefintroductiontothecomputationalaspectsofneuralnetworks,\nandconvolutionalneuralnetworksinparticular,emphasizingtheconceptsrequiredtoun-\nderstandanduse\nMatConvNet\n.\n2.1Overview\nA\nNeuralNetwork\n(NN)isafunction\ng\nmappingdata\nx\n,forexampleanimage,toanoutput\nvector\ny\n,forexampleanimagelabel.Thefunction\ng\n=\nf\nL\n\nf\n1\nisthecomposition\nofasequenceofsimplerfunctions\nf\nl\n,whicharecalled\ncomputationalblocks\nor\nlayers\n.Let\nx\n1\n;\nx\n2\n;:::;\nx\nL\nbetheoutputsofeachlayerinthenetwork,andlet\nx\n0\n=\nx\ndenotethenetwork\ninput.Eachintermediateoutput\nx\nl\n=\nf\nl\n(\nx\nl\n\n1\n;\nw\nl\n)iscomputedfromthepreviousoutput\nx\nl\n\n1\nbyapplyingthefunction\nf\nl\nwithparameters\nw\nl\n.\nIna\nConvolutionalNeuralNetwork\n(CNN),thedatahasaspatialstructure:each\nx\nl\n2\nR\nH\nl\n\nW\nl\n\nC\nl\nisa3Darrayor\ntensor\nwherethetwodimensions\nH\nl\n(height)and\nW\nl\n(width)\nareinterpretedasspatialdimensions.Thethirddimension\nC\nl\nisinsteadinterpretedas\nthe\nnumberoffeaturechannels\n.Hence,thetensor\nx\nl\nrepresentsa\nH\nl\n\nW\nl\nof\nC\nl\n-\ndimensionalfeaturevectors,oneforeachspatiallocation.Afourthdimension\nN\nl\ninthe\ntensorspansmultipledatasamplespackedinasingle\nbatch\nforparallelprocessing.\nThenumberofdatasamples\nN\nl\ninabatchiscalledthebatch\ncardinality\n.Thenetworkis\ncalled\nconvolutional\nbecausethefunctions\nf\nl\narelocalandtranslationinvariantoperators\n(i.e.non-linearlikelinearconvolution.\nItisalsopossibletoconceiveCNNswithmorethantwospatialdimensions,wherethe\nadditionaldimensionsmayrepresentvolumeortime.Infact,therearelittle\na-priori\nre-\nstrictionsontheformatofdatainneuralnetworksingeneral.ManyusefulNNscontaina\nmixtureofconvolutionallayerstogetherwithlayerthatprocessotherdatatypessuchastext\nstrings,orperformotheroperationsthatdonotstrictlyconformtotheCNNassumptions.\nMatConvNet\nincludesavarietyoflayers,containedinthe\nmatlab\n/\ndirectory,such\nas\nvl_nnconv\n(convolution),\nvl_nnconvt\n(convolutiontransposeordeconvolution),\nvl_nnpool\n(maxandaveragepooling),\nvl_nnrelu\n(ReLUactivation),\nvl_nnsigmoid\n(sigmoidactivation),\nvl_nnsoftmax\n(softmaxoperator),\nvl_nnloss\nlog-loss),\nvl_nnbnorm\n(batchnor-\nmalization),\nvl_nnspnorm\n(spatialnormalization),\nvl_nnnormalize\n(locarresponsenormal-\n9\n'b'10\nCHAPTER2.NEURALNETWORKCOMPUTATIONS\nization{LRN),or\nvl_nnpdist\n(\np\n-distance).Thereareenoughlayerstoimplementmany\ninterestingstate-of-the-artnetworksoutofthebox,orevenimportthemfromothertool-\nboxessuchas\nNNsareoftenusedasorregressors.Intheexampleof\n1.1\n,theoutput\n^\ny\n=\nf\n(\nx\n)isavectorofprobabilities,oneforeachofa1,000possibleimagelabels(dog,cat,\ntrilobite,...).If\ny\nisthetruelabelofimage\nx\n,wecanmeasuretheCNNperformancebya\nlossfunction\n`\ny\n(\n^\ny\n)\n2\nR\nwhichassignsapenaltytoerrors.TheCNNparameters\ncanthenbetunedor\nlearned\ntominimizethislossaveragedoveralargedatasetoflabelled\nexampleimages.\nLearninggenerallyusesavariantof\nstochasticgradientdescent\n(SGD).Whilethisisan\ntmethod(forthistypeofproblems),networksmaycontainseveralmillionparameters\nandneedtobetrainedonmillionsofimages;thus,isaparamountin\nMATLAB\ndesign,asfurtherdiscussedinsection\n1.4\n.SGDalsorequirestocomputetheCNNderivatives,\nasexplainedinthenextsection.\n2.2Networkstructures\nInthesimplestcase,layersinaNNarearrangedinasequence;however,morecomplex\ninterconnectionsarepossibleaswell,andinfactveryusefulinmanycases.Thissection\ndiscussessuchcandintroducesagraphicalnotationtovisualizethem.\n2.2.1Sequences\nStartbyconsideringacomputationalblock\nf\ninthenetwork.Thiscanberepresented\nschematicallyasaboxreceivingdata\nx\nandparameters\nw\nasinputsandproducingdata\ny\nasoutput:\nx\nf\ny\nw\nAsseenabove,inthesimplestcaseblocksarechainedinasequence\nf\n1\n!\nf\n2\n!!\nf\nL\nyieldingthestructure:\nx\n0\nf\n1\nf\n2\n...\nf\nL\nx\nL\nw\n1\nw\n2\nw\nL\nx\n1\nx\n2\nx\nL\n\n1\nGivenaninput\nx\n0\n,evaluatingthenetworkisasimplematterofevaluatingalltheblocks\nfromlefttoright,whichacompositefunction\nx\nL\n=\nf\n(\nx\n0\n;\nw\n1\n;:::;\nw\nL\n).\n'b'2.2.NETWORKSTRUCTURES\n11\nf\n1\nx\n1\nx\n0\nf\n3\nx\n3\nf\n2\nx\n2\nf\n5\nx\n7\nx\n5\nx\n4\nf\n4\nx\n6\nw\n1\nw\n2\nw\n4\nw\n5\nFigure2.1:\nExampleDAG.\n2.2.2Directedacyclicgraphs\nOneisnotlimitedtochaininglayersoneafteranother.Infact,theonlyrequirementfor\nevaluatingaNNisthat,whenalayerhastobeevaluated,allitsinputhavebeenevaluated\npriortoit.Thisispossibleexactlywhentheinterconnectionsbetweenlayersforma\ndirected\nacyclicgraph\n,orDAGforshort.\nInordertovisualizeDAGs,itisusefultointroduceadditionalnodesforthenetwork\nvariables,asintheexampleofFig.\n2.1\n.Hereboxesdenotefunctionsandcirclesdenote\nvariables(parametersaretreatedasaspecialkindofvariables).Intheexample,\nx\n0\nand\nx\n4\naretheinputsoftheCNNand\nx\n6\nand\nx\n7\ntheoutputs.Functionscantakeanynumberof\ninputs(e.g.\nf\n3\nand\nf\n5\ntaketwo)andhaveanynumberofoutputs(e.g.\nf\n4\nhastwo).There\nareafewnoteworthypropertiesofthisgraph:\n1.\nThegraphisbipartite,inthesensethatarrowsalwaysgofromboxestocirclesand\nfromcirclestoboxes.\n2.\nFunctionscanhaveanynumberofinputsoroutputs;variablesandparameterscan\nhaveanarbitrarynumberofoutputs(aparameterwithmoreofoneoutputis\nshared\nbetweentlayers);variableshaveatmostoneinputandparametersnone.\n3.\nVariableswithnoincomingarrowsandparametersarenotcomputedbythenetwork,\nbutmustbesetpriortoevaluation,i.e.theyare\ninputs\n.Anyvariable(orevenparam-\neter)maybeusedasoutput,althoughtheseareusuallythevariableswithnooutgoing\narrows.\n'b'12\nCHAPTER2.NEURALNETWORKCOMPUTATIONS\n4.\nSincethegraphisacyclic,theCNNcanbeevaluatedbysortingthefunctionsand\ncomputingthemoneafteranother(intheexample,evaluatingthefunctionsinthe\norder\nf\n1\n;f\n2\n;f\n3\n;f\n4\n;f\n5\nwouldwork).\n2.3Computingderivativeswithbackpropagation\nLearningaNNrequirescomputingthederivativeofthelosswithrespecttothenetwork\nparameters.Derivativesarecomputedusinganalgorithmcalled\nbackpropagation\n,whichis\natimplementationofthechainruleforderivatives.First,wediscussthe\nderivativesofasinglelayer,andthenofawholenetwork.\n2.3.1Derivativesoftensorfunctions\nInaCNN,alayerisafunction\ny\n=\nf\n(\nx\n)wherebothinput\nx\n2\nR\nH\n\nW\n\nC\nandoutput\ny\n2\nR\nH\n0\n\nW\n0\n\nC\n0\naretensors.Thederivativeofthefunction\nf\ncontainsthederivativeof\neachoutputcomponent\ny\ni\n0\nj\n0\nk\n0\nwithrespecttoeachinputcomponent\nx\nijk\n,foratotalof\nH\n0\n\nW\n0\n\nC\n0\n\nH\n\nW\n\nC\nelementsnaturallyarrangedina6Dtensor.Insteadofexpressing\nderivativesastensors,itisoftenusefultoswitchtoamatrixnotationby\nstacking\ntheinput\nandoutputtensorsintovectors.Thisisdonebythevecoperator,whichvisitseachelement\nofatensorinlexicographicalorderandproducesavector:\nvec\nx\n=\n2\n6\n6\n6\n6\n6\n6\n6\n6\n6\n4\nx\n111\nx\n211\n.\n.\n.\nx\nH\n11\nx\n121\n.\n.\n.\nx\nHWC\n3\n7\n7\n7\n7\n7\n7\n7\n7\n7\n5\n:\nBystackingbothinputandoutput,eachlayer\nf\ncanbeseenreinterpretedasvectorfunction\nvec\nf\n,whosederivativeistheconventionalJacobianmatrix:\nd\nvec\nf\nd\n(vec\nx\n)\n>\n=\n2\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n4\n@y\n111\n@x\n111\n@y\n111\n@x\n211\n:::\n@y\n111\n@x\nH\n11\n@y\n111\n@x\n121\n:::\n@y\n111\n@x\nHWC\n@y\n211\n@x\n111\n@y\n211\n@x\n211\n:::\n@y\n211\n@x\nH\n11\n@y\n211\n@x\n121\n:::\n@y\n211\n@x\nHWC\n.\n.\n.\n.\n.\n.\n:::\n.\n.\n.\n.\n.\n.\n:::\n.\n.\n.\n@y\nH\n0\n11\n@x\n111\n@y\nH\n0\n11\n@x\n211\n:::\n@y\nH\n0\n11\n@x\nH\n11\n@y\nH\n0\n11\n@x\n121\n:::\n@y\nH\n0\n11\n@x\nHWC\n@y\n121\n@x\n111\n@y\n121\n@x\n211\n:::\n@y\n121\n@x\nH\n11\n@y\n121\n@x\n121\n:::\n@y\n121\n@x\nHWC\n.\n.\n.\n.\n.\n.\n:::\n.\n.\n.\n.\n.\n.\n:::\n.\n.\n.\n@y\nH\n0\nW\n0\nC\n0\n@x\n111\n@y\nH\n0\nW\n0\nC\n0\n@x\n211\n:::\n@y\nH\n0\nW\n0\nC\n0\n@x\nH\n11\n@y\nH\n0\nW\n0\nC\n0\n@x\n121\n:::\n@y\nH\n0\nW\n0\nC\n0\n@x\nHWC\n3\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n5\n:\nThisnotationforthederivativesoftensorfunctionsistakenfrom[\n6\n]andisusedthroughout\nthisdocument.\n'b'2.3.COMPUTINGDERIVATIVESWITHBACKPROPAGATION\n13\nWhileitiseasytoexpressthederivativesoftensorfunctionsasmatrices,thesematrices\nareingeneralextremelylarge.Evenformoderatedatasizes(e.g.\nH\n=\nH\n0\n=\nW\n=\nW\n0\n=\n32and\nC\n=\nC\n0\n=128),thereare\nH\n0\nW\n0\nC\n0\nHWC\n\n17\n\n10\n9\nelementsintheJacobian.\nStoringthatrequires68GBofspaceinsingleprecision.Thepurposeofthebackpropagation\nalgorithmistocomputethederivativesrequiredforlearningwithoutincurringthishuge\nmemorycost.\n2.3.2Derivativesoffunctioncompositions\nInordertounderstandbackpropagation,considerasimpleCNNterminatinginaloss\nfunction\nf\nL\n=\n`\ny\n:\nx\n0\nf\n1\nf\n2\n...\nf\nL\nw\n1\nw\n2\nw\nL\nx\nl\n2\nR\nx\n1\nx\n2\nx\nL\n\n1\nThegoalistocomputethegradientofthelossvalue\nx\nL\n(output)withrespecttoeachnetwork\nparameter\nw\nl\n:\ndf\nd\n(vec\nw\nl\n)\n>\n=\nd\nd\n(vec\nw\nl\n)\n>\n[\nf\nL\n(\n\n;\nw\nL\n)\n\n:::\n\nf\n2\n(\n\n;\nw\n2\n)\n\nf\n1\n(\nx\n0\n;\nw\n1\n)]\n:\nByapplyingthechainruleandbyusingthematrixnotationintroducedabove,thederivative\ncanbewrittenas\ndf\nd\n(vec\nw\nl\n)\n>\n=\nd\nvec\nf\nL\n(\nx\nL\n\n1\n;\nw\nL\n)\nd\n(vec\nx\nL\n\n1\n)\n>\n\nd\nvec\nf\nl\n+1\n(\nx\nl\n;\nw\nl\n+1\n)\nd\n(vec\nx\nl\n)\n>\n\nd\nvec\nf\nl\n(\nx\nl\n\n1\n;\nw\nl\n)\nd\n(vec\nw\n>\nl\n)\n(2.1)\nwherethederivativesarecomputedattheworkingpointdeterminedbytheinput\nx\n0\nandthe\ncurrentvalueoftheparameters.\nNotethat,sincethenetworkoutput\nx\nl\nisa\nscalar\nquantity,thetargetderivative\ndf=d\n(vec\nw\nl\n)\n>\nhasthesamenumberofelementsoftheparametervector\nw\nl\n,whichismoder-\nate.However,theintermediateJacobianfactorshave,asseenabove,anunmanageablesize.\nInordertoavoidcomputingthesefactorexplicitly,wecanproceedasfollows.\nStartbymultiplyingtheoutputofthelastlayerbyatensor\np\nL\n=1(notethatthistensor\nisascalarjustlikethevariable\nx\nL\n):\np\nL\n\ndf\nd\n(vec\nw\nl\n)\n>\n=\np\nL\n\nd\nvec\nf\nL\n(\nx\nL\n\n1\n;\nw\nL\n)\nd\n(vec\nx\nL\n\n1\n)\n>\n|\n{z\n}\n(vec\np\nL\n\n1\n)\n>\n\nd\nvec\nf\nl\n+1\n(\nx\nl\n;\nw\nl\n+1\n)\nd\n(vec\nx\nl\n)\n>\n\nd\nvec\nf\nl\n(\nx\nl\n\n1\n;\nw\nl\n)\nd\n(vec\nw\n>\nl\n)\n=(vec\np\nL\n\n1\n)\n>\n\nd\nvec\nf\nl\n+1\n(\nx\nl\n;\nw\nl\n+1\n)\nd\n(vec\nx\nl\n)\n>\n\nd\nvec\nf\nl\n(\nx\nl\n\n1\n;\nw\nl\n)\nd\n(vec\nw\n>\nl\n)\nInthesecondlinethelasttwofactorstothelefthavebeenmultipliedobtaininganew\ntensor\np\nL\n\n1\nthathasthesamesizeasthevariable\nx\nL\n\n1\n.Thefactor\np\nL\n\n1\ncanthereforebe\n'b'14\nCHAPTER2.NEURALNETWORKCOMPUTATIONS\nexplicitlystored.Theconstructionisthenrepeatedbymultiplyingpairsoffactorsfromleft\ntoright,obtainingasequenceoftensors\np\nL\n\n2\n;:::;\np\nl\nuntilthedesiredderivativeisobtained.\nNotethat,indoingso,nolargetensoriseverstoredinmemory.Thisprocessisknownas\nbackpropagation\n.\nIngeneral,tensor\np\nl\nisobtainedfrom\np\nl\n+1\nastheproduct:\n(vec\np\nl\n)\n>\n=(vec\np\nl\n+1\n)\n>\n\nd\nvec\nf\nl\n+1\n(\nx\nl\n;\nw\nl\n+1\n)\nd\n(vec\nx\nl\n)\n>\n:\nThekeytoimplementbackpropagationistobeabletocomputetheseproductswithout\nexplicitlycomputingandstoringinmemorythesecondfactor,whichisalargeJacobian\nmatrix.Sincecomputingthederivativeisalinearoperation,thisproductcanbeinterpreted\nasthe\nderivativeofthelayerprojectedalongdirection\np\nl\n+1\n:\np\nl\n=\nd\nh\np\nl\n+1\n;f\n(\nx\nl\n;\nw\nl\n)\ni\nd\nx\nl\n:\n(2.2)\nHere\n\n;\n\ndenotestheinnerproductbetweentensors,whichresultsinascalarquantity.\nHencethederivative(\n2.2\n)needsnottousethevecnotation,andyieldsatensor\np\nl\nthathas\nthesamesizeas\nx\nl\nasexpected.\nInordertoimplementbackpropagation,aCNNtoolboxprovidesimplementationsofeach\nlayer\nf\nthatprovide:\n\nA\nforwardmode\n,computingtheoutput\ny\n=\nf\n(\nx\n;\nw\n)ofthelayergivenitsinput\nx\nandparameters\nw\n.\n\nA\nbackwardmode\n,computingtheprojectedderivatives\nd\nh\np\n;f\n(\nx\n;\nw\n)\ni\nd\nx\nand\nd\nh\np\n;f\n(\nx\n;\nw\n)\ni\nd\nw\n;\ngiven,inadditiontotheinput\nx\nandparameters\nw\n,atensor\np\nthatthesamesizeas\ny\n.\nThisisbestillustratedwithanexample.Consideralayer\nf\nsuchastheconvolutionoperator\nimplementedbythe\nMatConvNet\nvl_nnconv\ncommand.Inthe\\forward"mode,onecalls\nthefunctionas\ny\n=\nvl_nnconv\n(\nx\n,\nw\n,[])\ntoapplythe\nw\ntotheinput\nx\nandobtainthe\noutput\ny\n.Inthe\\backwardmode",onecalls\n[\ndx\n,\ndw\n]=\nvl_nnconv\n(\nx\n,\nw\n,[],\np\n)\n.Asexplained\nabove,\ndx\n,\ndw\n,and\np\nhavethesamesizeas\nx\n,\nw\n,and\ny\n,respectively.Thecomputationoflarge\nJacobianisencapsulatedinthefunctioncallandnevercarriedoutexplicitly.\n2.3.3Backpropagationnetworks\nInthissection,weprovideaschematicinterpretationofbackpropagationandshowhowit\ncanbeimplementedby\\reversing"theNNcomputationalgraph.\nTheprojectedderivativeofeq.(\n2.2\n)canbeseenasthederivativeofthefollowingmini-\nnetwork:\n'b'2.3.COMPUTINGDERIVATIVESWITHBACKPROPAGATION\n15\nx\nf\n\n;\n\nz\n2\nR\nw\np\ny\nInthecontextofback-propagation,itcanbeusefultothinkoftheprojection\np\nasthe\n\\linearization"oftherestofthenetworkfromvariable\ny\ndowntotheloss.Theprojected\nderivativecanalsobethoughofasanewlayer(\nd\nx\n;d\nw\n)=\ndf\n(\nx\n;\nw\n;\np\n)that,bycomputing\nthederivativeofthemini-network,operatesinthereversedirection:\ndf\nd\nx\nd\nw\nw\nx\np\nByconstruction(seeeq.(\n2.2\n)),thefunction\ndf\nis\nlinear\nintheargument\np\n.\nUsingthisnotation,theforwardandbackwardpassesthroughtheoriginalnetworkcan\nberewrittenasevaluatinganextendednetworkwhichcontainsaBP-reverseoftheoriginal\none(inblueinthediagram):\nx\n0\nf\n1\nx\n1\nw\n1\nf\n2\nx\n2\nw\n2\n:::\nx\nL\n\n1\nf\nL\nx\nL\nw\nL\ndf\nL\nd\np\nL\nd\nw\nL\nd\nx\nL\n\n1\n:::\ndf\n2\nd\nx\n2\nd\nw\n2\ndf\n1\nd\nx\n1\nd\nw\n1\nd\nx\n0\n2.3.4BackpropagationinDAGs\nAssumethattheDAGhasasingleoutputvariable\nx\nL\nandassume,withoutlossofgenerality,\nthatallvariablesaresortedinorderofcomputation(\nx\n0\n;\nx\n1\n;:::;\nx\nL\n\n1\n;\nx\nL\n)accordingtothe\n'b'16\nCHAPTER2.NEURALNETWORKCOMPUTATIONS\nDAGstructure.Furthermore,inordertosimplifythenotation,assumethatthislistcontains\nbothdataandparametervariables,asthedistinctionismootforthediscussioninthissection.\nWecancuttheDAGatanypointinthesequenceby\nx\n0\n;:::;\nx\nl\n\n1\ntosomearbitrary\nvalueanddroppingalltheDAGlayersthatfeedintothem,elytransformingthe\nl\nvariablesintoinputs.Then,therestoftheDAGafunction\nh\nl\nthatmapstheseinput\nvariablestotheoutput\nx\nL\n:\nx\nL\n=\nh\nl\n(\nx\n0\n;\nx\n1\n;:::;\nx\nl\n\n1\n)\n:\nNext,weshowthatbackpropagationinaDAGiterativelycomputestheprojectedderivatives\nofallfunctions\nh\n1\n;:::;h\nL\nwithrespecttoalltheirparameters.\nBackpropagationstartsbyinitializingvariables(\nd\nx\n0\n;:::;d\nx\nl\n\n1\n)tonulltensorsofthe\nsamesizeas(\nx\n0\n;:::;\nx\nl\n\n1\n).Next,itcomputestheprojectedderivativesof\nx\nL\n=\nh\nL\n(\nx\n0\n;\nx\n1\n;:::;\nx\nL\n\n1\n)=\nf\n\nL\n(\nx\n0\n;\nx\n1\n;:::;\nx\nL\n\n1\n)\n:\nHere\n\nl\ndenotestheindexofthelayer\nf\n\nl\nthatcomputesthevalueofthevariable\nx\nl\n.There\nisatmostonesuchlayer,ornoneif\nx\nl\nisaninputorparameteroftheoriginalNN.Inthe\ncase,thelayermaydependonanyofthevariablespriorto\nx\nl\ninthesequence,sothat\ngeneralonehas:\nx\nl\n=\nf\n\nl\n(\nx\n0\n;:::;\nx\nl\n\n1\n)\n:\nAtthebeginningofbackpropagation,sincetherearenointermediatevariablesbetween\nx\nL\n\n1\nand\nx\nL\n,thefunction\nh\nL\nisthesameasthelastlayer\nf\n\nL\n.Thustheprojectedderivativesof\nh\nL\narethesameastheprojectedderivativesof\nf\n\nL\n,resultingintheequation\n8\nt\n=0\n;:::;L\n\n1:\nd\nx\nt\n \nd\nx\nt\n+\nd\nh\np\nL\n;f\n\nL\n(\nx\n0\n;:::;\nx\nt\n\n1\n)\ni\nd\nx\nt\n:\nHere,foruniformitywiththeotheriterations,weusethefactthat\nd\nx\nl\nareinitializedtozero\nan\naccumulate\nthevaluesinsteadofstoringthem.Inpractice,theupdateoperationneedsto\nbecarriedoutonlyforthevariables\nx\nl\nthatareactualinputsto\nf\n\nL\n,whichisoftenatiny\nfractionofallthevariablesintheDAG.\nAftertheupdate,each\nd\nx\nt\ncontainstheprojectedderivativeoffunction\nh\nL\nwithrespect\ntothecorrespondingvariable:\n8\nt\n=0\n;:::;L\n\n1:\nd\nx\nt\n=\nd\nh\np\nL\n;h\nL\n(\nx\n0\n;:::;\nx\nl\n\n1\n)\ni\nd\nx\nt\n:\nGiventhisinformation,thenextiterationofbackpropagationupdatesthevariablestocon-\ntaintheprojectedderivativesof\nh\nL\n\n1\ninstead.Ingeneral,giventhederivativesof\nh\nl\n+1\n,\nbackpropagationcomputesthederivativesof\nh\nl\nbyusingtherelation\nx\nL\n=\nh\nl\n(\nx\n0\n;\nx\n1\n;:::;\nx\nl\n\n1\n)=\nh\nl\n+1\n(\nx\n0\n;\nx\n1\n;:::;\nx\nl\n\n1\n;f\n\nL\n(\nx\n0\n;:::;\nx\nl\n\n1\n))\nApplyingthechainruletothisexpression,forall0\n\nt\n\nl\n\n1:\nd\nh\np\n;h\nl\ni\nd\n(vec\nx\nt\n)\n>\n=\nd\nh\np\n;h\nl\n+1\ni\nd\n(vec\nx\nt\n)\n>\n+\nd\nh\np\nL\n;h\nl\n+1\ni\nd\n(vec\nx\nl\n)\n>\n|\n{z\n}\nvec\nd\nx\nl\nd\nvec\nf\n\nl\nd\n(vec\nx\nt\n)\n>\n:\n'b'2.3.COMPUTINGDERIVATIVESWITHBACKPROPAGATION\n17\nThisyieldstheupdateequation\n8\nt\n=0\n;:::;l\n\n1:\nd\nx\nt\n \nd\nx\nt\n+\nd\nh\np\nl\n;f\n\nl\n(\nx\n0\n;:::;\nx\nl\n\n1\n)\ni\nd\nx\nt\n;\nwhere\np\nl\n=\nd\nx\nl\n:\n(2.3)\nOncemore,theupdateneedstobeexplicitlycarriedoutonlyforthevariables\nx\nt\nthatare\nactualinputsof\nf\n\nl\n.Inparticular,if\nx\nl\nisadatainputoraparameteroftheoriginalneural\nnetwork,then\nx\nl\ndoesnotdependonanyothervariablesorparametersand\nf\n\nl\nisanullary\nfunction(i.e.afunctionwithnoarguments).Inthiscase,theupdatedoesnotdoanything.\nAfteriteration\nL\n\nl\n+1completes,backpropagationremainswith:\n8\nt\n=0\n;:::;l\n\n1:\nd\nx\nt\n=\nd\nh\np\nL\n;h\nl\n(\nx\n0\n;:::;\nx\nl\n\n1\n)\ni\nd\nx\nt\n:\nNotethatthederivativesforvariables\nx\nt\n;l\n\nt\n\nL\n\n1arenotupdatedsince\nh\nl\ndoesnot\ndependonanyofthose.Thus,afterall\nL\niterationsarecomplete,backpropagationterminates\nwith\n8\nl\n=1\n;:::;L\n:\nd\nx\nl\n\n1\n=\nd\nh\np\nL\n;h\nl\n(\nx\n0\n;:::;\nx\nl\n\n1\n)\ni\nd\nx\nl\n\n1\n:\nAsseenabove,functions\nh\nl\nareobtainedfromtheoriginalnetwork\nf\nbytransformingvariables\nx\n0\n;:::;\nx\nl\n\n1\nintotoinputs.If\nx\nl\n\n1\nwasalreadyaninput(dataorparameter)of\nf\n,thenthe\nderivative\nd\nx\nl\n\n1\nisapplicableto\nf\naswell.\nBackpropagationcanbesummarizedasfollows:\nGiven:aDAGneuralnetwork\nf\nwithasingleoutput\nx\nL\n,thevaluesofallinputvariables\n(includingtheparameters),andthevalueoftheprojection\np\nL\n(usually\nx\nL\nisascalar\nand\np\nL\n=\np\nL\n=1):\n1.\nSortallvariablesbycomputationorder(\nx\n0\n;\nx\n1\n;:::;\nx\nL\n)accordingtotheDAG.\n2.\nPerformaforwardpassthroughthenetworktocomputealltheintermediatevari-\nablevalues.\n3.\nInitialize(\nd\nx\n0\n;:::;d\nx\nL\n\n1\n)tonulltensorswiththesamesizeasthecorresponding\nvariables.\n4.\nFor\nl\n=\nL;L\n\n1\n;:::;\n2\n;\n1:\na)\nFindtheindex\n\nl\nofthelayer\nx\nl\n=\nf\n\nl\n(\nx\n0\n;:::;\nx\nl\n\n1\n)thatevaluatesvariable\nx\nl\n.\nIfthereisnosuchlayer(because\nx\nl\nisaninputorparameterofthenetwork),\ngotothenextiteration.\nb)\nUpdatethevariablesusingtheformula:\n8\nt\n=0\n;:::;l\n\n1:\nd\nx\nt\n \nd\nx\nt\n+\nd\nh\nd\nx\nl\n;f\n\nl\n(\nx\n0\n;:::;\nx\nl\n\n1\n)\ni\nd\nx\nt\n:\nTodosociently,usethe\\backwardmode"ofthelayer\nf\n\nl\ntocomputeits\nderivativeprojectedonto\nd\nx\nl\nasneeded.\n'b'18\nCHAPTER2.NEURALNETWORKCOMPUTATIONS\n2.3.5DAGbackpropagationnetworks\nJustlikeforsequences,backpropagationinDAGscanbeimplementedasacorresponding\nBP-reversedDAG.ToconstructthereversedDAG:\n1.\nForeachlayer\nf\nl\n,andvariable/parameter\nx\nt\nand\nw\nl\n,createacorrespondinglayer\ndf\nl\nandvariable/parameter\nd\nx\nt\nand\nd\nw\nl\n.\n2.\nIfavariable\nx\nt\n(orparameter\nw\nl\n)isaninputof\nf\nl\n,thenitisaninputof\ndf\nl\naswell.\n3.\nIfavariable\nx\nt\n(orparameter\nw\nl\n)isaninputof\nf\nl\n,thenthevariable\nd\nx\nt\n(orthe\nparameter\nd\nw\nl\n)isanoutput\ndf\nl\n.\n4.\nInthepreviousstep,ifavariable\nx\nt\n(orparameter\nw\nl\n)isinputtotwoormorelayersin\nf\n,then\nd\nx\nt\nwouldbetheoutputoftwoormorelayersinthereversednetwork,which\ncreatesaResolvethesebyinsertingasummationlayerthatadds\nthesecontributions(thiscorrespondstothesummationintheBPupdateequation\n(\n2.3\n)).\nTheBPnetworkcorrespondingtotheDAGofFig.\n2.1\nisgiveninFig.\n2.2\n.\n'b'2.3.COMPUTINGDERIVATIVESWITHBACKPROPAGATION\n19\nf\n1\nx\n1\nx\n0\nf\n3\nx\n3\nf\n2\nx\n2\nf\n5\nx\n7\nx\n5\nx\n4\nf\n4\nx\n6\ndf\n1\nd\nx\n1\nd\nx\n0\ndf\n3\nd\nx\n3\ndf\n2\nd\nx\n2\ndf\n5\np\n7\nd\nx\n5\nd\nx\n4\ndf\n4\np\n6\nw\n1\nw\n2\nw\n4\nw\n5\n\nd\nw\n1\nd\nw\n2\n\nd\nw\n4\nd\nw\n5\nFigure2.2:\nBackpropagationnetworkforaDAG.\n'b''b'Chapter3\nWrappersandpre-trainedmodels\nItiseasyenoughtocombinethecomputationalblocksofchapter\n4\n\\manually".However,it\nisusuallymuchmoreconvenienttousethemthrougha\nwrapper\nthatcanimplementCNN\narchitecturesgivenamodelspTheavailablewrappersaresummarisedin\nsection\n3.1\n.\nMatConvNet\nalsocomeswithmanypre-trainedmodelsforimage(most\nofwhicharetrainedontheImageNetILSVRCchallenge),imagesegmentation,textspotting,\nandfacerecognition.Theseareverysimpletouse,asillustratedinsection\n3.2\n.\n3.1Wrappers\nMatConvNet\nprovidestwowrappers:SimpleNNforbasicchainsofblocks(section\n3.1.1\n)\nandDagNNforblocksorganizedinmorecomplexdirectacyclicgraphs(section\n3.1.2\n).\n3.1.1SimpleNN\nTheSimpleNNwrapperissuitablefornetworksconsistingoflinearchainsofcomputational\nblocks.Itislargelyimplementedbythe\nvl_simplenn\nfunction(evaluationoftheCNNandof\nitsderivatives),withafewothersupportfunctionssuchas\nvl_simplenn_move\n(movingthe\nCNNbetweenCPUandGPU)and\nvl_simplenn_display\n(obtainand/orprintinformation\nabouttheCNN).\nvl_simplenn\ntakesasinputastructure\nnet\nrepresentingtheCNNaswellasinput\nx\nand\npotentiallyoutputderivatives\ndzdy\n,dependingonthemodeofoperation.Pleaserefertothe\ninlinehelpofthe\nvl_simplenn\nfunctionfordetailsontheinputandoutputformats.Infact,\ntheimplementationof\nvl_simplenn\nisagoodexampleofhowthebasicneuralnetbuilding\nblockscanbeusedtogetherandcanserveasabasisformorecompleximplementations.\n3.1.2DagNN\nTheDagNNwrapperismorecomplexthanSimpleNNasithastosupportarbitrarygraph\ntopologies.Itsdesignisobjectoriented,withoneclassimplementingeachlayertype.While\nthisaddscomplexity,andmakesthewrapperslightlyslowerfortinyCNNarchitectures(e.g.\nMNIST),itisinpracticemuchmoreandeasiertoextend.\n21\n'b"22\nCHAPTER3.WRAPPERSANDPRE-TRAINEDMODELS\nDagNNisimplementedbythe\ndagnn.DagNN\nclass(underthe\ndagnn\nnamespace).\n3.2Pre-trainedmodels\nvl_simplenn\niseasytousewithpre-trainedmodels(seethehomepagetodownloadsome).\nForexample,thefollowingcodedownloadsamodelpre-trainedontheImageNetdataand\nappliesittooneofMATLABstockimages:\n%\nsetup\nMatConvNet\nin\nMATLAB\nrunmatlab\n/\nvl_setupnn\n%\ndownload\na\npre\n\ntrained\nCNN\nfrom\nthe\nweb\nurlwrite\n(...\n'\nhttp\n://\nwww\n.\nvlfeat\n.\norg\n/\nmatconvnet\n/\nmodels\n/\nimagenet\n\nvgg\n\nf\n.\nmat\n'\n,...\n'\nimagenet\n\nvgg\n\nf\n.\nmat\n'\n);\nnet\n=\nload\n(\n'\nimagenet\n\nvgg\n\nf\n.\nmat\n'\n);\n%\nobtain\nand\npreprocess\nan\nimage\nim\n=\nimread\n(\n'\npeppers\n.\npng\n'\n);\nim_\n=\nsingle\n(\nim\n);\n%\nnote\n:\n255\nrange\nim_\n=\nimresize\n(\nim_\n,\nnet\n.\nmeta\n.\nnormalization\n.\nimageSize\n(1:2));\nim_\n=\nim_\n\nnet\n.\nmeta\n.\nnormalization\n.\naverageImage\n;\nNotethattheimageshouldbepreprocessedbeforerunningthenetwork.Whilepreprocessing\nspdependonthemodel,thepre-trainedmodelcontainsa\nnet.meta.normalization\nthatdescribesthetypeofpreprocessingthatisexpected.Noteinparticularthatthis\nnetworktakesimagesofasizeasinputandrequiresremovingthemean;also,image\nintensitiesarenormalizedintherange[0,255].\nThenextstepisrunningtheCNN.Thiswillreturna\nres\nstructurewiththeoutputof\nthenetworklayers:\n%\nrun\nthe\nCNN\nres\n=\nvl_simplenn\n(\nnet\n,\nim_\n);\nTheoutputofthelastlayercanbeusedtoclassifytheimage.Theclassnamesare\ncontainedinthe\nnet\nstructureforconvenience:\n%\nshow\nthe\nation\nresult\nscores\n=\nsqueeze\n(\ngather\n(\nres\n(\nend\n).\nx\n));\n[\nbestScore\n,\nbest\n]=\nmax\n(\nscores\n);\n\n(1);\nclf\n;\nimagesc\n(\nim\n);\ntitle\n(\nsprintf\n(\n'\n%\ns\n(%\nd\n),\nscore\n%.3\nf\n'\n,...\nnet\n.\nmeta\n.\nclasses\n.\ndescription\nf\nbest\ng\n,\nbest\n,\nbestScore\n));\nNotethatseveralextensionsarepossible.First,imagescanbecroppedratherthan\nrescaled.Second,multiplecropscanbefedtothenetworkandresultsaveraged,usuallyfor\nimprovedresults.Third,theoutputofthenetworkcanbeusedasgenericfeaturesforimage\nencoding.\n"b'3.3.LEARNINGMODELS\n23\n3.3Learningmodels\nAs\nMatConvNet\ncancomputederivativesoftheCNNusingbackpropagation,itissimple\ntoimplementlearningalgorithmswithit.Abasicimplementationofstochasticgradient\ndescentisthereforestraightforward.Examplecodeisprovidedin\nexamples/cnn_train\n.\nThiscodeisenoughtoallowtrainingonNMINST,CIFAR,ImageNet,andprobably\nmanyotherdatasets.Correspondingexamplesareprovidedinthe\nexamples/\ndirectory.\n3.4Runninglargescaleexperiments\nForlargescaleexperiments,suchaslearninganetworkforImageNet,aNVIDIAGPU(at\nleast6GBofmemory)andadequateCPUanddiskspeedsarehighlyrecommended.For\nexample,totrainonImageNet,wesuggestthefollowing:\n\nDownloadtheImageNetdata\nhttp://www.image-net.org/challenges/LSVRC\n.In-\nstallitsomewhereandlinktoitfrom\ndata/imagenet12\n\nConsiderpreprocessingthedatatoconvertallimagestohaveaheightof256pixels.\nThiscanbedonewiththesupplied\nutils/preprocess-imagenet.sh\nscript.Inthis\nmanner,trainingwillnothavetoresizetheimageseverytime.Donotforgettopoint\nthetrainingcodetothepre-processeddata.\n\nConsidercopyingthedatasetintoaRAMdisk(providedthatyouhaveenoughmemory)\nforfasteraccess.Donotforgettopointthetrainingcodetothiscopy.\n\nCompile\nMatConvNet\nwithGPUsupport.Seethehomepageforinstructions.\nOnceyoursetupisready,youshouldbeabletorun\nexamples/cnn_imagenet\n(editthe\nandchangeanyasneededtoenableGPUsupportandimagepre-fetchingonmultiple\nthreads).\nIfallgoeswell,youshouldexpecttobeabletotrainwith200-300images/sec.\n'b''b'Chapter4\nComputationalblocks\nThischaptersdescribestheindividualcomputationalblockssupportedby\nMatConvNet\n.\nTheinterfaceofaCNNcomputationalblock\n<\nblock\n>\nisdesignedafterthediscussionin\nchapter\n2\n.TheblockisimplementedasaMATLABfunction\ny\n=\nvl_nn\n<\nblock\n>\n(\nx\n,\nw\n)\nthat\ntakesasinputMATLABarrays\nx\nand\nw\nrepresentingtheinputdataandparametersand\nreturnsanarray\ny\nasoutput.Ingeneral,\nx\nand\ny\nare4Drealarrayspacking\nN\nmapsor\nimages,asdiscussedabove,whereas\nw\nmayhaveanarbitraryshape.\nThefunctionimplementingeachblockiscapableofworkinginthebackwarddirection\naswell,inordertocomputederivatives.Thisisdonebypassingathirdoptionalargument\ndzdy\nrepresentingthederivativeoftheoutputofthenetworkwithrespectto\ny\n;inthiscase,\nthefunctionreturnsthederivatives\n[\ndzdx\n,\ndzdw\n]=\nvl_nn\n<\nblock\n>\n(\nx\n,\nw\n,\ndzdy\n)\nwithrespectto\ntheinputdataandparameters.Thearrays\ndzdx\n,\ndzdy\nand\ndzdw\nhavethesamedimensions\nof\nx\n,\ny\nand\nw\nrespectively(seesection\n2.3\n).\ntfunctionsmayuseaslightlytsyntax,asneeded:manyfunctionscan\ntakeadditionaloptionalarguments,spasproperty-valuepairs;somedonothave\nparameters\nw\n(e.g.alinearunit);otherscantakemultipleinputsandparameters,in\nwhichcasetheremaybemorethanone\nx\n,\nw\n,\ndzdx\n,\ndzdy\nor\ndzdw\n.Seetherestofthechapter\nandMATLABinlinehelpfordetailsonthesyntax.\n1\nTherestofthechapterdescribestheblocksimplementedin\nMatConvNet\n,witha\nparticularfocusontheiranalyticalReferinsteadtoMATLABinlinehelpfor\nfurtherdetailsonthesyntax.\n4.1Convolution\nTheconvolutionalblockisimplementedbythefunction\nvl_nnconv\n.\ny\n=\nvl_nnconv\n(\nx\n,\nf\n,\nb\n)\ncom-\nputestheconvolutionoftheinputmap\nx\nwithabankof\nK\nmulti-dimensionals\nf\nand\nbiases\nb\n.Here\nx\n2\nR\nH\n\nW\n\nD\n;\nf\n2\nR\nH\n0\n\nW\n0\n\nD\n\nD\n00\n;\ny\n2\nR\nH\n00\n\nW\n00\n\nD\n00\n:\n1\nOtherpartsofthelibrarywillwrapthesefunctionsintoobjectswithaperfectlyuniforminterface;\nhowever,thelow-levelfunctionsaimatprovidingastraightforwardandobviousinterfaceevenifthismeans\nslightlyfromblocktoblock.\n25\n'b'26\nCHAPTER4.COMPUTATIONALBLOCKS\nFigure4.1:\nConvolution.\nTheillustratestheprocessofa1Dsignal\nx\nbya\n\nf\ntoobtainasignal\ny\n.Thehas\nH\n0\n=4elementsandisappliedwithastrideof\nS\nh\n=2samples.Thepurpleareasrepresentedpadding\nP\n\n=2and\nP\n+\n=3whichiszero-\nFiltersareappliedinasliding-windowmanneracrosstheinputsignal.Thesamplesof\nx\ninvolvedinthecalculationofasampleof\ny\nareshownwitharrow.Notethattherightmost\nsampleof\nx\nisneverprocessedbyanyapplicationduetothesamplingstep.Whilein\nthiscasethesampleisinthepaddedregion,thiscanhappenalsowithoutpadding.\nTheprocessofconvolvingasignalisillustrateding.\n4.1\nfora1Dslice.Formally,theoutput\nisgivenby\ny\ni\n00\nj\n00\nd\n00\n=\nb\nd\n00\n+\nH\n0\nX\ni\n0\n=1\nW\n0\nX\nj\n0\n=1\nD\nX\nd\n0\n=1\nf\ni\n0\nj\n0\nd\n\nx\ni\n00\n+\ni\n0\n\n1\n;j\n00\n+\nj\n0\n\n1\n;d\n0\n;d\n00\n:\nThecall\nvl_nnconv\n(\nx\n,\nf\n,[])\ndoesnotusethebiases.Notethatthefunctionworkswitharbi-\ntrarilysizedinputsand(asopposedto,forexample,squareimages).Seesection\n6.1\nfortechnicaldetails.\nPaddingandstride.\nvl_nnconv\nallowstospecifytop-bottom-left-rightpaddings\n(\nP\n\nh\n;P\n+\nh\n;P\n\nw\n;P\n+\nw\n)oftheinputarrayandsubsamplingstrides(\nS\nh\n;S\nw\n)oftheoutputarray:\ny\ni\n00\nj\n00\nd\n00\n=\nb\nd\n00\n+\nH\n0\nX\ni\n0\n=1\nW\n0\nX\nj\n0\n=1\nD\nX\nd\n0\n=1\nf\ni\n0\nj\n0\nd\n\nx\nS\nh\n(\ni\n00\n\n1)+\ni\n0\n\nP\n\nh\n;S\nw\n(\nj\n00\n\n1)+\nj\n0\n\nP\n\nw\n;d\n0\n;d\n00\n:\nInthisexpression,thearray\nx\nisimplicitlyextendedwithzerosasneeded.\nOutputsize.\nvl_nnconv\ncomputesonlythe\\valid"partoftheconvolution;i.e.itrequires\neachapplicationofatobefullycontainedintheinputsupport.Thesizeoftheoutput\niscomputedinsection\n5.2\nandisgivenby:\nH\n00\n=1+\n\nH\n\nH\n0\n+\nP\n\nh\n+\nP\n+\nh\nS\nh\n\n:\n'b'4.2.CONVOLUTIONTRANSPOSE(DECONVOLUTION)\n27\nNotethatthepaddedinputmustbeatleastaslargeasthe\nH\n+\nP\n\nh\n+\nP\n+\nh\n\nH\n0\n,\notherwiseanerroristhrown.\nReceptivesizeandgeometrictransformations.\nVeryoftenitisusefultogeo-\nmetricallyrelatetheindexesofthevariousarraytotheinputdata(usuallyimages)interms\nofcoordinatetransformationsandsizeofthereceptive(i.e.oftheimageregionthat\nanoutput).Thisisderivedinsection\n5.2\n.\nFullyconnectedlayers.\nInotherlibraries,\nfullyconnectedblocksorlayers\narelinear\nfunctionswhereeachoutputdimensiondependsonalltheinputdimensions.\nMatConvNet\ndoesnotdistinguishbetweenfullyconnectedlayersandconvolutionalblocks.Instead,the\nformerisaspecialcaseofthelatterobtainedwhentheoutputmap\ny\nhasdimensions\nW\n00\n=\nH\n00\n=1.Internally,\nvl_nnconv\nhandlesthiscasemoretlywhenpossible.\nFiltergroups.\nForadditionaly,\nvl_nnconv\nallowstogroupchannelsoftheinput\narray\nx\nandapplytsubsetsoftoeachgroup.Tousethisfeature,specify\nasinputabankof\nD\n00\n\nf\n2\nR\nH\n0\n\nW\n0\n\nD\n0\n\nD\n00\nsuchthat\nD\n0\ndividesthenumberofinput\ndimensions\nD\n.Thesearetreatedas\ng\n=\nD=D\n0\ngroups;thegroupisappliedto\ndimensions\nd\n=1\n;:::;D\n0\noftheinput\nx\n;thesecondgrouptodimensions\nd\n=\nD\n0\n+1\n;:::;\n2\nD\n0\nandsoon.Notethattheoutputisstillanarray\ny\n2\nR\nH\n00\n\nW\n00\n\nD\n00\n.\nAnapplicationofgroupingisimplementingtheKrizhevskyandHintonnetwork[\n7\n]which\nusestwosuchstreams.Anotherapplicationissumpooling;inthelattercase,onecanspecify\nD\ngroupsof\nD\n0\n=1dimensionalidenticalofvalue1(however,thisisconsiderably\nslowerthancallingthededicatedpoolingfunctionasgiveninsection\n4.3\n).\n4.2Convolutiontranspose(deconvolution)\nThe\nconvolutiontranspose\nblock(sometimesreferredtoas\\deconvolution")isthetranspose\noftheconvolutionblockdescribedinsection\n4.1\n.In\nMatConvNet\n,convolutiontranspose\nisimplementedbythefunction\nvl_nnconvt\n.\nInordertounderstandconvolutiontranspose,let:\nx\n2\nR\nH\n\nW\n\nD\n;\nf\n2\nR\nH\n0\n\nW\n0\n\nD\n\nD\n00\n;\ny\n2\nR\nH\n00\n\nW\n00\n\nD\n00\n;\nbetheinputtensor,andoutputtensors.Imagineoperatinginthereversedirection\nbyusingtherbank\nf\ntoconvolvetheoutput\ny\ntoobtaintheinput\nx\n,usingthe\ntionsgiveninsection\n4.1\nfortheconvolutionoperator;sinceconvolutionislinear,itcanbe\nexpressedasamatrix\nM\nsuchthatvec\nx\n=\nM\nvec\ny\n;convolutiontransposecomputesinstead\nvec\ny\n=\nM\n>\nvec\nx\n.Thisprocessisillustratedfora1Dslicein\n4.2\n.\nTherearetwoimportantapplicationsofconvolutiontranspose.Theonearethe\nsocalled\ndeconvolutionalnetworks\n[\n12\n]andothernetworkssuchasconvolutionaldecoders\nthatusethetransposeofaconvolution.Thesecondoneisimplementingdatainterpolation.\nInfact,astheconvolutionblocksupportsinputpaddingandoutputdownsampling,the\nconvolutiontransposeblocksupportsinputupsamplingandoutputcropping.\n'b'28\nCHAPTER4.COMPUTATIONALBLOCKS\nFigure4.2:\nConvolutiontranspose.\nTheillustratestheprocessofa1D\nsignal\nx\nbya\nf\ntoobtainasignal\ny\n.Theisappliedinasliding-window,ina\npatternthatisthetransposeof\n4.1\n.Thehas\nH\n0\n=4samplesintotal,althougheach\napplicationusestwoofthem(bluesquares)inacirculantmanner.Thepurpleareas\nrepresentcropswith\nC\n\n=2and\nC\n+\n=3whicharediscarded.Thesamplesof\nx\ninvolvedin\nthecalculationofasampleof\ny\nareshownwitharrow.Notethat,ntlyfrom\n4.1\n,\ntherearenosamplestotherightof\ny\nwhichareinvolvedinaconvolutionoperation.Thisis\nbecausethewidth\nH\n00\noftheoutput\ny\n,whichgiven\nH\n0\ncanbedeterminedupto\nU\nh\nsamples,\nisselectedtobethesmallestpossible.\nConvolutiontransposecanbeexpressedinclosedforminthefollowingratherunwieldy\nexpression(derivedinsection\n6.2\n):\ny\ni\n00\nj\n00\nd\n00\n=\nD\nX\nd\n0\n=1\nq\n(\nH\n0\n;S\nh\n)\nX\ni\n0\n=0\nq\n(\nW\n0\n;S\nw\n)\nX\nj\n0\n=0\nf\n1+\nS\nh\ni\n0\n+\nm\n(\ni\n00\n+\nP\n\nh\n;S\nh\n)\n;\n1+\nS\nw\nj\n0\n+\nm\n(\nj\n00\n+\nP\n\nw\n;S\nw\n)\n;d\n00\n;d\n0\n\nx\n1\n\ni\n0\n+\nq\n(\ni\n00\n+\nP\n\nh\n;S\nh\n)\n;\n1\n\nj\n0\n+\nq\n(\nj\n00\n+\nP\n\nw\n;S\nw\n)\n;d\n0\n(4.1)\nwhere\nm\n(\nk;S\n)=(\nk\n\n1)mod\nS;q\n(\nk;n\n)=\n\nk\n\n1\nS\n\n;\n(\nS\nh\n;S\nw\n)aretheverticalandhorizontal\ninputupsamplingfactors\n,(\nP\n\nh\n;P\n+\nh\n;P\n\nh\n;P\n+\nh\n)the\noutput\ncrops\n,and\nx\nand\nf\narezero-paddedasneededinthecalculation.Notealsothat\nk\nis\nstoredasaslice\nf\n:\n;\n:\n;k;\n:\nofthe4Dtensor\nf\n.\nTheheightoftheoutputarray\ny\nisgivenby\nH\n00\n=\nS\nh\n(\nH\n\n1)+\nH\n0\n\nP\n\nh\n\nP\n+\nh\n:\nAsimilarformulaholdstrueforthewidth.Theseformulasarederivedinsection\n5.3\nalong\nwithanexpressionforthereceptiveoftheoperator.\nWenowillustratetheactionofconvolutiontransposeinanexample(seealso\n4.2\n).\nConsidera1Dsliceintheverticaldirection,assumethatthecropparametersarezero,\n'b'4.3.SPATIALPOOLING\n29\nandthat\nS\nh\n>\n1.Considertheoutputsample\ny\ni\n00\nwheretheindex\ni\n00\nischosensuchthat\nS\nh\ndivides\ni\n00\n\n1;accordingto(\n4.1\n),thissampleisobtainedasaweightedsummationof\nx\ni\n00\n=S\nh\n;x\ni\n00\n=S\nh\n\n1\n;:::\n(notethattheorderisreversed).Theweightsaretheelements\nf\n1\n,\nf\nS\nh\n,\nf\n2\nS\nh\n;:::\nsubsampledwithastepof\nS\nh\n.Nowconsidercomputingtheelement\ny\ni\n00\n+1\n;dueto\ntheroundinginthequotientoperation\nq\n(\ni\n00\n;S\nh\n),thisoutputsampleisobtainedasaweighted\ncombinationofthesameelementsoftheinput\nx\nthatwereusedtocompute\ny\ni\n00\n;however,\ntheweightsarenowshiftedbyoneplacetotheright:\nf\n2\n,\nf\nS\nh\n+1\n,\nf\n2\nS\nh\n+1\n,\n:::\n.Thesame\nistruefor\ni\n00\n+2\n;i\n00\n+3\n;:::\nuntilwehit\ni\n00\n+\nS\nh\n.Herethecyclerestartsaftershifting\nx\nto\ntherightbyoneplace.ely,convolutiontransposeworksasan\ninterpolating\n.\n4.3Spatialpooling\nvl_nnpool\nimplementsmaxandsumpooling.The\nmaxpooling\noperatorcomputesthemax-\nimumresponseofeachfeaturechannelina\nH\n0\n\nW\n0\npatch\ny\ni\n00\nj\n00\nd\n=max\n1\n\ni\n0\n\nH\n0\n;\n1\n\nj\n0\n\nW\n0\nx\ni\n00\n+\ni\n\n1\n0\n;j\n00\n+\nj\n0\n\n1\n;d\n:\nresultinginanoutputofsize\ny\n2\nR\nH\n00\n\nW\n00\n\nD\n,similartotheconvolutionoperatorofsec-\ntion\n4.1\n.Sum-poolingcomputestheaverageofthevaluesinstead:\ny\ni\n00\nj\n00\nd\n=\n1\nW\n0\nH\n0\nX\n1\n\ni\n0\n\nH\n0\n;\n1\n\nj\n0\n\nW\n0\nx\ni\n00\n+\ni\n0\n\n1\n;j\n00\n+\nj\n0\n\n1\n;d\n:\nDetailedcalculationofthederivativesisprovidedinsection\n6.3\n.\nPaddingandstride.\nSimilartotheconvolutionoperatorofsection\n4.1\n,\nvl_nnpool\nsup-\nportspaddingtheinput;however,theistfrompaddingintheconvolutional\nblockaspoolingregionsstraddlingtheimageboundariesarecropped.Formaxpooling,\nthisisequivalenttoextendingtheinputdatawith\n\n;forsumpooling,thisissimilarto\npaddingwithzeros,butthenormalizationfactorattheboundariesissmallertoaccountfor\nthesmallerintegrationarea.\n4.4Activationfunctions\nMatConvNet\nsupportsthefollowingactivationfunctions:\n\nReLU.\nvl_nnrelu\ncomputesthe\nRedLinearUnit\n(ReLU):\ny\nijd\n=max\nf\n0\n;x\nijd\ng\n:\n\nSigmoid.\nvl_nnsigmoid\ncomputesthe\nsigmoid\n:\ny\nijd\n=\n\n(\nx\nijd\n)=\n1\n1+\ne\n\nx\nijd\n:\nSeesection\n6.4\nforimplementationdetails.\n'b'30\nCHAPTER4.COMPUTATIONALBLOCKS\n4.5Spatialbilinearresampling\nvl_nnbilinearsampler\nusesbilinearinterpolationtospatiallywarptheimageaccordingto\naninputtransformationgrid.Thisoperatorworkswithaninputimage\nx\n,agrid\ng\n,andan\noutputimage\ny\nasfollows:\nx\n2\nR\nH\n\nW\n\nC\n;\ng\n2\n[\n\n1\n;\n1]\n2\n\nH\n0\n\nW\n0\n;\ny\n2\nR\nH\n0\n\nW\n0\n\nC\n:\nThesametransformationisappliedtoallthefeatureschannelsintheinput,asfollows:\ny\ni\n00\nj\n00\nc\n=\nH\nX\ni\n=1\nW\nX\nj\n=1\nx\nijc\nmax\nf\n0\n;\n1\nj\n\nv\ng\n1\ni\n00\nj\n00\n+\n\nv\n\ni\njg\nmax\nf\n0\n;\n1\nj\n\nu\ng\n2\ni\n00\nj\n00\n+\n\nu\n\nj\njg\n;\n(4.2)\nwhere,foreachfeaturechannel\nc\n,theoutput\ny\ni\n00\nj\n00\nc\natthelocation(\ni\n00\n;j\n00\n),isaweightedsum\noftheinputvalues\nx\nijc\nintheneighborhoodoflocation(\ng\n1\ni\n00\nj\n00\n;g\n2\ni\n00\nj\n00\n).Theweights,asgiven\nin(\n4.2\n),correspondtoperformingbilinearinterpolation.Furthermore,thegridcoordinates\nareexpressednotinpixels,butrelativetoareferenceframethatextendsfrom\n\n1to1for\nallspatialdimensionsoftheinputimage;thisisgivenbychoosingthecotsas:\n\nv\n=\nH\n\n1\n2\n;\nv\n=\n\nH\n+1\n2\n;\nu\n=\nW\n\n1\n2\n;\nu\n=\n\nW\n+1\n2\n:\nSeesection\n6.5\nforimplementationdetails.\n4.6Normalization\n4.6.1Localresponsenormalization(LRN)\nvl_nnnormalize\nimplementstheLocalResponseNormalization(LRN)operator.Thisoper-\natorisappliedindependentlyateachspatiallocationandtogroupsoffeaturechannelsas\nfollows:\ny\nijk\n=\nx\nijk\n0\n@\n\n+\n\nX\nt\n2\nG\n(\nk\n)\nx\n2\nijt\n1\nA\n\n\n;\nwhere,foreachoutputchannel\nk\n,\nG\n(\nk\n)\nf\n1\n;\n2\n;:::;D\ng\nisacorrespondingsubsetofinput\nchannels.Notethatinput\nx\nandoutput\ny\nhavethesamedimensions.Notealsothatthe\noperatorisapplieduniformlyatallspatiallocations.\nSeesection\n6.6.1\nforimplementationdetails.\n4.6.2Batchnormalization\nvl_nnbnorm\nimplementsbatchnormalization[\n4\n].Batchnormalizationissomewhatt\nfromotherneuralnetworkblocksinthatitperformscomputationacrossimages/feature\nmapsinabatch(whereasmostblocksprocesstimages/featuremapsindividually).\n'b'4.7.CATEGORICALLOSSES\n31\ny\n=\nvl_nnbnorm\n(\nx\n,\nw\n,\nb\n)\nnormalizeseachchannelofthefeaturemap\nx\naveragingoverspatial\nlocationsandbatchinstances.Let\nT\nbethebatchsize;then\nx\n;\ny\n2\nR\nH\n\nW\n\nK\n\nT\n;\nw\n2\nR\nK\n;\nb\n2\nR\nK\n:\nNotethatinthiscasetheinputandoutputarraysareexplicitlytreatedas4Dtensorsin\nordertoworkwithabatchoffeaturemaps.Thetensors\nw\nand\nb\ncomponent-wise\nmultiplicativeandadditiveconstants.Theoutputfeaturemapisgivenby\ny\nijkt\n=\nw\nk\nx\nijkt\n\n\nk\np\n\n2\nk\n+\n\n+\nb\nk\n;\nk\n=\n1\nHWT\nH\nX\ni\n=1\nW\nX\nj\n=1\nT\nX\nt\n=1\nx\nijkt\n;\n2\nk\n=\n1\nHWT\nH\nX\ni\n=1\nW\nX\nj\n=1\nT\nX\nt\n=1\n(\nx\nijkt\n\n\nk\n)\n2\n:\nSeesection\n6.6.2\nforimplementationdetails.\n4.6.3Spatialnormalization\nvl_nnspnorm\nimplementsspatialnormalization.Thespatialnormalizationoperatoractson\ntfeaturechannelsindependentlyandrescaleseachinputfeaturebytheenergyofthe\nfeaturesinalocalneighbourhood.First,theenergyofthefeaturesinaneighbourhood\nW\n0\n\nH\n0\nisevaluated\nn\n2\ni\n00\nj\n00\nd\n=\n1\nW\n0\nH\n0\nX\n1\n\ni\n0\n\nH\n0\n;\n1\n\nj\n0\n\nW\n0\nx\n2\ni\n00\n+\ni\n0\n\n1\n\nH\n0\n\n1\n2\nc\n;j\n00\n+\nj\n0\n\n1\n\nW\n0\n\n1\n2\nc\n;d\n:\nInpractice,thefactor1\n=W\n0\nH\n0\nisadjustedattheboundariestoaccountforthefactthat\nneighborsmustbecropped.Thenthisisusedtonormalizetheinput:\ny\ni\n00\nj\n00\nd\n=\n1\n(1+\nn\n2\ni\n00\nj\n00\nd\n)\n\nx\ni\n00\nj\n00\nd\n:\nSeesection\n6.6.3\nforimplementationdetails.\n4.6.4Softmax\nvl_nnsoftmax\ncomputesthesoftmaxoperator:\ny\nijk\n=\ne\nx\nijk\nP\nD\nt\n=1\ne\nx\nijt\n:\nNotethattheoperatorisappliedacrossfeaturechannelsandinaconvolutionalmanner\natallspatiallocations.Softmaxcanbeseenasthecombinationofanactivationfunction\n(exponential)andanormalizationoperator.Seesection\n6.6.4\nforimplementationdetails.\n4.7Categoricallosses\nThepurposeofacategoricallossfunction\n`\n(\nx\n;\nc\n)istocompareaprediction\nx\ntoaground\ntruthclasslabel\nc\n.Asintherestof\nMatConvNet\n,thelossistreatedasaconvolutional\n'b'32\nCHAPTER4.COMPUTATIONALBLOCKS\noperator,inthesensethatthelossisevaluatedindependentlyateachspatiallocation.How-\never,thecontributionoftsamplesaresummedtogether(possiblyafterweighting)\nandtheoutputofthelossisascalar.Section\n4.7.1\nlossesusefulformulti-class\nandthesection\n4.7.2\nlossesusefulforbinaryattributeprediction.Furthertechnicaldetails\nareinsection\n6.7\n.\nvl_nnloss\nimplementsthefollowingallofthese.\n4.7.1losses\nlossesdecomposeadditivelyasfollows:\n`\n(\nx\n;\nc\n)=\nX\nijn\nw\nij\n1\nn\n`\n(\nx\nij\n:\nn\n;\nc\nij\n:\nn\n)\n:\n(4.3)\nHere\nx\n2\nR\nH\n\nW\n\nC\n\nN\nand\nc\n2f\n1\n;:::;C\ng\nH\n\nW\n\n1\n\nN\n,suchthattheslice\nx\nij\n:\nn\nrepresentavector\nof\nC\nclassscoresandand\nc\nij\n1\nn\nisthegroundtruthclasslabel.The\n`\ninstanceWeights\n`\noption\ncanbeusedtospecifythetensor\nw\nofweights,whichareotherwisesettoallones;\nw\nhas\nthesamedimensionas\nc\n.\nUnlessotherwisenoted,wedroptheotherindicesanddenoteby\nx\nand\nc\ntheslice\nx\nij\n:\nn\nandthescalar\nc\nij\n1\nn\n.\nvl_nnloss\nautomaticallyskipsallsamplessuchthat\nc\n=0,whichcan\nbeusedasan\\ignore"label.\nerror.\nTheerroriszeroifclass\nc\nisassignedthelargestscore\nandzerootherwise:\n`\n(\nx\n;c\n)=\n1\n\nc\n6\n=argmax\nk\nx\nc\n\n:\n(4.4)\nTiesarebrokenrandomly.\nTop-\nK\nerror.\nThetop-\nK\nerroriszeroifclass\nc\niswithinthe\ntop\nK\nrankedscores:\n`\n(\nx\n;c\n)=\n1\n[\njf\nk\n:\nx\nk\n\nx\nc\ngj\nK\n]\n:\n(4.5)\nTheerroristhesameasthetop-1error.\nLoglossornegativeposteriorlog-probability.\nInthiscase,\nx\nisinterpretedasavector\nofposteriorprobabilities\np\n(\nk\n)=\nx\nk\n;k\n=1\n;:::;C\noverthe\nC\nclasses.Thelossisthenegative\nlog-probabilityofthegroundtruthclass:\n`\n(\nx\n;c\n)=\n\nlog\nx\nc\n:\n(4.6)\nNotethatthismakestheimplicitassumption\nx\n\n0\n;\nP\nk\nx\nk\n=1.Notealsothat,unless\nx\nc\n>\n0,thelossisForthesereasons,\nx\nisusuallytheoutputofablocksuchas\nsoftmaxthatcanguaranteetheseconditions.However,thecompositionofthenaivelogloss\nandsoftmaxisnumericallyunstable.Thusthisisimplementedasaspecialcasebelow.\nGenerally,forsuchalosstomakesense,thescore\nx\nc\nshouldbesomehowincompetition\nwiththeotherscores\nx\nk\n;k\n6\n=\nc\n.Ifthisisnotthecase,minimizing(\n4.6\n)cantriviallybe\nachievedbymaxingall\nx\nk\nlarge,whereastheintendedisthat\nx\nc\nshouldbelargecom-\nparedtothe\nx\nk\n;k\n6\n=\nc\n.Thesoftmaxblockmakesthescorecompetethroughthenormalization\nfactor.\n'b'4.7.CATEGORICALLOSSES\n33\nSoftmaxlog-lossormultinomiallogisticloss.\nThislosscombinesthesoftmaxblock\nandthelog-lossblockintoasingleblock:\n`\n(\nx\n;c\n)=\n\nlog\ne\nx\nc\nP\nC\nk\n=1\ne\nx\nk\n=\n\nx\nc\n+log\nC\nX\nk\n=1\ne\nx\nk\n:\n(4.7)\nCombiningthetwoblocksexplicitlyisrequiredfornumericalstability.Notethat,bycombin-\ningthelog-losswithsoftmax,thislossautomaticallymakesthescorecompete:\n`\n(\nbx;c\n)\n\n0\nwhen\nx\nc\n\nP\nk\n6\n=\nc\nx\nk\n.\nThislossisimplementedalsointhe\ndeprecated\nfunction\nvl_softmaxloss\n.\nMulti-classhingeloss.\nThemulti-classlogisticlossisgivenby\n`\n(\nx\n;c\n)=max\nf\n0\n;\n1\n\nx\nc\ng\n:\n(4.8)\nNotethat\n`\n(\nx\n;c\n)=0\n,\nx\nc\n\n1.This,justasforthelog-lossabove,thislossdoesnot\nautomaticallymakethescorecompetes.Inordertodothat,thelossisusuallyprecededby\ntheblock:\ny\nc\n=\nx\nc\n\nmax\nk\n6\n=\nc\nx\nk\n:\nHence\ny\nc\nrepresentthe\ncemargin\nbetweenclass\nc\nandtheotherclasses\nk\n6\n=\nc\n.Just\nlikesoftmaxlog-losscombinessoftmaxandloss,thenextlosscombinesmargincomputation\nandhingeloss.\nStructuredmulti-classhingeloss.\nThestructuredmulti-classlogisticloss,alsoknowas\nCrammer-Singerloss,combinesthemulti-classhingelosswithablockcomputingthescore\nmargin:\n`\n(\nx\n;c\n)=max\n\n0\n;\n1\n\nx\nc\n+max\nk\n6\n=\nc\nx\nk\n\n:\n(4.9)\n4.7.2Attributelosses\nAttributelossesaresimilartolosses,butinthiscaseclassesarenotmutually\nexclusive;theyare,instead,binaryattributes.Attributelossesdecomposeadditivelyas\nfollows:\n`\n(\nx\n;\nc\n)=\nX\nijkn\nw\nijkn\n`\n(\nx\nijkn\n;\nc\nijkn\n)\n:\n(4.10)\nHere\nx\n2\nR\nH\n\nW\n\nC\n\nN\nand\nc\n2\n1\n;\n+1\ng\nH\n\nW\n\nC\n\nN\n,suchthatthescalar\nx\nijkn\nrepresent\nathatattribute\nk\nisonand\nc\nij\n1\nn\nisthegroundtruthattributelabel.The\n`\ninstanceWeights\n`\noptioncanbeusedtospecifythetensor\nw\nofweights,whichareoth-\nerwisesettoallones;\nw\nhasthesamedimensionas\nc\n.\nUnlessotherwisenoted,wedroptheotherindicesanddenoteby\nx\nand\nc\nthescalars\nx\nijkn\nand\nc\nijkn\n.Asbefore,sampleswith\nc\n=0areskipped.\n'b'34\nCHAPTER4.COMPUTATIONALBLOCKS\nBinaryerror.\nThislossiszeroonlyifthesignof\nx\n\n\nagreeswiththegroundtruthlabel\nc\n:\n`\n(\nx;c\nj\n\n)=\n1\n[sign(\nx\n\n\n)\n6\n=\nc\n]\n:\n(4.11)\nHere\n\nisathreshold,oftensettozero.\nBinarylog-loss.\nThisisthesameasthemulti-classlog-lossbutforbinaryattributes.\nNamely,thistime\nx\nk\n2\n[0\n;\n1]isinterpretedastheprobabilitythatattribute\nk\nison:\n`\n(\nx;c\n)=\n(\n\nlog\nx;c\n=+1\n;\n\nlog(1\n\nx\n)\n;c\n=\n\n1\n;\n(4.12)\n=\n\nlog\n\nc\n\nx\n\n1\n2\n\n+\n1\n2\n\n:\n(4.13)\nSimilarlytothemulti-classlogloss,theassumption\nx\n2\n[0\n;\n1]mustbeenforcedbytheblock\ncomputing\nx\n.\nBinarylogisticloss.\nThisisthesameasthemulti-classlogisticloss,butthistime\nx=\n2\nrepresentsthethattheattributeisonand\n\nx=\n2thatitisThisisobtained\nbyusingthelogisticfunction\n\n(\nx\n)\n`\n(\nx;c\n)=\n\nlog\n\n(\ncx\n)=\n\nlog\n1\n1+\ne\n\ncx\n=\n\nlog\ne\ncx\n2\ne\ncx\n2\n+\ne\n\ncx\n2\n:\n(4.14)\nBinaryhingeloss.\nThisisthesameasthestructuredmulti-classhingelossbutforbinary\nattributes:\n`\n(\nx;c\n)=max\nf\n0\n;\n1\n\ncx\ng\n:\n(4.15)\nThereisarelationshipbetweenthehingelossandthestructuredmulti-classhingelosswhich\nisanalogoustotherelationshipbetweenbinarylogisticlossandmulti-classlogisticloss.\nNamely,thehingelosscanberewrittenas:\n`\n(\nx;c\n)=max\n\n0\n;\n1\n\ncx\n2\n+max\nk\n6\n=\nc\nkx\n2\n\nHencethehingelossisthesameasthestructuremulti-classhingelossfor\nC\n=2classes,\nwhere\nx=\n2isthescoreassociatedtoclass\nc\n=1and\n\nx=\n2thescoreassociatedtoclass\nc\n=\n\n1.\n4.8Comparisons\n4.8.1\np\n-distance\nThe\nvl_nnpdist\nfunctioncomputesthe\np\n-distancebetweenthevectorsintheinputdata\nx\nandatarget\n\nx\n:\ny\nij\n=\n \nX\nd\nj\nx\nijd\n\n\nx\nijd\nj\np\n!\n1\np\n'b"4.8.COMPARISONS\n35\nNotethatthisoperatorisappliedconvolutionally,i.e.ateachspatiallocation\nij\noneextracts\nandcomparesvectors\nx\nij\n:\n.Byspecifyingtheoption\n'\nnoRoot\n'\n,\ntrue\nitispossibletocompute\navariantomittingtheroot:\ny\nij\n=\nX\nd\nj\nx\nijd\n\n\nx\nijd\nj\np\n;p>\n0\n:\nSeesection\n6.8.1\nforimplementationdetails.\n"b''b'Chapter5\nGeometry\nThischapterlooksatthegeometryoftheCNNinput-outputmapping.\n5.1Preliminaries\nInthissectionweareinterestedinunderstandinghowcomponentsinaCNNdependon\ncomponentsinthelayersbeforeit,andinparticularoncomponentsoftheinput.Since\nCNNscanincorporateblocksthatperformcomplexoperations,suchasforexamplecropping\ntheirinputsbasedondata-dependentterms(e.g.FastR-CNN),thisinformationisgenerally\navailableonlyat\\runtime"andcannotbeuniquelydeterminedgivenonlythestructure\nofthenetwork.Furthermore,blockscanimplementcomplexoperationsthatareto\ncharacteriseinsimpleterms.Therefore,theanalysiswillbenecessarilylimitedinscope.\nWeconsiderblockssuchasconvolutionsforwhichonecandeterministicallyestablish\ndependencychainsbetweennetworkcomponents.Wealsoassumethatalltheinputs\nx\nand\noutputs\ny\nareintheusualformofspatialmaps,andthereforeindexedas\nx\ni;j;d;k\nwhere\ni;j\narespatialcoordinates.\nConsideralayer\ny\n=\nf\n(\nx\n).Weareinterestedinestablishingwhichcomponentsof\nx\nwhichcomponentsof\ny\n.Wealsoassumethatthisrelationcanbeexpressedin\ntermsofaslidingrectangularwindow,called\nreceptive\n.Thismeansthattheoutput\ncomponent\ny\ni\n00\n;j\n00\ndependsonlyontheinputcomponents\nx\ni;j\nwhere(\ni;j\n)\n2\n\ni\n00\n;j\n00\n)(notethat\nfeaturechannelsareimplicitlycoalescedinthisdiscussion).Theset\ni\n00\n;j\n00\n)isarectangle\nasfollows:\ni\n2\n\nh\n(\ni\n00\n\n1)+\n\nh\n+\n\n\n\nh\n\n1\n2\n;\n\nh\n\n1\n2\n\n(5.1)\nj\n2\n\nv\n(\nj\n00\n\n1)+\n\nv\n+\n\n\n\nv\n\n1\n2\n;\n\nv\n\n1\n2\n\n(5.2)\nwhere(\n\nh\n;\nv\n)isthe\nstride\n,(\n\nh\n;\nv\n)theand\nh\n;\n\nv\n)the\nreceptivesize\n.\n37\n'b'38\nCHAPTER5.GEOMETRY\n5.2Simple\nWenowcomputethereceptivegeometry(\n\nh\n;\nv\n;\nh\n;\nv\n;\n\nh\n;\n\nv\n)forthemostcommon\noperators,namelyWeconsiderinparticular\nsimple\nthatarecharacterisedby\nanintegersize,stride,andpadding.\nIttoreasonin1D.Let\nH\n0\nbettheverticaldimension,\nS\nh\nthesubampling\nstride,and\nP\n\nh\nand\nP\n+\nh\ntheamountofzeropaddingappliedtothetopandthebottomofthe\ninput\nx\n.Herethevalue\ny\ni\n00\ndependsonthesamples:\nx\ni\n:\ni\n2\n[1\n;H\n0\n]+\nS\nh\n(\ni\n00\n\n1)\n\nP\n\nh\n=\n\n\nH\n0\n\n1\n2\n;\nH\n0\n\n1\n2\n\n+\nS\nh\n(\ni\n00\n\n1)\n\nP\n\nh\n+\nH\n0\n+1\n2\n:\nHence\n\nh\n=\nS\nh\n;\nh\n=\nH\n0\n+1\n2\n\nP\n\nh\n;\n\nh\n=\nH\n0\n:\nAsimilarrelationholdsforthehorizontaldirection.\nNotethatmanyblocks(e.g.maxpooling,LNR,ReLU,mostlossfunctionsetc.)havea\nereceptivegeometry.Forexample,ReLUcanbeconsidereda1\n\n1such\nthat\nH\n=\nS\nh\n=1and\nP\n\nh\n=\nP\n+\nh\n=0.Notethatinthiscase\n\nh\n=1,\n\nh\n=1and\nh\n=1.\nInadditiontocomputingthereceptivegeometry,weareofteninterestedindetermin-\ningthesizesofthearrays\nx\nand\ny\nthroughoutthearchitecture.Inthecaseofandonce\nmorereasoningfora1Dslice,wenoticethat\ny\n00\ni\ncanbeobtainedfor\ni\n00\n=1\n;\n2\n;:::;H\n00\nwhere\nH\n00\nisthelargestvalueof\ni\n00\nbeforethereceptivefallsoutside\nx\n(includingpadding).If\nH\nistheheightoftheinputarray\nx\n,wegetthecondition\nH\n0\n+\nS\nh\n(\nH\n00\n\n1)\n\nP\n\nh\n\nH\n+\nP\n+\nh\n:\nHence\nH\n00\n=\n\nH\n\nH\n0\n+\nP\n\nh\n+\nP\n+\nh\nS\nh\n\n+1\n:\n(5.3)\n5.2.1Poolingin\nMatConvNettreatspoolingoperatorslikeusingtherulesabove.Inthelibrary\nthisisdoneslightlytly,creatingsomeincompatibilities.Intheircase,thepooling\nwindowisallowedtoshiftenoughsuchthatthelastapplicationalwaysincludesthelastpixel\noftheinput.Ifthestrideisgreaterthanone,thismeansthatthelastapplicationofthe\npoolingwindowcanbepartiallyoutsidetheinputboundariesevenifpaddingis\nzero.\nMoreformally,if\nH\n0\nisthepoolsizeand\nH\nthesizeofthesignal,thelastapplicationof\nthepoolingwindowhasindex\ni\n00\n=\nH\n00\nsuchthat\nS\nh\n(\ni\n00\n\n1)+\nH\n0\n\n\ni\n00\n=\nH\n00\n\nH\n,\nH\n00\n=\n\nH\n\nH\n0\nS\nh\n\n+1\n:\nIfthereispadding,thesamelogicappliesafterpaddingtheinputimage,suchthattheoutput\nhasheight:\nH\n00\n=\n\nH\n\nH\n0\n+\nP\n\nh\n+\nP\n+\nh\nS\nh\n\n+1\n:\n'b'5.2.SIMPLEFILTERS\n39\nThisisthesameformulaasforabovebutwiththeceilinsteadoforoperator.Note\nthatinpractice\nP\n\nh\n=\nP\n+\nh\n=\nP\nh\nsincedoesnotsupportasymmetricpadding.\nUnfortunately,itgetsmorecomplicated.Usingtheformulaabove,itcanhappenthat\nthelastpaddingapplicationiscompletelyoutsidetheinputimageandtriestoavoid\nit.Thisrequires\nS\n(\ni\n00\n\n1)\n\nP\n\nh\n+1\n\n\ni\n00\n=\nH\n00\n\nH\n,\nH\n00\n\nH\n\n1+\nP\n\nh\nS\nh\n+1\n:\n(5.4)\nUsingthefactthatforintegers\na;b\n,onehas\nd\na=b\ne\n=\nb\n(\na\n+\nb\n\n1)\n=b\nc\n,wecanrewritethe\nexpressionfor\nH\n00\nasfollows\nH\n00\n=\n\nH\n\nH\n0\n+\nP\n\nh\n+\nP\n+\nh\nS\nh\n\n+1=\n\nH\n\n1+\nP\n\nh\nS\nh\n+\nP\n+\nh\n+\nS\nh\n\nH\n0\nS\nh\n\n+1\n:\nHenceif\nP\n+\nh\n+\nS\nh\n\nH\n0\nthenthesecondtermislessthanzeroand(\n5.4\n)isIn\npractice,assumesthat\nP\n+\nh\n;P\n\nh\n\nH\n0\n\n1,asotherwisetheapplicationfalls\nentirelyinthepaddedregion.Hence,wecanupperboundthesecondterm:\nP\n+\nh\n+\nS\nh\n\nH\n0\nS\nh\n\nS\nh\n\n1\nS\nh\n\n1\n:\nWeconcludethat,foranychoicesof\nP\n+\nh\nand\nS\nh\nallowedbytheformulaabovemay\nviolateconstraint(\n5.4\n)byatmostoneunit.hasaspecialprovisionforthatandlowers\nH\n00\nbyonewhenneeded.Furthermore,weseethatif\nP\n+\nh\n=0\nand\nS\nh\n\nH\n0\n(whichisoften\nthecaseandmaybeassumedbythentheequationisalsoandskips\nthecheck.\nNext,weMatConvNetequivalentsfortheseparameters.Assumethatapplies\nasymmetricpadding\nP\nh\n.TheninMatConvNet\nP\n\nh\n=\nP\nh\ntoalignthetoppartoftheoutput\nsignal.Tomatchthelastsampleofthelastapplicationhastobeonortothe\nrightofthelastpixel:\nS\nh\n0\nB\nB\nB\n@\n\nH\n\nH\n0\n+\nP\n\nh\n+\nP\n+\nh\nS\nh\n+1\n\n|\n{z\n}\nMatConvNetrightmostpoolingindex\n\n1\n1\nC\nC\nC\nA\n+\nH\n0\n|\n{z\n}\nMatConvNetrightmostpooledinputsample\n\nH\n+2\nP\n\nh\n|\n{z\n}\nrightmostinputsamplewithpadding\n:\nRearranging\n\nH\n\nH\n0\n+\nP\n\nh\n+\nP\n+\nh\nS\nh\n\n\nH\n\nH\n0\n+2\nP\n\nh\nS\nh\nUsing\nb\na=b\nc\n=\nd\n(\na\n\nb\n+1)\n=b\ne\nwegetthe\nequivalent\ncondition:\n\nH\n\nH\n0\n+2\nP\n\nh\nS\nh\n+\nP\n+\nh\n\nP\n\nh\n\nS\nh\n+1\nS\nh\n\n\nH\n\nH\n0\n+2\nP\n\nh\nS\nh\n'b'40\nCHAPTER5.GEOMETRY\nRemovingtheceiloperatorlowerboundstheleft-handsideoftheequationandproducesthe\n\ncondition\nP\n+\nh\n\nP\n\nh\n+\nS\nh\n\n1\n:\nAsbefore,thismaystillbetoomuchpadding,causingthelastpoolwindowapplicationto\nbeentirelyintherightmostpaddedarea.MatConvNetplacestherestriction\nP\n+\nh\n\nH\n0\n\n1,\nsothat\nP\n+\nh\n=min\nf\nP\n\nh\n+\nS\nh\n\n1\n;H\n0\n\n1\ng\n:\nForexample,apoolingregionofwidth\nH\n0\n=3sampleswithastrideof\nS\nh\n=1samplesand\nnullpadding\nP\n\nh\n=0,wouldresultinarightMatConvNetpaddingof\nP\n+\nh\n=1.\n5.3Convolutiontranspose\nTheconvolutiontransposeblockissimilartoasimplebutsomewhatmorecomplex.\nRecallthatconvolutiontranspose(section\n6.2\n)isthetransposeoftheconvolutionoperator,\nwhichinturnisaReasoningfora1Dslice,let\nx\ni\nbetheinputtotheconvolution\ntransposeblockand\ny\ni\n00\nitsoutput.Furthermorelet\nU\nh\n,\nC\n\nh\n,\nC\n+\nh\nand\nH\n0\nbetheupsampling\nfactor,topandbottomcrops,andheight,respectively.\nIfwelookattheconvolutiontransposebackward,fromtheoutputtotheinput(seealso\n\n4.2\n),thedatadependenciesarethesameasfortheconvolutionoperator,studiedin\nsection\n5.2\n.Hencethereisaninteractionbetween\nx\ni\nand\ny\ni\n00\nonlyif\n1+\nU\nh\n(\ni\n\n1)\n\nC\n\nh\n\ni\n00\n\nH\n0\n+\nU\nh\n(\ni\n\n1)\n\nC\n\nh\n(5.5)\nwherecroppingbecomespaddingandupsamplingbecomesdownsampling.Turningthis\nrelationaround,wethat\n\ni\n00\n+\nC\n\nh\n\nH\n0\nS\nh\n\n+1\n\ni\n\n\ni\n00\n+\nC\n\nh\n\n1\nS\nh\n\n+1\n:\nNotethat,duetorounding,itisnotpossibletoexpressthissettightlyintheformoutlined\nabove.Wecanhoweverrelaxthesetworelations(henceobtainingaslightlylargerreceptive\nandconcludethat\n\nh\n=\n1\nU\nh\n;\nh\n=\n2\nC\n\nh\n\nH\n0\n+1\n2\nU\nh\n+1\n;\n\nh\n=\nH\n0\n\n1\nU\nh\n+1\n:\nNext,wewanttodeterminetheheight\nH\n00\noftheoutput\ny\nofconvolutiontransposeas\nafunctionoftheheigh\nH\noftheinput\nx\nandtheotherparameters.Swappinginputand\noutputin(\n5.3\n)resultsintheconstraint:\nH\n=1+\n\nH\n00\n\nH\n0\n+\nC\n\nh\n+\nC\n+\nh\nU\nh\n\n:\nIf\nH\nisnowgivenasinput,itisnotpossibletorecover\nH\n00\nuniquelyfromthisexpression;\ninstead,allthefollowingvaluesarepossible\nS\nh\n(\nH\n\n1)+\nH\n0\n\nC\n\nh\n\nC\n+\nh\n\nH\n00\n<S\nh\nH\n+\nH\n0\n\nC\n\nh\n\nC\n+\nh\n:\n'b'5.4.TRANSPOSINGRECEPTIVEFIELDS\n41\nThisisduetothefactthat\nU\nh\nactsasadownsamplingfactorinthestandardconvolution\ndirectionandsomeofthesamplestotherightoftheconvolutioninput\ny\nmaybeignoredby\nthe(seealso\n4.1\nand\n4.2\n).\nSincetheheightof\ny\nisthendeterminedupto\nS\nh\nsamples,andsincetheextrasamples\nwouldbeignoredbythecomputationandstayzero,wechoosethetighterandset\nH\n00\n=\nU\nh\n(\nH\n\n1)+\nH\n0\n\nC\n\nh\n\nC\n+\nh\n:\n5.4Transposingreceptive\nSupposewehavedeterminedthatalater\ny\n=\nf\n(\nx\n)hasareceptivetransformation\n(\n\nh\n;\nh\n;\n\nh\n)(alongonespatialslice).Nowsupposewearegivenablock\nx\n=\ng\n(\ny\n)which\nisthe\\transpose"of\nf\n,justliketheconvolutiontransposelayeristhetransposeofthe\nconvolutionlayer.Bythis,wemeanthat,if\ny\ni\n00\ndependson\nx\ni\ndueto\nf\n,then\nx\ni\ndependson\ny\ni\n00\ndueto\ng\n.\nNotethat,byofreceptive\nf\nrelatestheinputsandoutputsindexpairs\n(\ni;i\n00\n)givenby(\n5.1\n),whichcanberewrittenas\n\n\nh\n\n1\n2\n\ni\n\n\nh\n(\ni\n00\n\n1)\n\n\nh\n\n\nh\n\n1\n2\n:\nAsimplemanipulationofthisexpressionresultsintheequivalentexpression:\n\n\nh\n+\n\nh\n\n1)\n\nh\n\n1\n2\n\ni\n00\n\n1\n\nh\n(\ni\n\n1)\n\n1+\n\nh\n\n\nh\n\nh\n\n\nh\n+\n\nh\n\n1)\n\nh\n\n1\n2\n\nh\n:\nHence,inthereversedirection,thiscorrespondstoaRFtransformation\n^\n\nh\n=\n1\n\nh\n;\n^\n\nh\n=\n1+\n\nh\n\n\nh\n\nh\n;\n^\n\nh\n=\n\nh\n+\n\nh\n\n1\n\nh\n:\nExample1.\nForconvolution,wehavefoundtheparameters:\n\nh\n=\nS\nh\n;\nh\n=\nH\n0\n+1\n2\n\nP\n\nh\n;\n\nh\n=\nH\n0\n:\nUsingtheformulasjustfound,wecanobtaintheRFtransformationforconvolutiontranspose:\n^\n\nh\n=\n1\n\nh\n=\n1\nS\nh\n;\n^\n\nh\n=\n1+\nS\nh\n\n(\nH\n0\n+1)\n=\n2+\nP\n\nh\nS\nh\n=\nP\n\nh\n\nH\n0\n=\n2+1\n=\n2\nS\nh\n+1=\n2\nP\n\nh\n\nH\n0\n+1\nS\nh\n+1\n;\n^\n\nh\n=\nH\n0\n+\nS\nh\n\n1\nS\nh\n=\nH\n0\n\n1\nS\nh\n+1\n:\nHenceweagaintheformulasobtainedinsection\n5.3\n.\n'b'42\nCHAPTER5.GEOMETRY\n5.5Composingreceptive\nConsidernowthecompositionoftwolayers\nh\n=\ng\n\nf\nwithreceptive(\n\nf\n;\nf\n;\n\nf\n)and\n(\n\ng\n;\ng\n;\n\ng\n)(onceagainweconsideronlya1Dsliceintheverticaldirection,thehorizontal\nonebeingthesame).Thegoalistocomputethereceptiveof\nh\n.\nTodoso,pickasample\ni\ng\ninthedomainof\ng\n.Theandlastsample\ni\nf\ninthedomain\nof\nf\nto\ni\ng\naregivenby:\ni\nf\n=\n\nf\n(\ni\ng\n\n1)+\n\nf\n\n\nf\n\n1\n2\n:\nLikewise,theandlastsample\ni\ng\ntoagivenoutputsample\ni\nh\naregivenby\ni\ng\n=\n\ng\n(\ni\nh\n\n1)+\n\ng\n\n\ng\n\n1\n2\n:\nSubstitutingonerelationintotheother,weseethattheandlastsample\ni\nf\ninthedomain\nof\ng\n\nf\nto\ni\nh\nare:\ni\nf\n=\n\nf\n\n\ng\n(\ni\nh\n\n1)+\n\ng\n\n\ng\n\n1\n2\n\n1\n\n+\n\nf\n\n\nf\n\n1\n2\n=\n\nf\n\ng\n(\ni\nh\n\n1)+\n\nf\n\ng\n\n1+\n\nf\n\n\nf\n\ng\n\n1)+\nf\n\n1\n2\n:\nWeconcludethat\n\nh\n=\n\nf\n\ng\n;\nh\n=\n\nf\n(\n\ng\n\n1)+\n\nf\n;\n\nh\n=\n\nf\n\ng\n\n1)+\nf\n:\n5.6Overlayingreceptive\nConsidernowthecombination\nh\n(\nf\n(\nx\n1\n)\n;g\n(\nx\n2\n))wherethedomainsof\nf\nand\ng\narethesame.\nGiventheruleabove,itispossibletocomputehoweachoutputsample\ni\nh\ndependsoneach\ninputsample\ni\nf\nthrough\nf\nandoneachinputsample\ni\ng\nthrough\ng\n.Supposethatthisgives\nreceptives(\n\nhf\n;\nhf\n;\n\nhf\n)and(\n\nhg\n;\nhg\n;\n\nhg\n)respectively.Nowassumethatthedomain\nof\nf\nand\ng\ncoincide,i.e.\nx\n=\nx\n1\n=\nx\n2\n.Thegoalistodeterminethecombinedreceptive\nThisisonlypossibleif,andonlyif,\n\n=\n\nhg\n=\n\nhf\n.Onlyinthiscase,infact,itis\npossibletondaslidingwindowreceptivethattightlyenclosesthereceptivedue\nto\ng\nand\nf\natallpointsaccordingtoformulas(\n5.1\n).Wesaythatthesetworeceptive\nare\ncompatible\n.Therangeofinputsamples\ni\n=\ni\nf\n=\ni\ng\nthatanyoutputsample\ni\nh\nis\nthengivenby\ni\nmax\n=\n\n(\ni\nh\n\n1)+\na;a\n=min\n\n\nhf\n\n\nhf\n\n1\n2\n;\ng\n\n\nhg\n\n1\n2\n\n;\ni\nmin\n=\n\n(\ni\nh\n\n1)+\nb;b\n=max\n\n\nhf\n+\n\nhf\n\n1\n2\n;\ng\n+\n\nhg\n\n1\n2\n\n:\nWeconcludethatthecombinedreceptiveis\n\n=\n\nhg\n=\n\nhf\n;\n=\na\n+\nb\n2\n;\n=\nb\n\na\n+1\n:\n'b'Chapter6\nImplementationdetails\nThischaptercontainscalculationsanddetails.\n6.1Convolution\nItisoftenconvenienttoexpresstheconvolutionoperationinmatrixform.Tothisend,let\n\n(\nx\n)bethe\nim2row\noperator,extractingall\nW\n0\n\nH\n0\npatchesfromthemap\nx\nandstoring\nthemasrowsofa(\nH\n00\nW\n00\n)\n\n(\nH\n0\nW\n0\nD\n)matrix.Formally,thisoperatorisgivenby:\n[\n\n(\nx\n)]\npq\n=\n(\ni;j;d\n)=\nt\n(\np;q\n)\nx\nijd\nwheretheindexmapping(\ni;j;d\n)=\nt\n(\np;q\n)is\ni\n=\ni\n00\n+\ni\n0\n\n1\n;j\n=\nj\n00\n+\nj\n0\n\n1\n;p\n=\ni\n00\n+\nH\n00\n(\nj\n00\n\n1)\n;q\n=\ni\n0\n+\nH\n0\n(\nj\n0\n\n1)+\nH\n0\nW\n0\n(\nd\n\n1)\n:\nItisalsousefultothe\\transposed"operator\nrow2im\n:\n[\n\n\n(\nM\n)]\nijd\n=\nX\n(\np;q\n)\n2\nt\n\n1\n(\ni;j;d\n)\nM\npq\n:\nNotethat\n\nand\n\n\narelinearoperators.Bothcanbeexpressedbyamatrix\nH\n2\nR\n(\nH\n00\nW\n00\nH\n0\nW\n0\nD\n)\n\n(\nHWD\n)\nsuchthat\nvec(\n\n(\nx\n))=\nH\nvec(\nx\n)\n;\nvec(\n\n\n(\nM\n))=\nH\n>\nvec(\nM\n)\n:\nHenceweobtainthefollowingexpressionforthevectorizedoutput(see[\n6\n]):\nvec\ny\n=vec(\n\n(\nx\n)\nF\n)=\n(\n(\nI\n\n\n(\nx\n))vec\nF;\nor,equivalently,\n(\nF\n>\n\nI\n)vec\n\n(\nx\n)\n;\nwhere\nF\n2\nR\n(\nH\n0\nW\n0\nD\n)\n\nK\nisthematrixobtainedbyreshapingthearray\nf\nand\nI\nisanidentity\nmatrixofsuitabledimensions.Thisallowsobtainingthefollowingformulasforthederiva-\ntives:\ndz\nd\n(vec\nF\n)\n>\n=\ndz\nd\n(vec\ny\n)\n>\n(\nI\n\n\n(\nx\n))=vec\n\n\n(\nx\n)\n>\ndz\ndY\n\n>\n43\n'b'44\nCHAPTER6.IMPLEMENTATIONDETAILS\nwhere\nY\n2\nR\n(\nH\n00\nW\n00\n)\n\nK\nisthematrixobtainedbyreshapingthearray\ny\n.Likewise:\ndz\nd\n(vec\nx\n)\n>\n=\ndz\nd\n(vec\ny\n)\n>\n(\nF\n>\n\nI\n)\nd\nvec\n\n(\nx\n)\nd\n(vec\nx\n)\n>\n=vec\n\ndz\ndY\nF\n>\n\n>\nH\nInsummary,afterreshapingthesetermsweobtaintheformulas:\nvec\ny\n=vec(\n\n(\nx\n)\nF\n)\n;\ndz\ndF\n=\n\n(\nx\n)\n>\ndz\ndY\n;\ndz\ndX\n=\n\n\n\ndz\ndY\nF\n>\n\nwhere\nX\n2\nR\n(\nH\n0\nW\n0\n)\n\nD\nisthematrixobtainedbyreshaping\nx\n.Notably,theseexpressionsare\nusedtoimplementtheconvolutionaloperator;whilethismayseemt,itisinstead\nafastapproachwhenthenumberofislargeanditallowsleveragingfastBLASand\nGPUBLASimplementations.\n6.2Convolutiontranspose\nInordertounderstandtheofconvolutiontranspose,let\ny\ntobeobtainedfrom\nx\nbytheconvolutionoperatorasinsection\n4.1\n(includingpaddinganddownsampling).\nSincethisisalinearoperation,itcanberewrittenasvec\ny\n=\nM\nvec\nx\nforasuitablematrix\nM\n;\nconvolutiontransposecomputesinsteadvec\nx\n=\nM\n>\nvec\ny\n.Whilethisissimpletodescribe\nintermofmatrices,whathappensintermofindexesistricky.Inordertoderiveaformula\nfortheconvolutiontranspose,startfromstandardconvolution(fora1Dsignal):\ny\ni\n00\n=\nH\n0\nX\ni\n0\n=1\nf\ni\n0\nx\nS\n(\ni\n00\n\n1)+\ni\n0\n\nP\n\nh\n;\n1\n\ni\n00\n\n1+\n\nH\n\nH\n0\n+\nP\n\nh\n+\nP\n+\nh\nS\n\n;\nwhere\nS\nisthedownsamplingfactor,\nP\n\nh\nand\nP\n+\nh\nthepadding,\nH\nthelengthoftheinput\nsignal,\nx\nand\nH\n0\nthelengthofthe\nf\n.Duetopadding,theindexoftheinputdata\nx\nmayexceedtherange[1\n;H\n];weimplicitlyassumethatthesignaliszeropaddedoutsidethis\nrange.\nInordertoderiveanexpressionoftheconvolutiontranspose,wemakeuseoftheidentity\nvec\ny\n>\n(\nM\nvec\nx\n)=(vec\ny\n>\nM\n)vec\nx\n=vec\nx\n>\n(\nM\n>\nvec\ny\n).Expandingthisinformulas:\nb\nX\ni\n00\n=1\ny\ni\n00\nW\n0\nX\ni\n0\n=1\nf\ni\n0\nx\nS\n(\ni\n00\n\n1)+\ni\n0\n\nP\n\nh\n=\n+\n1\nX\ni\n00\n=\n\n+\n1\nX\ni\n0\n=\n\ny\ni\n00\nf\ni\n0\nx\nS\n(\ni\n00\n\n1)+\ni\n0\n\nP\n\nh\n=\n+\n1\nX\ni\n00\n=\n\n+\n1\nX\nk\n=\n\ny\ni\n00\nf\nk\n\nS\n(\ni\n00\n\n1)+\nP\n\nh\nx\nk\n=\n+\n1\nX\ni\n00\n=\n\n+\n1\nX\nk\n=\n\ny\ni\n00\nf\n(\nk\n\n1+\nP\n\nh\n)mod\nS\n+\nS\n\n1\n\ni\n00\n+\n\nk\n\n1+\nP\n\nh\nS\n\n+1\nx\nk\n=\n+\n1\nX\nk\n=\n\nx\nk\n+\n1\nX\nq\n=\n\ny\n\nk\n\n1+\nP\n\nh\nS\n\n+2\n\nq\nf\n(\nk\n\n1+\nP\n\nh\n)mod\nS\n+\nS\n(\nq\n\n1)+1\n:\n'b'6.3.SPATIALPOOLING\n45\nSummationrangeshavebeenextendedtoybyassumingthatallsignalsarezeropadded\nasneeded.Inordertorecoversuchranges,notethat\nk\n2\n[1\n;H\n](sincethisistherangeof\nelementsof\nx\ninvolvedintheoriginalconvolution).Furthermore,\nq\n\n1istheminimumvalue\nof\nq\nforwhichtheter\nf\nisnonzero;likewise,\nq\nb\n(\nH\n0\n\n1)\n=\n2\nc\n+1isafairlytightupper\nboundonthemaximumvalue(although,dependingon\nk\n,therecouldbeanelementless).\nHence\nx\nk\n=\n1+\nb\nH\n0\n\n1\nS\nc\nX\nq\n=1\ny\n\nk\n\n1+\nP\n\nh\nS\n\n+2\n\nq\nf\n(\nk\n\n1+\nP\n\nh\n)mod\nS\n+\nS\n(\nq\n\n1)+1\n;k\n=1\n;:::;H:\n(6.1)\nNotethatthesummationextremain(\n6.1\n)canbeslightlytoaccountforthe\nsizeof\ny\nand\nw\n:\nmax\n\n1\n;\n\nk\n\n1+\nP\n\nh\nS\n\n+2\n\nH\n00\n\n\nq\n\n1+min\n\nH\n0\n\n1\n\n(\nk\n\n1+\nP\n\nh\n)mod\nS\nS\n\n;\n\nk\n\n1+\nP\n\nh\nS\n\n:\nThesize\nH\n00\noftheoutputofconvolutiontransposeisobtainedinsection\n5.3\n.\n6.3Spatialpooling\nSincemaxpoolingsimplyselectsforeachoutputelementaninputelement,therelationcan\nbeexpressedinmatrixformasvec\ny\n=\nS\n(\nx\n)vec\nx\nforasuitableselectormatrix\nS\n(\nx\n)\n2\nf\n0\n;\n1\ng\n(\nH\n00\nW\n00\nD\n)\n\n(\nHWD\n)\n.Thederivativescanthebewrittenas:\ndz\nd\n(vec\nx\n)\n>\n=\ndz\nd\n(vec\ny\n)\n>\nS\n(\nx\n)\n;\nforall\nbutanullsetofpoints,wheretheoperatorisnottiable(thisusuallydoesnotpose\nproblemsinoptimizationbystochasticgradient).Formax-pooling,similarrelationsexists\nwithtwoerences:\nS\ndoesnotdependontheinput\nx\nanditisnotbinary,inorderto\naccountforthenormalizationfactors.Insummary,wehavetheexpressions:\nvec\ny\n=\nS\n(\nx\n)vec\nx\n;\ndz\nd\nvec\nx\n=\nS\n(\nx\n)\n>\ndz\nd\nvec\ny\n:\n(6.2)\n6.4Activationfunctions\n6.4.1ReLU\nTheReLUoperatorcanbeexpressedinmatrixnotationas\nvec\ny\n=diag\ns\nvec\nx\n;\ndz\nd\nvec\nx\n=diag\ns\ndz\nd\nvec\ny\nwhere\ns\n=[vec\nx\n>\n0]\n2f\n0\n;\n1\ng\nHWD\nisanindicatorvector.\n'b'46\nCHAPTER6.IMPLEMENTATIONDETAILS\n6.4.2Sigmoid\nThederivativeofthesigmoidfunctionisgivenby\ndz\ndx\nijk\n=\ndz\ndy\nijd\ndy\nijd\ndx\nijd\n=\ndz\ndy\nijd\n\n1\n(1+\ne\n\nx\nijd\n)\n2\n(\n\ne\n\nx\nijd\n)\n=\ndz\ndy\nijd\ny\nijd\n(1\n\ny\nijd\n)\n:\nInmatrixnotation:\ndz\nd\nx\n=\ndz\nd\ny\n\ny\n\n(\n11\n>\n\ny\n)\n:\n6.5Spatialbilinearresampling\nTheprojectedderivative\nd\nh\np\n;\n(\nx\n;\ng\n)\ni\n=d\nx\nofthespatialbilinaerresampleroperatorwith\nrespecttotheinputimage\nx\ncanbefoundasfollows:\n@\n@x\nijc\n"\nX\ni\n00\nj\n00\nc\n00\np\ni\n00\nk\n00\nc\n00\nH\nX\ni\n0\n=1\nW\nX\nj\n0\n=1\nx\ni\n0\nj\n0\nc\n00\nmax\nf\n0\n;\n1\nj\n\nv\ng\n1\ni\n00\nj\n00\n+\n\nv\n\ni\n0\njg\nmax\nf\n0\n;\n1\nj\n\nu\ng\n2\ni\n00\nj\n00\n+\n\nu\n\nj\n0\njg\n#\n=\nX\ni\n00\nj\n00\np\ni\n00\nk\n00\nc\nmax\nf\n0\n;\n1\nj\n\nv\ng\n1\ni\n00\nj\n00\n+\n\nv\n\ni\njg\nmax\nf\n0\n;\n1\nj\n\nu\ng\n2\ni\n00\nj\n00\n+\n\nu\n\nj\njg\n:\n(6.3)\nNotethattheformulaissimilartoEq.\n4.2\n,withthethatsummationison\ni\n00\nrather\nthan\ni\n.\nTheprojectedderivative\nd\nh\np\n;\n(\nx\n;\ng\n)\ni\n=d\ng\nwithrespecttothegridissimilar:\n@\n@g\n1\ni\n0\nj\n0\n"\nX\ni\n00\nj\n00\nc\np\ni\n00\nk\n00\nc\nH\nX\ni\n=1\nW\nX\nj\n=1\nx\nijc\nmax\nf\n0\n;\n1\nj\n\nv\ng\n1\ni\n00\nj\n00\n+\n\nv\n\ni\njg\nmax\nf\n0\n;\n1\nj\n\nu\ng\n2\ni\n00\nj\n00\n+\n\nu\n\nj\njg\n#\n=\n\nX\nc\np\ni\n0\nj\n0\nc\nH\nX\ni\n=1\nW\nX\nj\n=1\n\nv\nx\nijc\nmax\nf\n0\n;\n1\n\n\nv\ng\n2\ni\n0\nj\n0\n+\n\nv\n\nj\njg\nsign(\n\nv\ng\n1\ni\n0\nj\n0\n+\n\nv\n\nj\n)\n1\n\n1\n\nu\ng\n2\ni\n0\nj\n0\n+\n\nu\n<\n1\ng\n:\n(6.4)\nAsimilarexpressionholdsfor\n@g\n2\ni\n0\nj\n0\n6.6Normalization\n6.6.1Localresponsenormalization(LRN)\nThederivativeiseasilycomputedas:\ndz\ndx\nijd\n=\ndz\ndy\nijd\nL\n(\ni;j;d\nj\nx\n)\n\n\n\n2\nx\nijd\nX\nk\n:\nd\n2\nG\n(\nk\n)\ndz\ndy\nijk\nL\n(\ni;j;k\nj\nx\n)\n\n\n\n1\nx\nijk\nwhere\nL\n(\ni;j;k\nj\nx\n)=\n\n+\n\nX\nt\n2\nG\n(\nk\n)\nx\n2\nijt\n:\n'b'6.6.NORMALIZATION\n47\n6.6.2Batchnormalization\nThederivativeofthenetworkoutput\nz\nwithrespecttothemultipliers\nw\nk\nandbiases\nb\nk\nis\ngivenby\ndz\ndw\nk\n=\nX\ni\n00\nj\n00\nk\n00\nt\n00\ndz\ndy\ni\n00\nj\n00\nk\n00\nt\n00\ndy\ni\n00\nj\n00\nk\n00\nt\n00\ndw\nk\n=\nX\ni\n00\nj\n00\nt\n00\ndz\ndy\ni\n00\nj\n00\nkt\n00\nx\ni\n00\nj\n00\nkt\n00\n\n\nk\np\n\n2\nk\n+\n\n;\ndz\ndb\nk\n=\nX\ni\n00\nj\n00\nk\n00\nt\n00\ndz\ndy\ni\n00\nj\n00\nk\n00\nt\n00\ndy\ni\n00\nj\n00\nk\n00\nt\n00\ndw\nk\n=\nX\ni\n00\nj\n00\nt\n00\ndz\ndy\ni\n00\nj\n00\nkt\n00\n:\nThederivativeofthenetworkoutput\nz\nwithrespecttotheblockinput\nx\niscomputedas\nfollows:\ndz\ndx\nijkt\n=\nX\ni\n00\nj\n00\nk\n00\nt\n00\ndz\ndy\ni\n00\nj\n00\nk\n00\nt\n00\ndy\ni\n00\nj\n00\nk\n00\nt\n00\ndx\nijkt\n:\nSincefeaturechannelsareprocessedindependently,alltermswith\nk\n00\n6\n=\nk\narezero.Hence\ndz\ndx\nijkt\n=\nX\ni\n00\nj\n00\nt\n00\ndz\ndy\ni\n00\nj\n00\nkt\n00\ndy\ni\n00\nj\n00\nkt\n00\ndx\nijkt\n;\nwhere\ndy\ni\n00\nj\n00\nkt\n00\ndx\nijkt\n=\nw\nk\n\n\ni\n=\ni\n00\n;j\n=\nj\n00\n;t\n=\nt\n00\n\n\nk\ndx\nijkt\n\n1\np\n\n2\nk\n+\n\n\nw\nk\n2\n(\nx\ni\n00\nj\n00\nkt\n00\n\n\nk\n)\n\n\n2\nk\n+\n\n\n\n3\n2\nd\n2\nk\ndx\nijkt\n;\nthederivativeswithrespecttothemeanandvariancearecomputedasfollows:\n\nk\ndx\nijkt\n=\n1\nHWT\n;\nd\n2\nk\ndx\ni\n0\nj\n0\nkt\n0\n=\n2\nHWT\nX\nijt\n(\nx\nijkt\n\n\nk\n)\n\n\ni\n=\ni\n0\n;j\n=\nj\n0\n;t\n=\nt\n0\n\n1\nHWT\n\n=\n2\nHWT\n(\nx\ni\n0\nj\n0\nkt\n0\n\n\nk\n)\n;\nand\n\nE\nistheindicatorfunctionoftheevent\nE\n.Hence\ndz\ndx\nijkt\n=\nw\nk\np\n\n2\nk\n+\n\n \ndz\ndy\nijkt\n\n1\nHWT\nX\ni\n00\nj\n00\nkt\n00\ndz\ndy\ni\n00\nj\n00\nkt\n00\n!\n\nw\nk\n2(\n\n2\nk\n+\n\n)\n3\n2\nX\ni\n00\nj\n00\nkt\n00\ndz\ndy\ni\n00\nj\n00\nkt\n00\n(\nx\ni\n00\nj\n00\nkt\n00\n\n\nk\n)\n2\nHWT\n(\nx\nijkt\n\n\nk\n)\ni.e.\ndz\ndx\nijkt\n=\nw\nk\np\n\n2\nk\n+\n\n \ndz\ndy\nijkt\n\n1\nHWT\nX\ni\n00\nj\n00\nkt\n00\ndz\ndy\ni\n00\nj\n00\nkt\n00\n!\n\nw\nk\np\n\n2\nk\n+\n\nx\nijkt\n\n\nk\np\n\n2\nk\n+\n\n1\nHWT\nX\ni\n00\nj\n00\nkt\n00\ndz\ndy\ni\n00\nj\n00\nkt\n00\nx\ni\n00\nj\n00\nkt\n00\n\n\nk\np\n\n2\nk\n+\n\n:\n'b'48\nCHAPTER6.IMPLEMENTATIONDETAILS\nWecanidentifysomeofthesetermswiththeonescomputedasderivativesofbnormwith\nrespectto\nw\nk\nand\n\nk\n:\ndz\ndx\nijkt\n=\nw\nk\np\n\n2\nk\n+\n\n \ndz\ndy\nijkt\n\n1\nHWT\ndz\ndb\nk\n\nx\nijkt\n\n\nk\np\n\n2\nk\n+\n\n1\nHWT\ndz\ndw\nk\n!\n:\n6.6.3Spatialnormalization\nTheneighbourhoodnorm\nn\n2\ni\n00\nj\n00\nd\ncanbecomputedbyapplyingaveragepoolingto\nx\n2\nijd\nusing\nvl_nnpool\nwitha\nW\n0\n\nH\n0\npoolingregion,toppadding\nb\nH\n0\n\n1\n2\nc\n,bottompadding\nH\n0\nb\nH\n\n1\n2\nc\n1,\nandsimilarlyforthehorizontalpadding.\nThederivativeofspatialnormalizationcanbeobtainedasfollows:\ndz\ndx\nijd\n=\nX\ni\n00\nj\n00\nd\ndz\ndy\ni\n00\nj\n00\nd\ndy\ni\n00\nj\n00\nd\ndx\nijd\n=\nX\ni\n00\nj\n00\nd\ndz\ndy\ni\n00\nj\n00\nd\n(1+\nn\n2\ni\n00\nj\n00\nd\n)\n\n\ndx\ni\n00\nj\n00\nd\ndx\nijd\n\n\ndz\ndy\ni\n00\nj\n00\nd\n(1+\nn\n2\ni\n00\nj\n00\nd\n)\n\n\n\n1\nx\ni\n00\nj\n00\nd\ndn\n2\ni\n00\nj\n00\nd\nd\n(\nx\n2\nijd\n)\ndx\n2\nijd\ndx\nijd\n=\ndz\ndy\nijd\n(1+\nn\n2\nijd\n)\n\n\n\n2\nx\nijd\n"\nX\ni\n00\nj\n00\nd\ndz\ndy\ni\n00\nj\n00\nd\n(1+\nn\n2\ni\n00\nj\n00\nd\n)\n\n\n\n1\nx\ni\n00\nj\n00\nd\ndn\n2\ni\n00\nj\n00\nd\nd\n(\nx\n2\nijd\n)\n#\n=\ndz\ndy\nijd\n(1+\nn\n2\nijd\n)\n\n\n\n2\nx\nijd\n"\nX\ni\n00\nj\n00\nd\n\ni\n00\nj\n00\nd\ndn\n2\ni\n00\nj\n00\nd\nd\n(\nx\n2\nijd\n)\n#\n;\ni\n00\nj\n00\nd\n=\ndz\ndy\ni\n00\nj\n00\nd\n(1+\nn\n2\ni\n00\nj\n00\nd\n)\n\n\n\n1\nx\ni\n00\nj\n00\nd\nNotethatthesummationcanbecomputedasthederivativeofthe\nvl_nnpool\nblock.\n6.6.4Softmax\nCaremustbetakeninevaluatingtheexponentialinordertoavoidworovw.\nThesimplestwaytodosoistodividethenumeratoranddenominatorbytheexponentialof\nthemaximumvalue:\ny\nijk\n=\ne\nx\nijk\n\nmax\nd\nx\nijd\nP\nD\nt\n=1\ne\nx\nijt\n\nmax\nd\nx\nijd\n:\nThederivativeisgivenby:\ndz\ndx\nijd\n=\nX\nk\ndz\ndy\nijk\n\ne\nx\nijd\nL\n(\nx\n)\n\n1\n\nf\nk\n=\nd\ng\n\ne\nx\nijd\ne\nx\nijk\nL\n(\nx\n)\n\n2\n\n;L\n(\nx\n)=\nD\nX\nt\n=1\ne\nx\nijt\n:\nSimplifying:\ndz\ndx\nijd\n=\ny\nijd\n \ndz\ndy\nijd\n\nK\nX\nk\n=1\ndz\ndy\nijk\ny\nijk\n:\n!\n:\nInmatrixform:\ndz\ndX\n=\nY\n\n\ndz\ndY\n\n\ndz\ndY\n\nY\n\n11\n>\n\n'b'6.7.CATEGORICALLOSSES\n49\nwhere\nX;Y\n2\nR\nHW\n\nD\narethematricesobtainedbyreshapingthearrays\nx\nand\ny\n.Notethat\nthenumericalimplementationofthisexpressionisstraightforwardoncetheoutput\nY\nhas\nbeencomputedwiththecaveatsabove.\n6.7Categoricallosses\nThissectionobtainstheprojectedderivativesofthecategoricallossesinsection\n4.7\n.Recall\nthatalllossesgiveascalaroutput,sotheprojectiontensor\np\nistrivial(ascalar).\n6.7.1losses\nTop-\nK\nerror.\nThederivativeiszeroa.e.\nLog-loss.\nTheprojectedderivativeis:\n@p`\n(\nx\n;c\n)\n@x\nk\n=\n\np\n@\nlog(\nx\nc\n)\n@x\nk\n=\n\npx\nc\n\nk\n=\nc\n:\nSoftmaxlog-loss.\nTheprojectedderivativeisgivenby:\n@p`\n(\nx\n;c\n)\n@x\nk\n=\n\np\n@\n@x\nk\n \nx\nc\n\nlog\nC\nX\nt\n=1\ne\nx\nt\n!\n=\n\np\n \n\nk\n=\nc\n\ne\nx\nc\nP\nC\nt\n=1\ne\nx\nt\n!\n:\nInbrackets,wecanrecognizetheoutputofthelossitself:\ny\n=\n`\n(\nx\n;c\n)=\ne\nx\nc\nP\nC\nt\n=1\ne\nx\nt\n:\nHencethelossderivativesrewrites:\n@p`\n(\nx\n;c\n)\n@x\nk\n=\n\np\n(\n\nk\n=\nc\n\ny\n)\n:\nMulti-classhingeloss.\nTheprojectedderivativeis:\n@p`\n(\nx\n;c\n)\n@x\nk\n=\n\np\n1\n[\nx\nc\n<\n1]\n\nk\n=\nc\n:\nStructuredmulti-classhingeloss.\nTheprojectedderivativeis:\n@p`\n(\nx\n;c\n)\n@x\nk\n=\n\np\n1\n[\nx\nc\n<\n1+max\nt\n6\n=\nc\nx\nt\n](\n\nk\n=\nc\n\n\nk\n=\nt\n\n)\n;t\n\n=argmax\nt\n=1\n;\n2\n;:::;C\nx\nt\n:\n6.7.2Attributelosses\nBinaryerror.\nThederivativeofthebinaryerroris0a.e.\n'b'50\nCHAPTER6.IMPLEMENTATIONDETAILS\nBinarylog-loss.\nTheprojectedderivativeis:\n@p`\n(\nx;c\n)\n@x\n=\n\np\nc\nc\n\nx\n\n1\n2\n\n+\n1\n2\n:\nBinarylogisticloss.\nTheprojectedderivativeis:\n@p`\n(\nx;c\n)\n@x\n=\n\np\n@\n@x\nlog\n1\n1+\ne\n\ncx\n=\n\np\nce\n\ncx\n1+\ne\n\ncx\n=\n\np\nc\ne\ncx\n+1\n=\n\npc\n(\n\ncx\n)\n:\nBinaryhingeloss.\nTheprojectedderivativeis\n@p`\n(\nx;c\n)\n@x\n=\n\npc\n1\n[\ncx<\n1]\n:\n6.8Comparisons\n6.8.1\np\n-distance\nThederivativeoftheoperatorwithoutrootisgivenby:\ndz\ndx\nijd\n=\ndz\ndy\nij\np\nj\nx\nijd\n\n\nx\nijd\nj\np\n\n1\nsign(\nx\nijd\n\n\nx\nijd\n)\n:\nThederivativeoftheoperatorwithrootisgivenby:\ndz\ndx\nijd\n=\ndz\ndy\nij\n1\np\n \nX\nd\n0\nj\nx\nijd\n0\n\n\nx\nijd\n0\nj\np\n!\n1\np\n\n1\np\nj\nx\nijd\n\n\nx\nijd\nj\np\n\n1\nsign(\nx\nijd\n\n\nx\nijd\n)\n=\ndz\ndy\nij\nj\nx\nijd\n\n\nx\nijd\nj\np\n\n1\nsign(\nx\nijd\n\n\nx\nijd\n)\ny\np\n\n1\nij\n;\ndz\nd\n\nx\nijd\n=\n\ndz\ndx\nijd\n:\nTheformulassimplifyalittlefor\np\n=1\n;\n2whicharethereforeimplementedasspecialcases.\n'b'Bibliography\n[1]\nK.CK.Simonyan,A.Vedaldi,andA.Zisserman.Returnofthedevilinthe\ndetails:Delvingdeepintoconvolutionalnets.In\nProc.BMVC\n,2014.\n[2]\nJ.Deng,W.Dong,R.Socher,L.-J.Li,K.Li,andL.Fei-Fei.ImageNet:ALarge-Scale\nHierarchicalImageDatabase.In\nProc.CVPR\n,2009.\n[3]\nS.andC.Szegedy.Batchnormalization:Acceleratingdeepnetworktrainingby\nreducinginternalcovariateshift.\nCoRR\n,2015.\n[4]\nS.andC.Szegedy.BatchNormalization:AcceleratingDeepNetworkTrainingby\nReducingInternalCovariateShift.\nArXive-prints\n,2015.\n[5]\nYangqingJia.:Anopensourceconvolutionalarchitectureforfastfeatureembed-\nding.\nhttp://caffe.berkeleyvision.org/\n,2013.\n[6]\nD.B.Kinghorn.Integralsandderivativesforcorrelatedgaussianfuctionsusingmatrix\ntialcalculus.\nInternationalJournalofQuantumChemestry\n,57:141{155,1996.\n[7]\nA.Krizhevsky,I.Sutskever,andG.E.Hinton.Imagenetclasscationwithdeepconvo-\nlutionalneuralnetworks.In\nProc.NIPS\n,2012.\n[8]\nMinLin,QiangChen,andShuichengYan.Networkinnetwork.\nCoRR\n,abs/1312.4400,\n2013.\n[9]\nK.Simonyan,A.Vedaldi,andA.Zisserman.Deepinsideconvolutionalnetworks:Visu-\nalisingimagetionmodelsandsaliencymaps.In\nProc.ICLR\n,2014.\n[10]\nK.SimonyanandA.Zisserman.Verydeepconvolutionalnetworksforlarge-scaleimage\nrecognition.2015.\n[11]\nA.VedaldiandB.Fulkerson.VLFeat{Anopenandportablelibraryofcomputervision\nalgorithms.In\nProc.ACMInt.Conf.onMultimedia\n,2010.\n[12]\nM.D.ZeilerandR.Fergus.Visualizingandunderstandingconvolutionalnetworks.In\nProc.ECCV\n,2014.\n51\n'