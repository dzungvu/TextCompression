b'DRAW:ARecurrentNeuralNetworkForImageGeneration\nKarolGregor\nKAROLG\n@\nGOOGLE\n.\nCOM\nIvoDanihelka\nDANIHELKA\n@\nGOOGLE\n.\nCOM\nAlexGraves\nGRAVESA\n@\nGOOGLE\n.\nCOM\nDaniloJimenezRezende\nDANILOR\n@\nGOOGLE\n.\nCOM\nDaanWierstra\nWIERSTRA\n@\nGOOGLE\n.\nCOM\nGoogleDeepMind\nAbstract\nThispaperintroducesthe\nDeepRecurrentAtten-\ntiveWriter\n(DRAW)neuralnetworkarchitecture\nforimagegeneration.DRAWnetworkscombine\nanovelspatialattentionmechanismthatmimics\nthefoveationofthehumaneye,withasequential\nvariationalauto-encodingframeworkthatallows\nfortheiterativeconstructionofcompleximages.\nThesystemsubstantiallyimprovesonthestate\noftheartforgenerativemodelsonMNIST,and,\nwhentrainedontheStreetViewHouseNumbers\ndataset,itgeneratesimagesthatcannotbedistin-\nguishedfromrealdatawiththenakedeye.\n1.Introduction\nApersonaskedtodraw,paintorotherwiserecreateavisual\nscenewillnaturallydosoinasequential,iterativefashion,\nreassessingtheirhandiworkaftereachRough\noutlinesaregraduallyreplacedbypreciseforms,linesare\nsharpened,darkenedorerased,shapesarealtered,andthe\npictureemerges.Mostapproachestoautomaticim-\nagegeneration,however,aimtogenerateentirescenesat\nonce.Inthecontextofgenerativeneuralnetworks,thistyp-\nicallymeansthatallthepixelsareconditionedonasingle\nlatentdistribution(\nDayanetal.\n,\n1995\n;\nHinton&Salakhut-\ndinov\n,\n2006\n;\nLarochelle&Murray\n,\n2011\n).Aswellaspre-\ncludingthepossibilityofiterativeself-correction,theone\nshotapproachisfundamentallydiftoscaletolarge\nimages.The\nDeepRecurrentAttentiveWriter\n(DRAW)ar-\nchitecturerepresentsashifttowardsamorenaturalformof\nimageconstruction,inwhichpartsofascenearecreated\nindependentlyfromothers,andapproximatesketchesare\nsuccessively\nProceedingsofthe\n32\nnd\nInternationalConferenceonMachine\nLearning\n,Lille,France,2015.JMLR:W&CPvolume37.Copy-\nright2015bytheauthor(s).\nFigure1.\nAtrainedDRAWnetworkgeneratingMNISTdig-\nits.\nEachrowshowssuccessivestagesinthegenerationofasin-\ngledigit.Notehowthelinescomposingthedigitsappeartobe\ndrawnbythenetwork.Theredrectangledelimitstheareaat-\ntendedtobythenetworkateachtime-step,withthefocalpreci-\nsionindicatedbythewidthoftherectangleborder.\nThecoreoftheDRAWarchitectureisapairofrecurrent\nneuralnetworks:an\nencoder\nnetworkthatcompressesthe\nrealimagespresentedduringtraining,anda\ndecoder\nthat\nreconstitutesimagesafterreceivingcodes.Thecombined\nsystemistrainedend-to-endwithstochasticgradientde-\nscent,wherethelossfunctionisavariationalupperbound\nonthelog-likelihoodofthedata.Itthereforebelongstothe\nfamilyof\nvariationalauto-encoders\n,arecentlyemerged\nhybridofdeeplearningandvariationalinferencethathas\nledtoadvancesingenerativemodelling(\nGre-\ngoretal.\n,\n2014\n;\nKingma&Welling\n,\n2014\n;\nRezendeetal.\n,\n2014\n;\nMnih&Gregor\n,\n2014\n;\nSalimansetal.\n,\n2014\n).Where\nDRAWdiffersfromitssiblingsisthat,ratherthangenerat-\narXiv:1502.04623v2  [cs.CV]  20 May 2015'b"DRAW:ARecurrentNeuralNetworkForImageGeneration\ningimagesinasinglepass,ititerativelyconstructsscenes\nthroughanaccumulationofemittedbythe\ndecoder,eachofwhichisobservedbytheencoder.\nAnobviouscorrelateofgeneratingimagesstepbystepis\ntheabilitytoselectivelyattendtopartsofthescenewhile\nignoringothers.Awealthofresultsinthepastfewyears\nsuggestthatvisualstructurecanbebettercapturedbyase-\nquenceofpartialglimpses,orfoveations,thanbyasin-\nglesweepthroughtheentireimage(\nLarochelle&Hinton\n,\n2010\n;\nDeniletal.\n,\n2012\n;\nTangetal.\n,\n2013\n;\nRanzato\n,\n2014\n;\nZhengetal.\n,\n2014\n;\nMnihetal.\n,\n2014\n;\nBaetal.\n,\n2014\n;\nSer-\nmanetetal.\n,\n2014\n).Themainchallengefacedbysequential\nattentionmodelsislearningwheretolook,whichcanbe\naddressedwithreinforcementlearningtechniquessuchas\npolicygradients(\nMnihetal.\n,\n2014\n).Theattentionmodelin\nDRAW,however,isfullydifferentiable,makingitpossible\ntotrainwithstandardbackpropagation.Inthissenseitre-\nsemblestheselectivereadandwriteoperationsdeveloped\nfortheNeuralTuringMachine(\nGravesetal.\n,\n2014\n).\nThefollowingsectiontheDRAWarchitecture,\nalongwiththelossfunctionusedfortrainingandthepro-\ncedureforimagegeneration.Section\n3\npresentstheselec-\ntiveattentionmodelandshowshowitisappliedtoread-\ningandmodifyingimages.Section\n4\nprovidesexperi-\nmentalresultsontheMNIST,StreetViewHouseNum-\nbersandCIFAR-10datasets,withexamplesofgenerated\nimages;andconcludingremarksaregiveninSection\n5\n.\nLastly,wewouldliketodirectthereadertothevideo\naccompanyingthispaper(\nhttps://www.youtube.\ncom/watch?v=Zt-7MI9eKEo\n)whichcontainsexam-\nplesofDRAWnetworksreadingandgeneratingimages.\n2.TheDRAWNetwork\nThebasicstructureofaDRAWnetworkissimilartothatof\nothervariationalauto-encoders:anencodernetworkdeter-\nminesadistributionoverlatentcodesthatcapturesalient\ninformationabouttheinputdata;adecodernetworkre-\nceivessamplesfromthecodedistribuionandusesthemto\nconditionitsowndistributionoverimages.Howeverthere\narethreekeydifferences.Firstly,boththeencoderandde-\ncoderarerecurrentnetworksinDRAW,sothata\nsequence\nofcodesamplesisexchangedbetweenthem;moreoverthe\nencoderisprivytothedecoder'spreviousoutputs,allow-\ningittotailorthecodesitsendsaccordingtothedecoder's\nbehavioursofar.Secondly,thedecoder'soutputsaresuc-\ncessivelyaddedtothedistributionthatwillultimatelygen-\neratethedata,asopposedtoemittingthisdistributionin\nasinglestep.Andthirdly,adynamicallyupdatedatten-\ntionmechanismisusedtorestrictboththeinputregion\nobservedbytheencoder,andtheoutputregion\nbythedecoder.Insimpleterms,thenetworkdecidesat\neachtime-stepwheretoreadandwheretowriteaswell\nFigure2.\nLeft:ConventionalVariationalAuto-Encoder\n.Dur-\ninggeneration,asample\nz\nisdrawnfromaprior\nP\n(\nz\n)\nandpassed\nthroughthefeedforwarddecodernetworktocomputetheproba-\nbilityoftheinput\nP\n(\nx\nj\nz\n)\ngiventhesample.Duringinferencethe\ninput\nx\nispassedtotheencodernetwork,producinganapprox-\nimateposterior\nQ\n(\nz\nj\nx\n)\noverlatentvariables.Duringtraining,\nz\nissampledfrom\nQ\n(\nz\nj\nx\n)\nandthenusedtocomputethetotalde-\nscriptionlength\nKL\n\nQ\n(\nZ\nj\nx\n)\njj\nP\n(\nZ\n)\n\n\nlog(\nP\n(\nx\nj\nz\n))\n,whichis\nminimisedwithstochasticgradientdescent.\nRight:DRAWNet-\nwork\n.Ateachtime-stepasample\nz\nt\nfromtheprior\nP\n(\nz\nt\n)\nis\npassedtotherecurrentdecodernetwork,whichthenpart\nofthecanvasmatrix.Thecanvasmatrix\nc\nT\nisusedtocom-\npute\nP\n(\nx\nj\nz\n1:\nT\n)\n.Duringinferencetheinputisreadateverytime-\nstepandtheresultispassedtotheencoderRNN.TheRNNsat\ntheprevioustime-stepspecifywheretoread.Theoutputofthe\nencoderRNNisusedtocomputetheapproximateposteriorover\nthelatentvariablesatthattime-step.\naswhattowrite.ThearchitectureissketchedinFig.\n2\n,\nalongsideafeedforwardvariationalauto-encoder.\n2.1.NetworkArchitecture\nLet\nRNN\nenc\nbethefunctionenactedbytheencodernet-\nworkatasingletime-step.Theoutputof\nRNN\nenc\nattime\nt\nistheencoderhiddenvector\nh\nenc\nt\n.Similarlytheoutputof\nthedecoder\nRNN\ndec\nat\nt\nisthehiddenvector\nh\ndec\nt\n.Ingen-\neraltheencoderanddecodermaybeimplementedbyany\nrecurrentneuralnetwork.Inourexperimentsweusethe\nLongShort-TermMemory\narchitecture(LSTM;\nHochreiter\n&Schmidhuber\n(\n1997\n))forboth,intheextendedformwith\nforgetgates\n(\nGersetal.\n,\n2000\n).WefavourLSTMdue\ntoitsproventrackrecordforhandlinglong-rangedepen-\ndenciesinrealsequentialdata(\nGraves\n,\n2013\n;\nSutskever\netal.\n,\n2014\n).Throughoutthepaper,weusethenotation\nb\n=\nW\n(\na\n)\ntodenotealinearweightmatrixwithbiasfrom\nthevector\na\ntothevector\nb\n.\nAteachtime-step\nt\n,theencoderreceivesinputfromboth\ntheimage\nx\nandfromthepreviousdecoderhiddenvector\nh\ndec\nt\n\n1\n.Thepreciseformoftheencoderinputdependsona\nread\noperation,whichwillbeinthenextsection.\nTheoutput\nh\nenc\nt\noftheencoderisusedtoparameterisea\ndistribution\nQ\n(\nZ\nt\nj\nh\nenc\nt\n)\noverthelatentvector\nz\nt\n.Inour\n"b'DRAW:ARecurrentNeuralNetworkForImageGeneration\nexperimentsthelatentdistributionisadiagonalGaussian\nN\n(\nZ\nt\nj\n\nt\n;\nt\n)\n:\n\nt\n=\nW\n(\nh\nenc\nt\n)\n(1)\n\nt\n=exp(\nW\n(\nh\nenc\nt\n))\n(2)\nBernoullidistributionsaremorecommonthanGaussians\nforlatentvariablesinauto-encoders(\nDayanetal.\n,\n1995\n;\nGregoretal.\n,\n2014\n);howeveragreatadvantageofGaus-\nsianlatentsisthatthegradientofafunctionofthesam-\npleswithrespecttothedistributionparameterscanbeeas-\nilyobtainedusingtheso-called\nreparameterizationtrick\n(\nKingma&Welling\n,\n2014\n;\nRezendeetal.\n,\n2014\n).This\nmakesitstraightforwardtoback-propagateunbiased,low\nvariancestochasticgradientsofthelossfunctionthrough\nthelatentdistribution.\nAteachtime-stepasample\nz\nt\n\nQ\n(\nZ\nt\nj\nh\nenc\nt\n)\ndrawnfrom\nthelatentdistributionispassedasinputtothedecoder.The\noutput\nh\ndec\nt\nofthedecoderisadded(viaa\nwrite\nopera-\ntion,inthesequel)toacumulative\ncanvas\nmatrix\nc\nt\n,whichisultimatelyusedtoreconstructtheimage.The\ntotalnumberoftime-steps\nT\nconsumedbythenetworkbe-\nforeperformingthereconstructionisafreeparameterthat\nmustbeinadvance.\nForeachimage\nx\npresentedtothenetwork,\nc\n0\n;h\nenc\n0\n;h\ndec\n0\nareinitialisedtolearnedbiases,andtheDRAWnet-\nworkiterativelycomputesthefollowingequationsfor\nt\n=\n1\n:::;T\n:\n^\nx\nt\n=\nx\n\n\n(\nc\nt\n\n1\n)\n(3)\nr\nt\n=\nread\n(\nx\nt\n;\n^\nx\nt\n;h\ndec\nt\n\n1\n)\n(4)\nh\nenc\nt\n=\nRNN\nenc\n(\nh\nenc\nt\n\n1\n;\n[\nr\nt\n;h\ndec\nt\n\n1\n])\n(5)\nz\nt\n\nQ\n(\nZ\nt\nj\nh\nenc\nt\n)\n(6)\nh\ndec\nt\n=\nRNN\ndec\n(\nh\ndec\nt\n\n1\n;z\nt\n)\n(7)\nc\nt\n=\nc\nt\n\n1\n+\nwrite\n(\nh\ndec\nt\n)\n(8)\nwhere\n^\nx\nt\nisthe\nerrorimage\n,\n[\nv;w\n]\nistheconcatenation\nofvectors\nv\nand\nw\nintoasinglevector,and\n\ndenotes\nthelogisticsigmoidfunction:\n\n(\nx\n)=\n1\n1+exp(\n\nx\n)\n.Note\nthat\nh\nenc\nt\n,andhence\nQ\n(\nZ\nt\nj\nh\nenc\nt\n)\n,dependsonboth\nx\nandthehistory\nz\n1:\nt\n\n1\nofpreviouslatentsamples.We\nwillsometimesmakethisdependencyexplicitbywriting\nQ\n(\nZ\nt\nj\nx;z\n1:\nt\n\n1\n)\n,asshowninFig.\n2\n.\nh\nenc\ncanalsobe\npassedasinputtothe\nread\noperation;howeverwedidnot\nthatthishelpedperformanceandthereforeomittedit.\n2.2.LossFunction\nThecanvasmatrix\nc\nT\nisusedtoparameteriseamodel\nD\n(\nX\nj\nc\nT\n)\noftheinputdata.Iftheinputisbinary,thenatural\nchoicefor\nD\nisaBernoullidistributionwithmeansgiven\nby\n\n(\nc\nT\n)\n.The\nreconstructionloss\nL\nx\nisasthe\nnegativelogprobabilityof\nx\nunder\nD\n:\nL\nx\n=\n\nlog\nD\n(\nx\nj\nc\nT\n)\n(9)\nThe\nlatentloss\nL\nz\nforasequenceoflatentdistributions\nQ\n(\nZ\nt\nj\nh\nenc\nt\n)\nisasthesummedKullback-Leiblerdi-\nvergenceofsomelatentprior\nP\n(\nZ\nt\n)\nfrom\nQ\n(\nZ\nt\nj\nh\nenc\nt\n)\n:\nL\nz\n=\nT\nX\nt\n=1\nKL\n\nQ\n(\nZ\nt\nj\nh\nenc\nt\n)\njj\nP\n(\nZ\nt\n)\n\n(10)\nNotethatthislossdependsuponthelatentsamples\nz\nt\ndrawnfrom\nQ\n(\nZ\nt\nj\nh\nenc\nt\n)\n,whichdependinturnontheinput\nx\n.IfthelatentdistributionisadiagonalGaussianwith\n\nt\n,\n\nt\nasinEqs\n1\nand\n2\n,asimplechoicefor\nP\n(\nZ\nt\n)\nis\nastandardGaussianwithmeanzeroandstandarddeviation\none,inwhichcaseEq.\n10\nbecomes\nL\nz\n=\n1\n2\n \nT\nX\nt\n=1\n\n2\nt\n+\n\n2\nt\n\nlog\n\n2\nt\n!\n\nT=\n2\n(11)\nThetotalloss\nL\nforthenetworkistheexpectationofthe\nsumofthereconstructionandlatentlosses:\nL\n=\nhL\nx\n+\nL\nz\ni\nz\n\nQ\n(12)\nwhichweoptimiseusingasinglesampleof\nz\nforeach\nstochasticgradientdescentstep.\nL\nz\ncanbeinterpretedasthenumberofnatsrequiredto\ntransmitthelatentsamplesequence\nz\n1:\nT\ntothedecoder\nfromtheprior,and(if\nx\nisdiscrete)\nL\nx\nisthenumberof\nnatsrequiredforthedecodertoreconstruct\nx\ngiven\nz\n1:\nT\n.\nThetotallossisthereforeequivalenttotheexpectedcom-\npressionofthedatabythedecoderandprior.\n2.3.StochasticDataGeneration\nAnimage\n~\nx\ncanbegeneratedbyaDRAWnetworkbyit-\nerativelypickinglatentsamples\n~\nz\nt\nfromtheprior\nP\n,then\nrunningthedecodertoupdatethecanvasmatrix\n~\nc\nt\n.After\nT\nrepetitionsofthisprocessthegeneratedimageisasample\nfrom\nD\n(\nX\nj\n~\nc\nT\n)\n:\n~\nz\nt\n\nP\n(\nZ\nt\n)\n(13)\n~\nh\ndec\nt\n=\nRNN\ndec\n(\n~\nh\ndec\nt\n\n1\n;\n~\nz\nt\n)\n(14)\n~\nc\nt\n=~\nc\nt\n\n1\n+\nwrite\n(\n~\nh\ndec\nt\n)\n(15)\n~\nx\n\nD\n(\nX\nj\n~\nc\nT\n)\n(16)\nNotethattheencoderisnotinvolvedinimagegeneration.\n3.ReadandWriteOperations\nTheDRAWnetworkdescribedintheprevioussectionis\nnotcompleteuntilthe\nread\nand\nwrite\noperationsinEqs.\n4\nand\n8\nhavebeenThissectiondescribestwoways\ntodoso,onewithselectiveattentionandonewithout.\n'b"DRAW:ARecurrentNeuralNetworkForImageGeneration\n3.1.ReadingandWritingWithoutAttention\nInthesimplestinstantiationofDRAWtheentireinputim-\nageispassedtotheencoderateverytime-step,andthede-\ncodertheentirecanvasmatrixateverytime-step.\nInthiscasethe\nread\nand\nwrite\noperationsreduceto\nread\n(\nx;\n^\nx\nt\n;h\ndec\nt\n\n1\n)=[\nx;\n^\nx\nt\n]\n(17)\nwrite\n(\nh\ndec\nt\n)=\nW\n(\nh\ndec\nt\n)\n(18)\nHoweverthisapproachdoesnotallowtheencodertofo-\ncusononlypartoftheinputwhencreatingthelatentdis-\ntribution;nordoesitallowthedecodertomodifyonlya\npartofthecanvasvector.Inotherwordsitdoesnotpro-\nvidethenetworkwithanexplicitselectiveattentionmech-\nanism,whichwebelievetobecrucialtolargescaleimage\ngeneration.WerefertotheaboveasDRAW\nwithoutattention.\n3.2.SelectiveAttentionModel\nToendowthenetworkwithselectiveattentionwithoutsac-\ntheofgradientdescenttraining,wetakein-\nspirationfromthedifferentiableattentionmechanismsre-\ncentlyusedinhandwritingsynthesis(\nGraves\n,\n2013\n)and\nNeuralTuringMachines(\nGravesetal.\n,\n2014\n).Unlike\ntheaforementionedworks,weconsideranexplicitlytwo-\ndimensionalformofattention,whereanarrayof2DGaus-\nsianisappliedtotheimage,yieldinganimage\n`patch'ofsmoothlyvaryinglocationandzoom.Thiscon-\nwhichwerefertosimplyasDRAW,some-\nwhatresemblestheaftransformationsusedincomputer\ngraphics-basedautoencoders(\nTieleman\n,\n2014\n).\nAsillustratedinFig.\n3\n,the\nN\n\nN\ngridofGaussianis\npositionedontheimagebyspecifyingtheco-ordinatesof\nthegridcentreandthestridedistancebetweenadjacent\nters.Thestridecontrolsthe`zoom'ofthepatch;thatis,the\nlargerthestride,thelargeranareaoftheoriginalimagewill\nbevisibleintheattentionpatch,butthelowertheeffective\nresolutionofthepatchwillbe.Thegridcentre\n(\ng\nX\n;g\nY\n)\nandstride\n\n(bothofwhicharereal-valued)determinethe\nmeanlocation\n\ni\nX\n;\nj\nY\noftheatrow\ni\n,column\nj\ninthe\npatchasfollows:\n\ni\nX\n=\ng\nX\n+(\ni\n\nN=\n2\n\n0\n:\n5)\n\n(19)\n\nj\nY\n=\ng\nY\n+(\nj\n\nN=\n2\n\n0\n:\n5)\n\n(20)\nTwomoreparametersarerequiredtofullyspecifytheat-\ntentionmodel:theisotropicvariance\n\n2\noftheGaussian\nandascalarintensity\n\nthatmultipliesthere-\nsponse.Givenan\nA\n\nB\ninputimage\nx\n,allveattention\nparametersaredynamicallydeterminedateachtimestep\nFigure3.\nLeft:\nA\n3\n\n3\ngridofsuperimposedonanimage.\nThestride(\n\n)andcentrelocation(\ng\nX\n;g\nY\n)areindicated.\nRight:\nThree\nN\n\nN\npatchesextractedfromtheimage(\nN\n=12\n).The\ngreenrectanglesontheleftindicatetheboundaryandprecision\n(\n\n)ofthepatches,whilethepatchesthemselvesareshowntothe\nright.Thetoppatchhasasmall\n\nandhigh\n\n,givingazoomed-in\nbutblurryviewofthecentreofthedigit;themiddlepatchhas\nlarge\n\nandlow\n\n,effectivelydownsamplingthewholeimage;\nandthebottompatchhashigh\n\nand\n\n.\nviaalineartransformationofthedecoderoutput\nh\ndec\n:\n(~\ng\nX\n;\n~\ng\nY\n;\nlog\n\n2\n;\nlog\n~\n;\nlog\n\n)=\nW\n(\nh\ndec\n)\n(21)\ng\nX\n=\nA\n+1\n2\n(~\ng\nX\n+1)\n(22)\ng\nY\n=\nB\n+1\n2\n(~\ng\nY\n+1)\n(23)\n\n=\nmax(\nA;B\n)\n\n1\nN\n\n1\n~\n\n(24)\nwherethevariance,strideandintensityareemittedinthe\nlog-scaletoensurepositivity.Thescalingof\ng\nX\n,\ng\nY\nand\n\nischosentoensurethattheinitialpatch(witharandomly\ninitialisednetwork)roughlycoversthewholeinputimage.\nGiventheattentionparametersemittedbythedecoder,the\nhorizontalandverticalmatrices\nF\nX\nand\nF\nY\n(di-\nmensions\nN\n\nA\nand\nN\n\nB\nrespectively)areas\nfollows:\nF\nX\n[\ni;a\n]=\n1\nZ\nX\nexp\n\n\n(\na\n\n\ni\nX\n)\n2\n2\n\n2\n\n(25)\nF\nY\n[\nj;b\n]=\n1\nZ\nY\nexp\n \n\n(\nb\n\n\nj\nY\n)\n2\n2\n\n2\n!\n(26)\nwhere\n(\ni;j\n)\nisapointintheattentionpatch,\n(\na;b\n)\nisapoint\nintheinputimage,and\nZ\nx\n;Z\ny\narenormalisationconstants\nthatensurethat\nP\na\nF\nX\n[\ni;a\n]=1\nand\nP\nb\nF\nY\n[\nj;b\n]=1\n.\n"b"DRAW:ARecurrentNeuralNetworkForImageGeneration\nFigure4.\nZooming.TopLeft:\nTheoriginal\n100\n\n75\nimage.\nTop\nMiddle:\nA\n12\n\n12\npatchextractedwith1442DGaussian\nTopRight:\nThereconstructedimagewhenapplyingtransposed\nonthepatch.\nBottom:\nOnlytwo2DGaussianare\ndisplayed.Theoneisusedtoproducethetop-leftpatchfea-\nture.Thelastisusedtoproducethebottom-rightpatchfea-\nture.Byusingdifferentweights,theattentioncanbemoved\ntoadifferentlocation.\n3.3.ReadingandWritingWithAttention\nGiven\nF\nX\n,\nF\nY\nandintensity\n\ndeterminedby\nh\ndec\nt\n\n1\n,along\nwithaninputimage\nx\nanderrorimage\n^\nx\nt\n,the\nread\nopera-\ntionreturnstheconcatenationoftwo\nN\n\nN\npatchesfrom\ntheimageanderrorimage:\nread\n(\nx;\n^\nx\nt\n;h\ndec\nt\n\n1\n)=\n\n[\nF\nY\nxF\nT\nX\n;F\nY\n^\nxF\nT\nX\n]\n(27)\nNotethatthesameareusedforboththeimage\nanderrorimage.Forthewriteoperation,adistinctsetof\nattentionparameters\n^\n\n,\n^\nF\nX\nand\n^\nF\nY\nareextractedfrom\nh\ndec\nt\n,\ntheorderoftranspositionisreversed,andtheintensityis\ninverted:\nw\nt\n=\nW\n(\nh\ndec\nt\n)\n(28)\nwrite\n(\nh\ndec\nt\n)=\n1\n^\n\n^\nF\nT\nY\nw\nt\n^\nF\nX\n(29)\nwhere\nw\nt\nisthe\nN\n\nN\nwritingpatch\nemittedby\nh\ndec\nt\n.For\ncolourimageseachpointintheinputanderrorimage(and\nhenceinthereadingandwritingpatches)isanRGBtriple.\nInthiscasethesamereadingandwritingareusedfor\nallthreechannels.\n4.ExperimentalResults\nWeassesstheabilityofDRAWtogeneraterealistic-\nlookingimagesbytrainingonthreedatasetsofprogres-\nsivelyincreasingvisualcomplexity:MNIST(\nLeCunetal.\n,\n1998\n),StreetViewHouseNumbers(SVHN)(\nNetzeretal.\n,\n2011\n)andCIFAR-10(\nKrizhevsky\n,\n2009\n).Theimages\ngeneratedbythenetworkarealwaysnovel(notsimply\ncopiesoftrainingexamples),andarevirtuallyindistin-\nguishablefromrealdataforMNISTandSVHN;thegener-\natedCIFARimagesaresomewhatblurry,butstillcontain\nrecognisablestructurefromnaturalscenes.Thebinarized\nMNISTresultssubstantiallyimproveonthestateoftheart.\nAsapreliminaryexercise,wealsoevaluatethe2Datten-\ntionmoduleoftheDRAWnetworkonclutteredMNIST\n\nForallexperiments,themodel\nD\n(\nX\nj\nc\nT\n)\noftheinputdata\nwasaBernoullidistributionwithmeansgivenby\n\n(\nc\nT\n)\n.\nFortheMNISTexperiments,thereconstructionlossfrom\nEq\n9\nwastheusualbinarycross-entropyterm.Forthe\nSVHNandCIFAR-10experiments,thered,greenandblue\npixelintensitieswererepresentedasnumbersbetween0\nand1,whichweretheninterpretedasindependentcolour\nemissionprobabilities.Thereconstructionlosswasthere-\nforethecross-entropybetweenthepixelintensitiesandthe\nmodelprobabilities.Althoughthisapproachworkedwell\ninpractice,itmeansthatthetraininglossdidnotcorre-\nspondtothetruecompressioncostofRGBimages.\nNetworkhyper-parametersforalltheexperimentsare\npresentedinTable\n3\n.TheAdamoptimisationalgo-\nrithm(\nKingma&Ba\n,\n2014\n)wasusedthroughout.Ex-\namplesofgenerationsequencesforMNISTandSVHN\nareprovidedintheaccompanyingvideo(\nhttps://www.\nyoutube.com/watch?v=Zt-7MI9eKEo\n).\n4.1.ClutteredMNIST\nTotesttheefyoftheDRAWattention\nmechanism(asopposedtoitsabilitytoaidinimagegener-\nation),weevaluateitsperformanceonthe\n100\n\n100\nclut-\nteredtranslatedMNISTtask(\nMnihetal.\n,\n2014\n).Eachim-\nageinclutteredMNISTcontainsmanydigit-likefragments\nofvisualclutterthatthenetworkmustdistinguishfromthe\ntruedigittobeAsillustratedinFig.\n5\n,having\naniterativeattentionmodelallowsthenetworktoprogres-\nsivelyzoominontherelevantregionoftheimage,and\nignoretheclutteroutsideit.\nOurmodelconsistsofanLSTMrecurrentnetworkthatre-\nceivesa\n12\n\n12\n`glimpse'fromtheinputimageateach\ntime-step,usingtheselective\nread\noperationinSec-\ntion\n3.2\n.Afteraednumberofglimpsesthenetworkuses\nasoftmaxlayertoclassifytheMNISTdigit.Thenetwork\nissimilartotherecentlyintroducedRecurrentAttention\nModel(RAM)(\nMnihetal.\n,\n2014\n),exceptthatourattention\nmethodisdifferentiable;wethereforerefertoitasDiffer-\nentiableRAM.\nTheresultsinTable\n1\ndemonstrateaimprove-\nmentintesterrorovertheoriginalRAMnetwork.More-\noverourmodelhadonlyasingleattentionpatchateach\n"b'DRAW:ARecurrentNeuralNetworkForImageGeneration\nFigure5.\nClutteredMNISTwithattention.\nEach\nsequenceshowsasuccessionoffourglimpsestakenbythenet-\nworkwhileclassifyingclutteredtranslatedMNIST.Thegreen\nrectangleindicatesthesizeandlocationoftheattentionpatch,\nwhilethelinewidthrepresentsthevarianceofthe\nTable1.\ntesterroron\n100\n\n100\nClutteredTrans-\nlatedMNIST.\nModel\nError\nConvolutional,2layers\n14.35%\nRAM,4glimpses,\n12\n\n12\n,4scales\n9.41%\nRAM,8glimpses,\n12\n\n12\n,4scales\n8.11%\nDifferentiableRAM,4glimpses,\n12\n\n12\n4.18%\nDifferentiableRAM,8glimpses,\n12\n\n12\n3.36\n%\ntime-step,whereasRAMusedfour,atdifferentzooms.\n4.2.MNISTGeneration\nWetrainedthefullDRAWnetworkasagenerativemodel\nonthebinarizedMNISTdataset(\nSalakhutdinov&Mur-\nray\n,\n2008\n).Thisdatasethasbeenwidelystudiedinthe\nliterature,allowingustocomparethenumericalperfor-\nmance(measuredinaveragenatsperimageonthetest\nset)ofDRAWwithexistingmethods.Table\n2\nshowsthat\nDRAWwithoutselectiveattentionperformscomparablyto\notherrecentgenerativemodelssuchasDARN,NADEand\nDBMs,andthatDRAWwithattentionconsiderablyim-\nprovesonthestateoftheart.\nTable2.\nNegativelog-likelihood(innats)pertest-setexampleon\nthebinarisedMNISTdataset.Therighthandcolumn,where\npresent,givesanupperbound(Eq.\n12\n)onthenegativelog-\nlikelihood.Thepreviousresultsarefrom[1](\nSalakhutdinov&\nHinton\n,\n2009\n),[2](\nMurray&Salakhutdinov\n,\n2009\n),[3](\nUria\netal.\n,\n2014\n),[4](\nRaikoetal.\n,\n2014\n),[5](\nRezendeetal.\n,\n2014\n),\n[6](\nSalimansetal.\n,\n2014\n),[7](\nGregoretal.\n,\n2014\n).\nModel\n\nlog\np\n\nDBM2hl[1]\n\n84\n:\n62\nDBN2hl[2]\n\n84\n:\n55\nNADE[3]\n88\n:\n33\nEoNADE2hl(128orderings)[3]\n85\n:\n10\nEoNADE-52hl(128orderings)[4]\n84\n:\n68\nDLGM[5]\n\n86\n:\n60\nDLGM8leapfrogsteps[6]\n\n85\n:\n5188\n:\n30\nDARN1hl[7]\n\n84\n:\n1388\n:\n30\nDARN12hl[7]\n-87.72\nDRAWwithoutattention\n-87.40\nDRAW\n-\n80.97\nFigure6.\nGeneratedMNISTimages.\nAlldigitsweregenerated\nbyDRAWexceptthoseintherightmostcolumn,whichshowsthe\ntrainingsetimagesclosesttothoseinthecolumnsecondtothe\nright(pixelwise\nL\n2\nisthedistancemeasure).Notethatthenet-\nworkwastrainedonbinarysamples,whilethegeneratedimages\naremeanprobabilities.\nOncetheDRAWnetworkwastrained,wegenerated\nMNISTdigitsfollowingthemethodinSection\n2.3\n,exam-\nplesofwhicharepresentedinFig.\n6\n.Fig.\n7\nillustrates\ntheimagegenerationsequenceforaDRAWnetworkwith-\noutselectiveattention(seeSection\n3.1\n).Itisinterestingto\ncomparethiswiththegenerationsequenceforDRAWwith\nattention,asdepictedinFig.\n1\n.Whereaswithoutattention\nitprogressivelysharpensablurredimageinaglobalway,\n'b'DRAW:ARecurrentNeuralNetworkForImageGeneration\nFigure7.\nMNISTgenerationsequencesforDRAWwithoutat-\ntention.\nNoticehowthenetworkgeneratesaveryblurryim-\nagethatissubsequently\nwithattentionitconstructsthedigitbytracingthelines\nmuchlikeapersonwithapen.\n4.3.MNISTGenerationwithTwoDigits\nThemainmotivationforusinganattention-basedgenera-\ntivemodelisthatlargeimagescanbebuiltupiteratively,\nbyaddingtoasmallpartoftheimageatatime.Totest\nthiscapabilityinacontrolledfashion,wetrainedDRAW\ntogenerateimageswithtwo\n28\n\n28\nMNISTimagescho-\nsenatrandomandplacedatrandomlocationsina\n60\n\n60\nblackbackground.Incaseswherethetwodigitsoverlap,\nthepixelintensitieswereaddedtogetherateachpointand\nclippedtobenogreaterthanone.Examplesofgenerated\ndataareshowninFig.\n8\n.Thenetworktypicallygenerates\nonedigitandthentheother,suggestinganabilitytorecre-\natecompositescenesfromsimplepieces.\n4.4.StreetViewHouseNumberGeneration\nMNISTdigitsareverysimplisticintermsofvisualstruc-\nture,andwewerekeentoseehowwellDRAWperformed\nonnaturalimages.Ournaturalimagegenerationex-\nperimentusedthemulti-digitStreetViewHouseNumbers\ndataset(\nNetzeretal.\n,\n2011\n).Weusedthesamepreprocess-\ningas(\nGoodfellowetal.\n,\n2013\n),yieldinga\n64\n\n64\nhouse\nnumberimageforeachtrainingexample.Thenetworkwas\nthentrainedusing\n54\n\n54\npatchesextractedatrandomlo-\ncationsfromthepreprocessedimages.TheSVHNtraining\nsetcontains231,053images,andthevalidationsetcontains\n4,701images.\nThehousenumberimagesgeneratedbythenetworkare\nFigure8.\nGeneratedMNISTimageswithtwodigits.\nFigure9.\nGeneratedSVHNimages.\nTherightmostcolumn\nshowsthetrainingimagesclosest(in\nL\n2\ndistance)tothegener-\natedimagesbesidethem.Notethatthetwocolumnsarevisually\nsimilar,butthenumbersaregenerallydifferent.\nhighlyrealistic,asshowninFigs.\n9\nand\n10\n.Fig.\n11\nreveals\nthat,despitethelongtrainingtime,theDRAWnetworkun-\ntheSVHNtrainingdata.\n4.5.GeneratingCIFARImages\nThemostchallengingdatasetweappliedDRAWtowas\ntheCIFAR-10collectionofnaturalimages(\nKrizhevsky\n,\n'b'DRAW:ARecurrentNeuralNetworkForImageGeneration\nTable3.\nExperimentalHyper-Parameters.\nTask#glimpsesLSTM#\nh\n#\nz\nReadSizeWriteSize\n100\n\n100\nMNIST\n8256\n-\n12\n\n12\n-\nMNISTModel\n642561002\n\n25\n\n5\nSVHNModel\n3280010012\n\n1212\n\n12\nCIFARModel\n644002005\n\n55\n\n5\ns\nFigure10.\nSVHNGenerationSequences.\nTheredrectanglein-\ndicatestheattentionpatch.Noticehowthenetworkdrawsthedig-\nitsoneatatime,andhowitmovesandscalesthewritingpatchto\nproducenumberswithdifferentslopesandsizes.\nFigure11.\nTrainingandvalidationcostonSVHN.\nThevalida-\ntioncostisconsistentlylowerbecausethevalidationsetpatches\nwereextractedfromtheimagecentre(ratherthanfromrandom\nlocations,asinthetrainingset).Thenetworkwasneverableto\novonthetrainingdata.\n2009\n).CIFAR-10isverydiverse,andwithonly50,000\ntrainingexamplesitisverydiftogeneraterealistic-\nFigure12.\nGeneratedCIFARimages.\nTherightmostcolumn\nshowsthenearesttrainingexamplestothecolumnbesideit.\nlookingobjectswithoutov(inotherwords,without\ncopyingfromthetrainingset).Nonethelesstheimagesin\nFig.\n12\ndemonstratethatDRAWisabletocapturemuchof\ntheshape,colourandcompositionofrealphotographs.\n5.Conclusion\nThispaperintroducedtheDeepRecurrentAttentiveWriter\n(DRAW)neuralnetworkarchitecture,anddemonstratedits\nabilitytogeneratehighlyrealisticnaturalimagessuchas\nphotographsofhousenumbers,aswellasimprovingonthe\nbestknownresultsforbinarizedMNISTgeneration.We\nalsoestablishedthatthetwo-dimensionaldifferentiableat-\ntentionmechanismembeddedinDRAWisnot\nonlytoimagegeneration,butalsotoimage\nAcknowledgments\nOfthemanywhoassistedincreatingthispaper,wearees-\npeciallythankfultoKorayKavukcuoglu,VolodymyrMnih,\nJimmyBa,YaroslavBulatov,GregWayne,AndreiRusu\nandShakirMohamed.\n'b"DRAW:ARecurrentNeuralNetworkForImageGeneration\nReferences\nBa,Jimmy,Mnih,Volodymyr,andKavukcuoglu,Koray.\nMultipleobjectrecognitionwithvisualattention.\narXiv\npreprintarXiv:1412.7755\n,2014.\nDayan,Peter,Hinton,GeoffreyE,Neal,RadfordM,and\nZemel,RichardS.Thehelmholtzmachine.\nNeuralcom-\nputation\n,7(5):889904,1995.\nDenil,Misha,Bazzani,Loris,Larochelle,Hugo,and\ndeFreitas,Nando.Learningwheretoattendwithdeep\narchitecturesforimagetracking.\nNeuralcomputation\n,\n24(8):21512184,2012.\nGers,FelixA,Schmidhuber,J\n\nurgen,andCummins,Fred.\nLearningtoforget:Continualpredictionwithlstm.\nNeu-\nralcomputation\n,12(10):24512471,2000.\nGoodfellow,IanJ,Bulatov,Yaroslav,Ibarz,Julian,\nArnoud,Sacha,andShet,Vinay.Multi-digit\nnumberrecognitionfromstreetviewimageryusing\ndeepconvolutionalneuralnetworks.\narXivpreprint\narXiv:1312.6082\n,2013.\nGraves,Alex.Generatingsequenceswithrecurrentneural\nnetworks.\narXivpreprintarXiv:1308.0850\n,2013.\nGraves,Alex,Wayne,Greg,andDanihelka,Ivo.Neural\nturingmachines.\narXivpreprintarXiv:1410.5401\n,2014.\nGregor,Karol,Danihelka,Ivo,Mnih,Andriy,Blundell,\nCharles,andWierstra,Daan.Deepautoregressivenet-\nworks.In\nProceedingsofthe31stInternationalConfer-\nenceonMachineLearning\n,2014.\nHinton,GeoffreyEandSalakhutdinov,RuslanR.Reduc-\ningthedimensionalityofdatawithneuralnetworks.\nSci-\nence\n,313(5786):504507,2006.\nHochreiter,SeppandSchmidhuber,J\n\nurgen.Longshort-\ntermmemory.\nNeuralcomputation\n,9(8):17351780,\n1997.\nKingma,DiederikandBa,Jimmy.Adam:A\nmethodforstochasticoptimization.\narXivpreprint\narXiv:1412.6980\n,2014.\nKingma,DiederikPandWelling,Max.Auto-encoding\nvariationalbayes.In\nProceedingsoftheInternational\nConferenceonLearningRepresentations(ICLR)\n,2014.\nKrizhevsky,Alex.Learningmultiplelayersoffeatures\nfromtinyimages.2009.\nLarochelle,HugoandHinton,GeoffreyE.Learningto\ncombinefovealglimpseswithathird-orderboltzmann\nmachine.In\nAdvancesinNeuralInformationProcessing\nSystems\n,pp.12431251.2010.\nLarochelle,HugoandMurray,Iain.Theneuralautoregres-\nsivedistributionestimator.\nJournalofMachineLearning\nResearch\n,15:2937,2011.\nLeCun,Yann,Bottou,L\n\neon,Bengio,Yoshua,andHaffner,\nPatrick.Gradient-basedlearningappliedtodocument\nrecognition.\nProceedingsoftheIEEE\n,86(11):2278\n2324,1998.\nMnih,AndriyandGregor,Karol.Neuralvariationalinfer-\nenceandlearninginbeliefnetworks.In\nProceedingsof\nthe31stInternationalConferenceonMachineLearning\n,\n2014.\nMnih,Volodymyr,Heess,Nicolas,Graves,Alex,etal.Re-\ncurrentmodelsofvisualattention.In\nAdvancesinNeural\nInformationProcessingSystems\n,pp.22042212,2014.\nMurray,IainandSalakhutdinov,Ruslan.Evaluatingprob-\nabilitiesunderhigh-dimensionallatentvariablemodels.\nIn\nAdvancesinneuralinformationprocessingsystems\n,\npp.11371144,2009.\nNetzer,Yuval,Wang,Tao,Coates,Adam,Bissacco,\nAlessandro,Wu,Bo,andNg,AndrewY.Readingdig-\nitsinnaturalimageswithunsupervisedfeaturelearning.\n2011.\nRaiko,Tapani,Li,Yao,Cho,Kyunghyun,andBengio,\nYoshua.Iterativeneuralautoregressivedistributiones-\ntimatornade-k.In\nAdvancesinNeuralInformationPro-\ncessingSystems\n,pp.325333.2014.\nRanzato,Marc'Aurelio.Onlearningwheretolook.\narXiv\npreprintarXiv:1405.5488\n,2014.\nRezende,DaniloJ,Mohamed,Shakir,andWierstra,Daan.\nStochasticbackpropagationandapproximateinference\nindeepgenerativemodels.In\nProceedingsofthe31stIn-\nternationalConferenceonMachineLearning\n,pp.1278\n1286,2014.\nSalakhutdinov,RuslanandHinton,GeoffreyE.Deepboltz-\nmannmachines.In\nInternationalConferenceon\ncialIntelligenceandStatistics\n,pp.448455,2009.\nSalakhutdinov,RuslanandMurray,Iain.Onthequantita-\ntiveanalysisofDeepBeliefNetworks.In\nProceedings\nofthe25thAnnualInternationalConferenceonMachine\nLearning\n,pp.872879.Omnipress,2008.\nSalimans,Tim,Kingma,DiederikP,andWelling,Max.\nMarkovchainmontecarloandvariationalinference:\nBridgingthegap.\narXivpreprintarXiv:1410.6460\n,2014.\nSermanet,Pierre,Frome,Andrea,andReal,Esteban.At-\ntentionforcategorization.\narXivpreprint\narXiv:1412.7054\n,2014.\n"b'DRAW:ARecurrentNeuralNetworkForImageGeneration\nSutskever,Ilya,Vinyals,Oriol,andLe,QuocVV.Se-\nquencetosequencelearningwithneuralnetworks.In\nAdvancesinNeuralInformationProcessingSystems\n,pp.\n31043112,2014.\nTang,Yichuan,Srivastava,Nitish,andSalakhutdinov,Rus-\nlan.Learninggenerativemodelswithvisualattention.\narXivpreprintarXiv:1312.6110\n,2013.\nTieleman,Tijmen.\nOptimizingNeuralNetworksthatGen-\nerateImages\n.PhDthesis,UniversityofToronto,2014.\nUria,Benigno,Murray,Iain,andLarochelle,Hugo.Adeep\nandtractabledensityestimator.In\nProceedingsofthe\n31stInternationalConferenceonMachineLearning\n,pp.\n467475,2014.\nZheng,Yin,Zemel,RichardS,Zhang,Yu-Jin,and\nLarochelle,Hugo.Aneuralautoregressiveapproach\ntoattention-basedrecognition.\nInternationalJournalof\nComputerVision\n,pp.113,2014.\n'